{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "yolov3_training_furits.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bavel4885/python_report/blob/main/yolov3_training_furits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpvZnHe4OeLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f28d068-9dd3-4dd7-d735-39c30b9a9a8c"
      },
      "source": [
        "import os, sys \n",
        "from google.colab import drive \n",
        "\n",
        "drive.mount('/content/MyDrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "B0f4OjZhFxlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4af481b-9f7e-442b-9513-8c4fa358c21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcontent\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKw7HZG7LgtP",
        "outputId": "2aa74161-b469-4380-c9ed-96fd623536bb"
      },
      "source": [
        "cd /content/MyDrive/MyDrive/CV_exprt"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MyDrive/MyDrive/CV_exprt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0epuu1ng1Sp",
        "outputId": "03358c1c-488e-46df-add5-a85534ae14ef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'01 tensorflow.ipynb'\n",
            "'01 tensorflow.ipynb의 정답'\n",
            "'02. keras .ipynb'\n",
            "'03 Keras API 를 활용한 MNIST 데이터셋 분류 실습.ipynb'\n",
            "'04 Keras API를 활용한 자동차 연비 예측 실습.ipynb'\n",
            "'05. Fashion_minist 분류모델 .ipynb'\n",
            " \u001b[0m\u001b[01;34mCNN\u001b[0m/\n",
            " \u001b[01;34mfruit\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.19.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHTotNPCGdB5",
        "outputId": "3e8c7d6d-5433-44c7-ac3a-99556f032126"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.19.0 in /usr/local/lib/python3.7/dist-packages (1.19.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ISd6eTgXQNd"
      },
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQzbsaxpXSVs"
      },
      "source": [
        "# YOLO options\n",
        "YOLO_DARKNET_WEIGHTS        = \"./fruit/yolov3.weights\"\n",
        "YOLO_COCO_CLASSES           = \"./fruit/coco.names\"\n",
        "YOLO_STRIDES                = [8, 16, 32]\n",
        "YOLO_IOU_LOSS_THRESH        = 0.5\n",
        "YOLO_ANCHOR_PER_SCALE       = 3\n",
        "YOLO_MAX_BBOX_PER_SCALE     = 100\n",
        "YOLO_INPUT_SIZE             = 416\n",
        "YOLO_ANCHORS                = [[[10,  13], [16,   30], [33,   23]],\n",
        "                               [[30,  61], [62,   45], [59,  119]],\n",
        "                               [[116, 90], [156, 198], [373, 326]]]\n",
        "# Train options\n",
        "TRAIN_CLASSES               = \"./fruit/fruits.names\"\n",
        "TRAIN_ANNOT_PATH            = \"./fruit/train.txt\"\n",
        "TRAIN_BATCH_SIZE            = 4\n",
        "TRAIN_INPUT_SIZE            = 416\n",
        "TRAIN_DATA_AUG              = True\n",
        "TRAIN_TRANSFER              = False\n",
        "TRAIN_FROM_CHECKPOINT       = False #\"./checkpoints_furits/yolov3_custom\"\n",
        "\n",
        "TRAIN_LR_INIT               = 1e-4\n",
        "TRAIN_LR_END                = 1e-6\n",
        "TRAIN_WARMUP_EPOCHS         = 2\n",
        "TRAIN_EPOCHS                = 20\n",
        "\n",
        "# TEST options\n",
        "TEST_ANNOT_PATH             = \"./fruit/test.txt\"\n",
        "TEST_BATCH_SIZE             = 4\n",
        "TEST_INPUT_SIZE             = 416\n",
        "TEST_DATA_AUG               = False\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KzpkCyp95zC"
      },
      "source": [
        "import cv2\n",
        "import time\n",
        "import random\n",
        "import colorsys\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8JMPV9_-m1M"
      },
      "source": [
        "utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raaIsjRk98PJ"
      },
      "source": [
        "def load_yolo_weights(model, weights_file):\n",
        "    tf.keras.backend.clear_session() # used to reset layer names\n",
        "    # load Darknet original weights to Keras model\n",
        "    with open(weights_file, 'rb') as wf:\n",
        "        major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n",
        "\n",
        "        j = 0\n",
        "        for i in range(75):\n",
        "            if i > 0:\n",
        "                conv_layer_name = 'conv2d_%d' %i\n",
        "            else:\n",
        "                conv_layer_name = 'conv2d'\n",
        "                \n",
        "            if j > 0:\n",
        "                bn_layer_name = 'batch_normalization_%d' %j\n",
        "            else:\n",
        "                bn_layer_name = 'batch_normalization'\n",
        "            \n",
        "            conv_layer = model.get_layer(conv_layer_name)\n",
        "            filters = conv_layer.filters\n",
        "            k_size = conv_layer.kernel_size[0]\n",
        "            in_dim = conv_layer.input_shape[-1]\n",
        "\n",
        "            if i not in [58, 66, 74]:\n",
        "                # darknet weights: [beta, gamma, mean, variance]\n",
        "                bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n",
        "                # tf weights: [gamma, beta, mean, variance]\n",
        "                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n",
        "                bn_layer = model.get_layer(bn_layer_name)\n",
        "                j += 1\n",
        "            else:\n",
        "                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n",
        "\n",
        "            # darknet shape (out_dim, in_dim, height, width)\n",
        "            conv_shape = (filters, in_dim, k_size, k_size)\n",
        "            conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n",
        "            # tf shape (height, width, in_dim, out_dim)\n",
        "            conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n",
        "\n",
        "            if i not in [58, 66, 74]:\n",
        "                conv_layer.set_weights([conv_weights])\n",
        "                bn_layer.set_weights(bn_weights)\n",
        "            else:\n",
        "                conv_layer.set_weights([conv_weights, conv_bias])\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBrLfn24-CaP"
      },
      "source": [
        "def read_class_names(class_file_name):\n",
        "    # loads class name from a file\n",
        "    #print(class_file_name)\n",
        "    names = {}\n",
        "    with open(class_file_name, 'r') as data:\n",
        "        for ID, name in enumerate(data):\n",
        "            names[ID] = name.strip('\\n')\n",
        "    return names"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXTy87MuLlX0",
        "outputId": "230b0dbe-4c57-4130-ba5a-bbac4a3380c6"
      },
      "source": [
        "read_class_names(TRAIN_CLASSES)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'apple', 1: 'banana', 2: 'orange'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IKBK700-EXF"
      },
      "source": [
        "def image_preprocess(image, target_size, gt_boxes=None):\n",
        "    ih, iw    = target_size\n",
        "    h,  w, _  = image.shape\n",
        "\n",
        "    scale = min(iw/w, ih/h)\n",
        "    nw, nh  = int(scale * w), int(scale * h)\n",
        "    image_resized = cv2.resize(image, (nw, nh))\n",
        "\n",
        "    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n",
        "    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n",
        "    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n",
        "    image_paded = image_paded / 255.\n",
        "    print(gt_boxes)\n",
        "    if gt_boxes is None:\n",
        "        return image_paded\n",
        "\n",
        "    else:\n",
        "        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\n",
        "        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\n",
        "        return image_paded, gt_boxes"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDX_984L-F1Y"
      },
      "source": [
        "\n",
        "def draw_bbox(image, bboxes, CLASSES=TRAIN_CLASSES, show_label=True, show_confidence = True, Text_colors=(255,255,0), rectangle_colors=''):   \n",
        "    NUM_CLASS = read_class_names(CLASSES)\n",
        "    num_classes = len(NUM_CLASS)\n",
        "    image_h, image_w, _ = image.shape\n",
        "    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
        "    #print(\"hsv_tuples\", hsv_tuples)\n",
        "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "\n",
        "    random.seed(0)\n",
        "    random.shuffle(colors)\n",
        "    random.seed(None)\n",
        "\n",
        "    for i, bbox in enumerate(bboxes):\n",
        "        coor = np.array(bbox[:4], dtype=np.int32)\n",
        "        score = bbox[4]\n",
        "        class_ind = int(bbox[5])\n",
        "        bbox_color = rectangle_colors if rectangle_colors != ''else colors[class_ind]\n",
        "        bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n",
        "        if bbox_thick < 1: bbox_thick = 1\n",
        "        #print(image_h, image_w, bbox_thick)\n",
        "        fontScale = 0.75 * bbox_thick\n",
        "        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
        "\n",
        "        # put object rectangle\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)\n",
        "\n",
        "        if show_label:\n",
        "            # get text label\n",
        "            score_str = f' {score:.2f}' if show_confidence else '' \n",
        "            label = f'{NUM_CLASS[class_ind]}' + score_str\n",
        "\n",
        "            # get text size\n",
        "            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
        "                                                                  fontScale, thickness=bbox_thick)\n",
        "            # put filled text rectangle\n",
        "            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n",
        "\n",
        "            # put text above rectangle\n",
        "            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
        "                        fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n",
        "\n",
        "    return image"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY2vi4Fi-OsJ"
      },
      "source": [
        "def bboxes_iou(boxes1, boxes2):\n",
        "    boxes1 = np.array(boxes1)\n",
        "    boxes2 = np.array(boxes2)\n",
        "\n",
        "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
        "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
        "\n",
        "    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n",
        "    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
        "    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area    = boxes1_area + boxes2_area - inter_area\n",
        "    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(float).eps)\n",
        "\n",
        "    return ious\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxduNYb6-QNh"
      },
      "source": [
        "\n",
        "def nms(bboxes, iou_threshold, sigma=0.3):\n",
        "\n",
        "    classes_in_img = list(set(bboxes[:, 5]))\n",
        "    best_bboxes = []\n",
        "\n",
        "    for cls in classes_in_img:\n",
        "        cls_mask = (bboxes[:, 5] == cls)\n",
        "        cls_bboxes = bboxes[cls_mask]\n",
        "        # Process 1: Determine whether the number of bounding boxes is greater than 0 \n",
        "        while len(cls_bboxes) > 0:\n",
        "            # Process 2: Select the bounding box with the highest score according to socre order A\n",
        "            max_ind = np.argmax(cls_bboxes[:, 4])\n",
        "            best_bbox = cls_bboxes[max_ind]\n",
        "            best_bboxes.append(best_bbox)\n",
        "            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n",
        "            # Process 3: Calculate this bounding box A and\n",
        "            # Remain all iou of the bounding box and remove those bounding boxes whose iou value is higher than the threshold \n",
        "            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n",
        "            weight = np.ones((len(iou),), dtype=float)\n",
        "\n",
        "            iou_mask = iou > iou_threshold\n",
        "            weight[iou_mask] = 0.0\n",
        "\n",
        "            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n",
        "            score_mask = cls_bboxes[:, 4] > 0.\n",
        "            cls_bboxes = cls_bboxes[score_mask]\n",
        "\n",
        "    return best_bboxes"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcenMeqH-SeO"
      },
      "source": [
        "def postprocess_boxes(pred_bbox, original_image, input_size, score_threshold):\n",
        "    valid_scale=[0, np.inf]\n",
        "    pred_bbox = np.array(pred_bbox)\n",
        "\n",
        "    pred_xywh = pred_bbox[:, 0:4]\n",
        "    pred_conf = pred_bbox[:, 4]\n",
        "    pred_prob = pred_bbox[:, 5:]\n",
        "\n",
        "    # 1. (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n",
        "    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n",
        "                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n",
        "    # 2. (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n",
        "    org_h, org_w = original_image.shape[:2]\n",
        "    resize_ratio = min(input_size / org_w, input_size / org_h)\n",
        "\n",
        "    dw = (input_size - resize_ratio * org_w) / 2\n",
        "    dh = (input_size - resize_ratio * org_h) / 2\n",
        "\n",
        "    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n",
        "    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n",
        "\n",
        "    # 3. clip some boxes those are out of range\n",
        "    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n",
        "                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n",
        "    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n",
        "    pred_coor[invalid_mask] = 0\n",
        "\n",
        "    # 4. discard some invalid boxes\n",
        "    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n",
        "    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n",
        "\n",
        "    # 5. discard boxes with low scores\n",
        "    classes = np.argmax(pred_prob, axis=-1)\n",
        "    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n",
        "    score_mask = scores > score_threshold\n",
        "    mask = np.logical_and(scale_mask, score_mask)\n",
        "    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n",
        "    #print(coors)\n",
        "\n",
        "    #print(coors[0])\n",
        "    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf91vRsKXTQd"
      },
      "source": [
        "def detect_image(YoloV3, image_path, output_path, input_size=416, show=False, CLASSES=TRAIN_CLASSES, score_threshold=0.3, iou_threshold=0.45, rectangle_colors=''):\n",
        "    original_image      = cv2.imread(image_path)\n",
        "    original_image      = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "    #original_image      = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    image_data = image_preprocess(np.copy(original_image), [input_size, input_size])\n",
        "    image_data = tf.expand_dims(image_data, 0)\n",
        "\n",
        "    pred_bbox = YoloV3.predict(image_data)\n",
        "    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n",
        "    pred_bbox = tf.concat(pred_bbox, axis=0)\n",
        "    \n",
        "    bboxes = postprocess_boxes(pred_bbox, original_image, input_size, score_threshold)\n",
        "    #xx=np.array(bboxes)\n",
        "    #print(xx.shape)\n",
        "\n",
        "    bboxes = nms(bboxes, iou_threshold)\n",
        "    image = draw_bbox(original_image, bboxes, CLASSES=CLASSES, rectangle_colors=rectangle_colors)\n",
        "    #cv2.imwrite('./output.jpg', image)\n",
        "        \n",
        "    return image"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrWfbwjQ-fiU"
      },
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDDDzxjh-qmA"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, ZeroPadding2D, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "\n",
        "STRIDES         = np.array(YOLO_STRIDES)\n",
        "ANCHORS         = (np.array(YOLO_ANCHORS).T/STRIDES).T\n",
        "IOU_LOSS_THRESH = YOLO_IOU_LOSS_THRESH"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMrWDDaDXaE_"
      },
      "source": [
        "class BatchNormalization(BatchNormalization):\n",
        "    # \"Frozen state\" and \"inference mode\" are two separate concepts.\n",
        "    # `layer.trainable = False` is to freeze the layer, so the layer will use\n",
        "    # stored moving `var` and `mean` in the \"inference mode\", and both `gama`\n",
        "    # and `beta` will not be updated !\n",
        "    def call(self, x, training=False):\n",
        "        if not training:\n",
        "            training = tf.constant(False)\n",
        "        training = tf.logical_and(training, self.trainable)\n",
        "        return super().call(x, training)\n",
        "\n",
        "def convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True):\n",
        "    if downsample:\n",
        "        input_layer = ZeroPadding2D(((1, 0), (1, 0)))(input_layer)\n",
        "        padding = 'valid'\n",
        "        strides = 2\n",
        "    else:\n",
        "        strides = 1\n",
        "        padding = 'same'\n",
        "\n",
        "    conv = Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], strides=strides,\n",
        "                  padding=padding, use_bias=not bn, kernel_regularizer=l2(0.0005),\n",
        "                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
        "                  bias_initializer=tf.constant_initializer(0.))(input_layer)\n",
        "    if bn:\n",
        "        conv = BatchNormalization()(conv)\n",
        "    if activate == True:\n",
        "        conv = LeakyReLU(alpha=0.1)(conv)\n",
        "\n",
        "    return conv\n",
        "\n",
        "def residual_block(input_layer, input_channel, filter_num1, filter_num2):\n",
        "    short_cut = input_layer\n",
        "    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1))\n",
        "    conv = convolutional(conv       , filters_shape=(3, 3, filter_num1,   filter_num2))\n",
        "\n",
        "    residual_output = short_cut + conv\n",
        "    return residual_output\n",
        "\n",
        "def upsample(input_layer):\n",
        "    return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='nearest')\n",
        "\n",
        "\n",
        "def darknet53(input_data):\n",
        "    input_data = convolutional(input_data, (3, 3,  3,  32))\n",
        "    input_data = convolutional(input_data, (3, 3, 32,  64), downsample=True)\n",
        "\n",
        "    for i in range(1):\n",
        "        input_data = residual_block(input_data,  64,  32, 64)\n",
        "\n",
        "    input_data = convolutional(input_data, (3, 3,  64, 128), downsample=True)\n",
        "\n",
        "    for i in range(2):\n",
        "        input_data = residual_block(input_data, 128,  64, 128)\n",
        "\n",
        "    input_data = convolutional(input_data, (3, 3, 128, 256), downsample=True)\n",
        "\n",
        "    for i in range(8):\n",
        "        input_data = residual_block(input_data, 256, 128, 256)\n",
        "\n",
        "    route_1 = input_data\n",
        "    input_data = convolutional(input_data, (3, 3, 256, 512), downsample=True)\n",
        "\n",
        "    for i in range(8):\n",
        "        input_data = residual_block(input_data, 512, 256, 512)\n",
        "\n",
        "    route_2 = input_data\n",
        "    input_data = convolutional(input_data, (3, 3, 512, 1024), downsample=True)\n",
        "\n",
        "    for i in range(4):\n",
        "        input_data = residual_block(input_data, 1024, 512, 1024)\n",
        "\n",
        "    return route_1, route_2, input_data\n",
        "\n",
        "def YOLOv3(input_layer, NUM_CLASS):\n",
        "    # After the input layer enters the Darknet-53 network, we get three branches\n",
        "    route_1, route_2, conv = darknet53(input_layer)\n",
        "    # See the orange module (DBL) in the figure above, a total of 5 Subconvolution operation\n",
        "    conv = convolutional(conv, (1, 1, 1024,  512))\n",
        "    conv = convolutional(conv, (3, 3,  512, 1024))\n",
        "    conv = convolutional(conv, (1, 1, 1024,  512))\n",
        "    conv = convolutional(conv, (3, 3,  512, 1024))\n",
        "    conv = convolutional(conv, (1, 1, 1024,  512))\n",
        "    conv_lobj_branch = convolutional(conv, (3, 3, 512, 1024))\n",
        "    \n",
        "    # conv_lbbox is used to predict large-sized objects , Shape = [None, 13, 13, 255] \n",
        "    conv_lbbox = convolutional(conv_lobj_branch, (1, 1, 1024, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n",
        "\n",
        "    conv = convolutional(conv, (1, 1,  512,  256))\n",
        "    # upsample here uses the nearest neighbor interpolation method, which has the advantage that the\n",
        "    # upsampling process does not need to learn, thereby reducing the network parameter  \n",
        "    conv = upsample(conv)\n",
        "\n",
        "    conv = tf.concat([conv, route_2], axis=-1)\n",
        "    conv = convolutional(conv, (1, 1, 768, 256))\n",
        "    conv = convolutional(conv, (3, 3, 256, 512))\n",
        "    conv = convolutional(conv, (1, 1, 512, 256))\n",
        "    conv = convolutional(conv, (3, 3, 256, 512))\n",
        "    conv = convolutional(conv, (1, 1, 512, 256))\n",
        "    conv_mobj_branch = convolutional(conv, (3, 3, 256, 512))\n",
        "\n",
        "    # conv_mbbox is used to predict medium-sized objects, shape = [None, 26, 26, 255]\n",
        "    conv_mbbox = convolutional(conv_mobj_branch, (1, 1, 512, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n",
        "\n",
        "    conv = convolutional(conv, (1, 1, 256, 128))\n",
        "    conv = upsample(conv)\n",
        "\n",
        "    conv = tf.concat([conv, route_1], axis=-1)\n",
        "    conv = convolutional(conv, (1, 1, 384, 128))\n",
        "    conv = convolutional(conv, (3, 3, 128, 256))\n",
        "    conv = convolutional(conv, (1, 1, 256, 128))\n",
        "    conv = convolutional(conv, (3, 3, 128, 256))\n",
        "    conv = convolutional(conv, (1, 1, 256, 128))\n",
        "    conv_sobj_branch = convolutional(conv, (3, 3, 128, 256))\n",
        "    \n",
        "    # conv_sbbox is used to predict small size objects, shape = [None, 52, 52, 255]\n",
        "    conv_sbbox = convolutional(conv_sobj_branch, (1, 1, 256, 3*(NUM_CLASS +5)), activate=False, bn=False)\n",
        "        \n",
        "    return [conv_sbbox, conv_mbbox, conv_lbbox]\n",
        "\n",
        "def Create_Yolov3(input_size=416, channels=3, training=False, CLASSES=TRAIN_CLASSES):\n",
        "    NUM_CLASS = len(read_class_names(CLASSES))\n",
        "    input_layer  = Input([input_size, input_size, channels])\n",
        "\n",
        "    conv_tensors = YOLOv3(input_layer, NUM_CLASS)\n",
        "\n",
        "    output_tensors = []\n",
        "    for i, conv_tensor in enumerate(conv_tensors):\n",
        "        pred_tensor = decode(conv_tensor, NUM_CLASS, i)\n",
        "        if training: output_tensors.append(conv_tensor)\n",
        "        output_tensors.append(pred_tensor)\n",
        "\n",
        "    YoloV3 = tf.keras.Model(input_layer, output_tensors)\n",
        "    return YoloV3\n",
        "\n",
        "def decode(conv_output, NUM_CLASS, i=0):\n",
        "    # where i = 0, 1 or 2 to correspond to the three grid scales  \n",
        "    conv_shape       = tf.shape(conv_output)\n",
        "    batch_size       = conv_shape[0]\n",
        "    output_size      = conv_shape[1]\n",
        "\n",
        "    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n",
        "\n",
        "    conv_raw_dxdy = conv_output[:, :, :, :, 0:2] # offset of center position     \n",
        "    conv_raw_dwdh = conv_output[:, :, :, :, 2:4] # Prediction box length and width offset\n",
        "    conv_raw_conf = conv_output[:, :, :, :, 4:5] # confidence of the prediction box\n",
        "    conv_raw_prob = conv_output[:, :, :, :, 5: ] # category probability of the prediction box \n",
        "\n",
        "    # next need Draw the grid. Where output_size is equal to 13, 26 or 52  \n",
        "    y = tf.range(output_size, dtype=tf.int32)\n",
        "    y = tf.expand_dims(y, -1)\n",
        "    y = tf.tile(y, [1, output_size])\n",
        "    x = tf.range(output_size,dtype=tf.int32)\n",
        "    x = tf.expand_dims(x, 0)\n",
        "    x = tf.tile(x, [output_size, 1])\n",
        "\n",
        "    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n",
        "    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])\n",
        "    xy_grid = tf.cast(xy_grid, tf.float32)\n",
        "\n",
        "    # Calculate the center position of the prediction box:\n",
        "    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]\n",
        "    # Calculate the length and width of the prediction box:\n",
        "    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]\n",
        "\n",
        "    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n",
        "    pred_conf = tf.sigmoid(conv_raw_conf) # object box calculates the predicted confidence\n",
        "    pred_prob = tf.sigmoid(conv_raw_prob) # calculating the predicted probability category box object\n",
        "\n",
        "    # calculating the predicted probability category box object\n",
        "    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n",
        "\n",
        "def bbox_iou(boxes1, boxes2):\n",
        "    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n",
        "    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n",
        "\n",
        "    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n",
        "    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n",
        "\n",
        "    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n",
        "    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    inter_section = tf.maximum(right_down - left_up, 0.0)\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area = boxes1_area + boxes2_area - inter_area\n",
        "\n",
        "    return 1.0 * inter_area / union_area\n",
        "\n",
        "def bbox_giou(boxes1, boxes2):\n",
        "    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n",
        "    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n",
        "\n",
        "    boxes1 = tf.concat([tf.minimum(boxes1[..., :2], boxes1[..., 2:]),\n",
        "                        tf.maximum(boxes1[..., :2], boxes1[..., 2:])], axis=-1)\n",
        "    boxes2 = tf.concat([tf.minimum(boxes2[..., :2], boxes2[..., 2:]),\n",
        "                        tf.maximum(boxes2[..., :2], boxes2[..., 2:])], axis=-1)\n",
        "\n",
        "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
        "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
        "\n",
        "    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n",
        "    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    inter_section = tf.maximum(right_down - left_up, 0.0)\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area = boxes1_area + boxes2_area - inter_area\n",
        "    iou = inter_area / union_area\n",
        "\n",
        "    enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])\n",
        "    enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "    enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)\n",
        "    enclose_area = enclose[..., 0] * enclose[..., 1]\n",
        "    giou = iou - 1.0 * (enclose_area - union_area) / enclose_area\n",
        "\n",
        "    return giou\n",
        "\n",
        "\n",
        "def compute_loss(pred, conv, label, bboxes, i=0, CLASSES=TRAIN_CLASSES):\n",
        "    NUM_CLASS = len(read_class_names(CLASSES))\n",
        "    conv_shape  = tf.shape(conv)\n",
        "    batch_size  = conv_shape[0]\n",
        "    output_size = conv_shape[1]\n",
        "    input_size  = STRIDES[i] * output_size\n",
        "    conv = tf.reshape(conv, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n",
        "\n",
        "    conv_raw_conf = conv[:, :, :, :, 4:5]\n",
        "    conv_raw_prob = conv[:, :, :, :, 5:]\n",
        "\n",
        "    pred_xywh     = pred[:, :, :, :, 0:4]\n",
        "    pred_conf     = pred[:, :, :, :, 4:5]\n",
        "\n",
        "    label_xywh    = label[:, :, :, :, 0:4]\n",
        "    respond_bbox  = label[:, :, :, :, 4:5]\n",
        "    label_prob    = label[:, :, :, :, 5:]\n",
        "\n",
        "    giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)\n",
        "    input_size = tf.cast(input_size, tf.float32)\n",
        "\n",
        "    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\n",
        "    giou_loss = respond_bbox * bbox_loss_scale * (1- giou)\n",
        "\n",
        "    iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])\n",
        "    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)\n",
        "\n",
        "    respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou < IOU_LOSS_THRESH, tf.float32)\n",
        "\n",
        "    conf_focal = tf.pow(respond_bbox - pred_conf, 2)\n",
        "\n",
        "    conf_loss = conf_focal * (\n",
        "            respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n",
        "            +\n",
        "            respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n",
        "    )\n",
        "\n",
        "    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n",
        "\n",
        "    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1,2,3,4]))\n",
        "    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1,2,3,4]))\n",
        "    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1,2,3,4]))\n",
        "\n",
        "    return giou_loss, conf_loss, prob_loss"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoTCdT7V-isD"
      },
      "source": [
        "dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9FsKyf7Xdfu"
      },
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "class Dataset(object):\n",
        "    # Dataset preprocess implementation\n",
        "    def __init__(self, dataset_type):\n",
        "        self.annot_path  = TRAIN_ANNOT_PATH if dataset_type == 'train' else TEST_ANNOT_PATH\n",
        "        self.input_sizes = TRAIN_INPUT_SIZE if dataset_type == 'train' else TEST_INPUT_SIZE\n",
        "        self.batch_size  = TRAIN_BATCH_SIZE if dataset_type == 'train' else TEST_BATCH_SIZE\n",
        "        self.data_aug    = TRAIN_DATA_AUG   if dataset_type == 'train' else TEST_DATA_AUG\n",
        "\n",
        "        self.train_input_sizes = TRAIN_INPUT_SIZE\n",
        "        self.strides = np.array(YOLO_STRIDES)\n",
        "        self.classes = read_class_names(TRAIN_CLASSES)\n",
        "        self.num_classes = len(self.classes)\n",
        "        self.anchors = (np.array(YOLO_ANCHORS).T/self.strides).T\n",
        "        self.anchor_per_scale = YOLO_ANCHOR_PER_SCALE\n",
        "        self.max_bbox_per_scale = YOLO_MAX_BBOX_PER_SCALE\n",
        "\n",
        "        self.annotations = self.load_annotations(dataset_type)\n",
        "        self.num_samples = len(self.annotations)\n",
        "        self.num_batchs = int(np.ceil(self.num_samples / self.batch_size))\n",
        "        self.batch_count = 0\n",
        "\n",
        "\n",
        "    def load_annotations(self, dataset_type):\n",
        "        final_annotations = []\n",
        "        with open(self.annot_path, 'r') as f:\n",
        "            txt = f.readlines()\n",
        "            annotations = [line.strip() for line in txt if len(line.strip().split()[1:]) != 0]\n",
        "        np.random.shuffle(annotations)\n",
        "        \n",
        "        for annotation in annotations:\n",
        "            # fully parse annotations\n",
        "            line = annotation.split()\n",
        "            image_path, index = \"\", 1\n",
        "            for i, one_line in enumerate(line):\n",
        "                if not one_line.replace(\",\",\"\").isnumeric():\n",
        "                    if image_path != \"\": image_path += \" \"\n",
        "                    image_path += one_line\n",
        "                else:\n",
        "                    index = i\n",
        "                    break\n",
        "            if not os.path.exists(image_path):\n",
        "                raise KeyError(\"%s does not exist ... \" %image_path)\n",
        "\n",
        "            final_annotations.append([image_path, line[index:]])\n",
        "\n",
        "        return final_annotations\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        with tf.device('/cpu:0'):\n",
        "            self.train_input_size = random.choice([self.train_input_sizes])\n",
        "            self.train_output_sizes = self.train_input_size // self.strides\n",
        "\n",
        "            batch_image = np.zeros((self.batch_size, self.train_input_size, self.train_input_size, 3), dtype=np.float32)\n",
        "\n",
        "            batch_label_sbbox = np.zeros((self.batch_size, self.train_output_sizes[0], self.train_output_sizes[0],\n",
        "                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n",
        "            batch_label_mbbox = np.zeros((self.batch_size, self.train_output_sizes[1], self.train_output_sizes[1],\n",
        "                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n",
        "            batch_label_lbbox = np.zeros((self.batch_size, self.train_output_sizes[2], self.train_output_sizes[2],\n",
        "                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n",
        "\n",
        "            batch_sbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n",
        "            batch_mbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n",
        "            batch_lbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n",
        "\n",
        "            num = 0\n",
        "            if self.batch_count < self.num_batchs:\n",
        "                while num < self.batch_size:\n",
        "                    index = self.batch_count * self.batch_size + num\n",
        "                    if index >= self.num_samples: index -= self.num_samples\n",
        "                    annotation = self.annotations[index]\n",
        "                    image, bboxes = self.parse_annotation(annotation)\n",
        "                    label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes = self.preprocess_true_boxes(bboxes)\n",
        "\n",
        "                    batch_image[num, :, :, :] = image\n",
        "                    batch_label_sbbox[num, :, :, :, :] = label_sbbox\n",
        "                    batch_label_mbbox[num, :, :, :, :] = label_mbbox\n",
        "                    batch_label_lbbox[num, :, :, :, :] = label_lbbox\n",
        "                    batch_sbboxes[num, :, :] = sbboxes\n",
        "                    batch_mbboxes[num, :, :] = mbboxes\n",
        "                    batch_lbboxes[num, :, :] = lbboxes\n",
        "                    num += 1\n",
        "                self.batch_count += 1\n",
        "                batch_smaller_target = batch_label_sbbox, batch_sbboxes\n",
        "                batch_medium_target  = batch_label_mbbox, batch_mbboxes\n",
        "                batch_larger_target  = batch_label_lbbox, batch_lbboxes\n",
        "\n",
        "                return batch_image, (batch_smaller_target, batch_medium_target, batch_larger_target)\n",
        "            else:\n",
        "                self.batch_count = 0\n",
        "                np.random.shuffle(self.annotations)\n",
        "                raise StopIteration\n",
        "\n",
        "    def random_horizontal_flip(self, image, bboxes):\n",
        "        if random.random() < 0.5:\n",
        "            _, w, _ = image.shape\n",
        "            image = image[:, ::-1, :]\n",
        "            bboxes[:, [0,2]] = w - bboxes[:, [2,0]]\n",
        "\n",
        "        return image, bboxes\n",
        "\n",
        "    def random_crop(self, image, bboxes):\n",
        "        if random.random() < 0.5:\n",
        "            h, w, _ = image.shape\n",
        "            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n",
        "\n",
        "            max_l_trans = max_bbox[0]\n",
        "            max_u_trans = max_bbox[1]\n",
        "            max_r_trans = w - max_bbox[2]\n",
        "            max_d_trans = h - max_bbox[3]\n",
        "\n",
        "            crop_xmin = max(0, int(max_bbox[0] - random.uniform(0, max_l_trans)))\n",
        "            crop_ymin = max(0, int(max_bbox[1] - random.uniform(0, max_u_trans)))\n",
        "            crop_xmax = max(w, int(max_bbox[2] + random.uniform(0, max_r_trans)))\n",
        "            crop_ymax = max(h, int(max_bbox[3] + random.uniform(0, max_d_trans)))\n",
        "\n",
        "            image = image[crop_ymin : crop_ymax, crop_xmin : crop_xmax]\n",
        "\n",
        "            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] - crop_xmin\n",
        "            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] - crop_ymin\n",
        "\n",
        "        return image, bboxes\n",
        "\n",
        "    def random_translate(self, image, bboxes):\n",
        "        if random.random() < 0.5:\n",
        "            h, w, _ = image.shape\n",
        "            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n",
        "\n",
        "            max_l_trans = max_bbox[0]\n",
        "            max_u_trans = max_bbox[1]\n",
        "            max_r_trans = w - max_bbox[2]\n",
        "            max_d_trans = h - max_bbox[3]\n",
        "\n",
        "            tx = random.uniform(-(max_l_trans - 1), (max_r_trans - 1))\n",
        "            ty = random.uniform(-(max_u_trans - 1), (max_d_trans - 1))\n",
        "\n",
        "            M = np.array([[1, 0, tx], [0, 1, ty]])\n",
        "            image = cv2.warpAffine(image, M, (w, h))\n",
        "\n",
        "            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx\n",
        "            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty\n",
        "\n",
        "        return image, bboxes\n",
        "\n",
        "    def parse_annotation(self, annotation):\n",
        "\n",
        "        image_path = annotation[0]\n",
        "        image = cv2.imread(image_path)\n",
        "            \n",
        "        bboxes = np.array([list(map(int, box.split(','))) for box in annotation[1]])\n",
        "\n",
        "        if self.data_aug:\n",
        "            image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes))\n",
        "            image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes))\n",
        "            image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes))\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image, bboxes = image_preprocess(np.copy(image), [self.train_input_size, self.train_input_size], np.copy(bboxes))\n",
        "        return image, bboxes\n",
        "\n",
        "    def preprocess_true_boxes(self, bboxes):\n",
        "        label = [np.zeros((self.train_output_sizes[i], self.train_output_sizes[i], self.anchor_per_scale,\n",
        "                           5 + self.num_classes)) for i in range(3)]\n",
        "        bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4)) for _ in range(3)]\n",
        "        bbox_count = np.zeros((3,))\n",
        "\n",
        "        for bbox in bboxes:\n",
        "            bbox_coor = bbox[:4]\n",
        "            bbox_class_ind = bbox[4]\n",
        "\n",
        "            onehot = np.zeros(self.num_classes, dtype=np.float64)\n",
        "            onehot[bbox_class_ind] = 1.0\n",
        "            uniform_distribution = np.full(self.num_classes, 1.0 / self.num_classes)\n",
        "            deta = 0.01\n",
        "            smooth_onehot = onehot * (1 - deta) + deta * uniform_distribution\n",
        "\n",
        "            bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2]) * 0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1)\n",
        "            bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis]\n",
        "\n",
        "            iou = []\n",
        "            exist_positive = False\n",
        "            for i in range(3):\n",
        "                anchors_xywh = np.zeros((self.anchor_per_scale, 4))\n",
        "                anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n",
        "                anchors_xywh[:, 2:4] = self.anchors[i]\n",
        "\n",
        "                iou_scale = bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)\n",
        "                iou.append(iou_scale)\n",
        "                iou_mask = iou_scale > 0.3\n",
        "\n",
        "                if np.any(iou_mask):\n",
        "                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)\n",
        "\n",
        "                    label[i][yind, xind, iou_mask, :] = 0\n",
        "                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\n",
        "                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n",
        "                    label[i][yind, xind, iou_mask, 5:] = smooth_onehot\n",
        "\n",
        "                    bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale)\n",
        "                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n",
        "                    bbox_count[i] += 1\n",
        "\n",
        "                    exist_positive = True\n",
        "\n",
        "            if not exist_positive:\n",
        "                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n",
        "                best_detect = int(best_anchor_ind / self.anchor_per_scale)\n",
        "                best_anchor = int(best_anchor_ind % self.anchor_per_scale)\n",
        "                xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)\n",
        "\n",
        "                label[best_detect][yind, xind, best_anchor, :] = 0\n",
        "                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n",
        "                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n",
        "                label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot\n",
        "                \n",
        "                bbox_ind = int(bbox_count[best_detect] % self.max_bbox_per_scale)\n",
        "                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n",
        "                bbox_count[best_detect] += 1\n",
        "\n",
        "        label_sbbox, label_mbbox, label_lbbox = label\n",
        "        sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
        "        return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batchs\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgvPlA8Ufp8f",
        "outputId": "eee6ab35-bfc8-4d38-8805-391697d491b8"
      },
      "source": [
        "trainset = Dataset('train')\n",
        "steps_per_epoch = len(trainset)\n",
        "steps_per_epoch"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIQuRMfmXpgX"
      },
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMp-XBHSXicX",
        "outputId": "ad9c5d6f-059b-4ba2-e5ac-0f102ac79b12"
      },
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "global TRAIN_FROM_CHECKPOINT\n",
        "input_size = YOLO_INPUT_SIZE\n",
        "Darknet_weights = YOLO_DARKNET_WEIGHTS\n",
        "\n",
        "\n",
        "save_best_only = True # saves only best model according validation loss\n",
        "save_checkpoints = False # saves all best validated checkpoints in training process (may require a lot disk space)\n",
        "\n",
        "\n",
        "trainset = Dataset('train')\n",
        "testset = Dataset('test')\n",
        "\n",
        "steps_per_epoch = len(trainset)\n",
        "global_steps = tf.Variable(1, trainable=False, dtype=tf.int64)\n",
        "warmup_steps = TRAIN_WARMUP_EPOCHS * steps_per_epoch\n",
        "total_steps = TRAIN_EPOCHS * steps_per_epoch\n",
        "\n",
        "if TRAIN_TRANSFER:\n",
        "    Darknet = Create_Yolov3(input_size=input_size)\n",
        "    load_yolo_weights(Darknet, Darknet_weights) # use darknet weights\n",
        "\n",
        "yolo = Create_Yolov3(input_size=input_size, training=True, CLASSES=TRAIN_CLASSES)\n",
        "\n",
        "#TRAIN_FROM_CHECKPOINT = False\n",
        "if TRAIN_FROM_CHECKPOINT:\n",
        "    yolo.load_weights(TRAIN_FROM_CHECKPOINT)\n",
        "\n",
        "## transfer && Not use checkpoint\n",
        "if TRAIN_TRANSFER and not TRAIN_FROM_CHECKPOINT:\n",
        "    for i, l in enumerate(Darknet.layers):\n",
        "        layer_weights = l.get_weights()\n",
        "        if layer_weights != []:\n",
        "            try:\n",
        "                yolo.layers[i].set_weights(layer_weights)\n",
        "            except:\n",
        "                print(\"skipping\", yolo.layers[i].name)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def train_step(image_data, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred_result = yolo(image_data, training=True)\n",
        "        giou_loss=conf_loss=prob_loss=0\n",
        "        # optimizing process\n",
        "        for i in range(3):\n",
        "            conv, pred = pred_result[i*2], pred_result[i*2+1]\n",
        "            loss_items = compute_loss(pred, conv, *target[i], i, CLASSES=TRAIN_CLASSES)\n",
        "            giou_loss += loss_items[0]\n",
        "            conf_loss += loss_items[1]\n",
        "            prob_loss += loss_items[2]\n",
        "        total_loss = giou_loss + conf_loss + prob_loss\n",
        "        gradients = tape.gradient(total_loss, yolo.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, yolo.trainable_variables))\n",
        "\n",
        "\n",
        "        # update learning rate\n",
        "        global_steps.assign_add(1)\n",
        "        if global_steps < warmup_steps:# and not TRAIN_TRANSFER:\n",
        "            lr = global_steps / warmup_steps * TRAIN_LR_INIT\n",
        "        else:\n",
        "            lr = TRAIN_LR_END + 0.5 * (TRAIN_LR_INIT - TRAIN_LR_END)*(\n",
        "                (1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi)))\n",
        "        optimizer.lr.assign(lr.numpy())\n",
        "\n",
        "    return global_steps.numpy(), optimizer.lr.numpy(), giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), total_loss.numpy()\n",
        "\n",
        "\n",
        "\n",
        "def validate_step(image_data, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred_result = yolo(image_data, training=False)\n",
        "        giou_loss=conf_loss=prob_loss=0\n",
        "\n",
        "        # optimizing process\n",
        "        for i in range(3):\n",
        "            conv, pred = pred_result[i*2], pred_result[i*2+1]\n",
        "            loss_items = compute_loss(pred, conv, *target[i], i, CLASSES=TRAIN_CLASSES)\n",
        "            giou_loss += loss_items[0]\n",
        "            conf_loss += loss_items[1]\n",
        "            prob_loss += loss_items[2]\n",
        "\n",
        "        total_loss = giou_loss + conf_loss + prob_loss\n",
        "        \n",
        "    return giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), total_loss.numpy()\n",
        "\n",
        "\n",
        "best_val_loss = 1000 # should be large at start\n",
        "\n",
        "\n",
        "for epoch in range(TRAIN_EPOCHS):\n",
        "    for image_data, target in trainset:\n",
        "        results = train_step(image_data, target)\n",
        "        cur_step = results[0]%steps_per_epoch\n",
        "        print(\"epoch:{:2.0f} step:{:5.0f}/{}, lr:{:.6f}, giou_loss:{:7.2f}, conf_loss:{:7.2f}, prob_loss:{:7.2f}, total_loss:{:7.2f}\"\n",
        "                  .format(epoch, cur_step, steps_per_epoch, results[1], results[2], results[3], results[4], results[5]))\n",
        "        \n",
        "    count, giou_val, conf_val, prob_val, total_val = 0., 0, 0, 0, 0\n",
        "    for image_data, target in testset:\n",
        "        results = validate_step(image_data, target)\n",
        "        count += 1\n",
        "        giou_val += results[0]\n",
        "        conf_val += results[1]\n",
        "        prob_val += results[2]\n",
        "        total_val += results[3]\n",
        "\n",
        "        \n",
        "    print(\"\\n\\ngiou_val_loss:{:7.2f}, conf_val_loss:{:7.2f}, prob_val_loss:{:7.2f}, total_val_loss:{:7.2f}\\n\\n\".\n",
        "          format(giou_val/count, conf_val/count, prob_val/count, total_val/count))\n",
        "\n",
        "    if save_best_only and best_val_loss > total_val/count: \n",
        "        yolo.save_weights(\"./chkpt_fruits/yolov3_custom\")\n",
        "        best_val_loss = total_val/count\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "[[  1   1 794 738   0]]\n",
            "\n",
            "\n",
            "giou_val_loss:   1.63, conf_val_loss:  53.56, prob_val_loss:   7.00, total_val_loss:  62.18\n",
            "\n",
            "\n",
            "[[  2  35 250 291   2]]\n",
            "[[ 19  16 206 218   2]]\n",
            "[[  20   55 1887 1875    2]]\n",
            "[[240 330 752 887   0]]\n",
            "epoch: 8 step:    2/60, lr:0.000075, giou_loss:   0.79, conf_loss:  37.72, prob_loss:   2.41, total_loss:  40.92\n",
            "[[ 203  142 1182 1137    0]]\n",
            "[[111   1 474 348   0]]\n",
            "[[ 15  77 474 327   1]]\n",
            "[[  2  94 203 299   2]]\n",
            "epoch: 8 step:    3/60, lr:0.000075, giou_loss:   1.02, conf_loss:  37.04, prob_loss:   0.73, total_loss:  38.80\n",
            "[[182   1 686 528   0]]\n",
            "[[ 20  54 169 269   1]]\n",
            "[[ 397  277 1355 1017    2  822  643 1620 1066    2]]\n",
            "[[ 23  17 429 436   0]]\n",
            "epoch: 8 step:    4/60, lr:0.000075, giou_loss:   0.82, conf_loss:  36.69, prob_loss:   0.68, total_loss:  38.20\n",
            "[[117  35 295 213   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "[[209 176 971 911   2]]\n",
            "[[ 367   51 1014  415    1]]\n",
            "[[ 515  962 1149 1962    1  546  621 1071 1679    1]]\n",
            "epoch: 8 step:    5/60, lr:0.000075, giou_loss:   1.11, conf_loss:  37.46, prob_loss:   1.23, total_loss:  39.80\n",
            "[[ 56   6 375 118   1]]\n",
            "[[  9  22 205 205   0]]\n",
            "[[  1  35 299 323   0]]\n",
            "[[422 293 718 565   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "epoch: 8 step:    6/60, lr:0.000074, giou_loss:   1.33, conf_loss:  37.76, prob_loss:   3.39, total_loss:  42.49\n",
            "[[316 171 735 569   2 664 128 919 369   2]]\n",
            "[[231  57 547 350   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "[[ 65  18 693 192   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "[[180 331 344 481   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "epoch: 8 step:    7/60, lr:0.000074, giou_loss:   1.59, conf_loss:  37.71, prob_loss:   1.60, total_loss:  40.90\n",
            "[[ 23  55 458 497   0]]\n",
            "[[218 163 546 487   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "[[388  12 724 382   2 430  58 759 380   2]]\n",
            "[[  6  11 206 199   0]]\n",
            "epoch: 8 step:    8/60, lr:0.000074, giou_loss:   1.02, conf_loss:  37.39, prob_loss:   1.72, total_loss:  40.13\n",
            "[[127  29 560 458   0]]\n",
            "[[ 69  91 525 460   2]]\n",
            "[[137  96 386 348   0 314 112 594 385   0]]\n",
            "[[ 34  91 641 694   0]]\n",
            "epoch: 8 step:    9/60, lr:0.000074, giou_loss:   0.59, conf_loss:  36.87, prob_loss:   0.60, total_loss:  38.06\n",
            "[[ 10  16 120 126   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[ 94 138 377 408   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[ 98  92 578 296   1]]\n",
            "[[  4   1 327 335   0]]\n",
            "epoch: 8 step:   10/60, lr:0.000074, giou_loss:   1.75, conf_loss:  38.52, prob_loss:   4.47, total_loss:  44.75\n",
            "[[337  83 579 338   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "[[129 118 613 594   0   1 132 368 605   0]]\n",
            "[[240  39 473 306   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "[[ 26  28 300 295   2]]\n",
            "epoch: 8 step:   11/60, lr:0.000074, giou_loss:   1.26, conf_loss:  37.70, prob_loss:   2.25, total_loss:  41.21\n",
            "[[ 26 340 318 456   1]]\n",
            "[[152  98 350 289   2 214  75 383 248   2]]\n",
            "[[  52   18 1866  941    1]]\n",
            "[[407 248 781 626   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "epoch: 8 step:   12/60, lr:0.000074, giou_loss:   1.33, conf_loss:  36.00, prob_loss:   0.74, total_loss:  38.08\n",
            "[[ 207  369 1314 1124    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "[[ 216  126 1723  902    1]]\n",
            "[[  4   1 233 225   2]]\n",
            "[[ 75  21 331 277   0]]\n",
            "epoch: 8 step:   13/60, lr:0.000074, giou_loss:   0.88, conf_loss:  36.42, prob_loss:   0.49, total_loss:  37.79\n",
            "[[119 240 477 598   2  77 133 674 590   1]]\n",
            "[[ 59  74 205 217   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "[[ 78 144 357 425   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "[[151  72 544 474   2]]\n",
            "epoch: 8 step:   14/60, lr:0.000073, giou_loss:   1.32, conf_loss:  37.61, prob_loss:   2.12, total_loss:  41.05\n",
            "[[ 786   51 1412 1189    1]]\n",
            "[[  6   3 299 119   1]]\n",
            "[[ 35  20 285 275   0]]\n",
            "[[ 25  62 294 326   0]]\n",
            "epoch: 8 step:   15/60, lr:0.000073, giou_loss:   1.48, conf_loss:  37.44, prob_loss:   1.81, total_loss:  40.73\n",
            "[[ 21   3 232 199   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[ 89  72 708 686   0]]\n",
            "[[ 20  81 408 256   1]]\n",
            "[[ 969  133 1392  540    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "epoch: 8 step:   16/60, lr:0.000073, giou_loss:   1.17, conf_loss:  37.51, prob_loss:   1.80, total_loss:  40.48\n",
            "[[282   2 467  94   1 305  81 489 175   1]]\n",
            "[[431 587 596 782   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[ 23  33 272 265   2]]\n",
            "[[ 351  146  615  398    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "epoch: 8 step:   17/60, lr:0.000073, giou_loss:   2.27, conf_loss:  38.67, prob_loss:   3.45, total_loss:  44.39\n",
            "[[ 12  76 569 310   1]]\n",
            "[[ 34   8 258 239   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "[[ 57   4 511 460   2]]\n",
            "[[148 109 651 355   1]]\n",
            "epoch: 8 step:   18/60, lr:0.000073, giou_loss:   0.92, conf_loss:  35.62, prob_loss:   1.46, total_loss:  38.00\n",
            "[[ 81  81 319 359   2 347 127 603 398   2]]\n",
            "[[ 30   1 115  95   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "[[ 45  44 219 219   0  18  22 168 190   0]]\n",
            "[[  83    8 1440 1328    0]]\n",
            "epoch: 8 step:   19/60, lr:0.000073, giou_loss:   1.19, conf_loss:  35.94, prob_loss:   0.79, total_loss:  37.92\n",
            "[[ 366  575 1173 1514    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "[[118 160 260 305   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "[[   7  450 1408 1841    2]]\n",
            "[[ 48  53 255 261   0]]\n",
            "epoch: 8 step:   20/60, lr:0.000073, giou_loss:   1.34, conf_loss:  35.86, prob_loss:   1.45, total_loss:  38.65\n",
            "[[ 88  47 531 442   1]]\n",
            "[[145  21 614 355   0   3  40 475 439   0]]\n",
            "[[ 35 124 548 636   2]]\n",
            "[[ 561    1 1261  708    0]]\n",
            "epoch: 8 step:   21/60, lr:0.000073, giou_loss:   1.28, conf_loss:  35.52, prob_loss:   0.90, total_loss:  37.69\n",
            "[[283   4 642 337   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[ 79  56 563 346   1]]\n",
            "[[182  12 479 299   0]]\n",
            "[[ 10  68 151 225   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "epoch: 8 step:   22/60, lr:0.000072, giou_loss:   1.19, conf_loss:  35.10, prob_loss:   2.78, total_loss:  39.07\n",
            "[[ 388  442 1193  655    1]]\n",
            "[[  4  45 244 249   0]]\n",
            "[[231 108 582 481   0 363  83 770 514   0]]\n",
            "[[201  15 503 329   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "epoch: 8 step:   23/60, lr:0.000072, giou_loss:   1.24, conf_loss:  36.82, prob_loss:   3.64, total_loss:  41.70\n",
            "[[182  51 576 432   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "[[ 99  78 311 283   2]]\n",
            "[[ 708   81 1253  598    2]]\n",
            "[[ 70  25 290 226   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "epoch: 8 step:   24/60, lr:0.000072, giou_loss:   1.57, conf_loss:  37.22, prob_loss:   2.15, total_loss:  40.94\n",
            "[[ 26 233 928 711   1]]\n",
            "[[  1 150 427 364   1]]\n",
            "[[ 90  43 523 368   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[  8  17 155 161   0]]\n",
            "epoch: 8 step:   25/60, lr:0.000072, giou_loss:   1.06, conf_loss:  35.67, prob_loss:   1.14, total_loss:  37.87\n",
            "[[  2  23 170 193   0]]\n",
            "[[318 146 680 511   0]]\n",
            "[[ 70  44 381 315   1 100  65 361 353   1]]\n",
            "[[ 89  27 751 730   2]]\n",
            "epoch: 8 step:   26/60, lr:0.000072, giou_loss:   1.01, conf_loss:  34.99, prob_loss:   0.89, total_loss:  36.90\n",
            "[[ 19  17 306 303   0]]\n",
            "[[ 16  52 142 192   0]]\n",
            "[[263  43 564 363   2]]\n",
            "[[  58   85 2790 2568    1]]\n",
            "epoch: 8 step:   27/60, lr:0.000072, giou_loss:   0.75, conf_loss:  34.13, prob_loss:   0.56, total_loss:  35.44\n",
            "[[ 16  47 213 817   1]]\n",
            "[[221 161 412 350   2 328 173 488 351   2]]\n",
            "[[208  41 349 282   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[123  49 491 438   2]]\n",
            "epoch: 8 step:   28/60, lr:0.000072, giou_loss:   1.80, conf_loss:  35.47, prob_loss:   5.44, total_loss:  42.70\n",
            "[[  3 147 422 341   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "[[ 36   1 462 332   1]]\n",
            "[[ 66  47 276 309   1]]\n",
            "[[  39  397  994 1033    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "epoch: 8 step:   29/60, lr:0.000072, giou_loss:   1.39, conf_loss:  35.54, prob_loss:   3.51, total_loss:  40.44\n",
            "[[  57  215 1711 1112    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[191 169 540 537   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "[[133  16 329 261   1]]\n",
            "[[ 28  37 165 196   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "epoch: 8 step:   30/60, lr:0.000071, giou_loss:   1.14, conf_loss:  34.92, prob_loss:   2.35, total_loss:  38.42\n",
            "[[196 317 575 666   0]]\n",
            "[[ 28  62 578 614   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "[[  98  177  862  519    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "[[133 187 600 462   1 190 100 623 330   1]]\n",
            "epoch: 8 step:   31/60, lr:0.000071, giou_loss:   1.33, conf_loss:  35.22, prob_loss:   1.38, total_loss:  37.93\n",
            "[[  0  53 405 395   1]]\n",
            "[[  87   22 1552 1596    2]]\n",
            "[[ 350    1 1923  624    1]]\n",
            "[[ 58 137 267 358   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "epoch: 8 step:   32/60, lr:0.000071, giou_loss:   0.90, conf_loss:  34.40, prob_loss:   2.01, total_loss:  37.32\n",
            "[[ 75 116 385 432   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[ 86   1 511 410   2]]\n",
            "[[166  36 586 412   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "[[  2  41 282 342   2]]\n",
            "epoch: 8 step:   33/60, lr:0.000071, giou_loss:   1.00, conf_loss:  34.10, prob_loss:   2.05, total_loss:  37.15\n",
            "[[ 710  161 1359  865    0]]\n",
            "[[211  20 598 213   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[ 110   73  843  848    2  580   21 1273  753    2]]\n",
            "[[ 134   32 1013  918    2]]\n",
            "epoch: 8 step:   34/60, lr:0.000071, giou_loss:   1.31, conf_loss:  34.96, prob_loss:   2.02, total_loss:  38.28\n",
            "[[  6  40 183 203   0]]\n",
            "[[ 570   11 1148  523    2]]\n",
            "[[262  52 508 313   2]]\n",
            "[[108  22 277 177   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "epoch: 8 step:   35/60, lr:0.000071, giou_loss:   0.91, conf_loss:  33.77, prob_loss:   1.81, total_loss:  36.49\n",
            "[[ 77 114 341 427   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "[[100  29 204 136   2]]\n",
            "[[ 17   4 171 149   2]]\n",
            "[[ 257  388 1228 1172    1  445  447 1069 1046    0]]\n",
            "epoch: 8 step:   36/60, lr:0.000071, giou_loss:   1.49, conf_loss:  34.88, prob_loss:   4.64, total_loss:  41.01\n",
            "[[  70   32  592  366    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "[[ 10   4 262 263   2]]\n",
            "[[  33  160 1075  651    1   68  224 1011  751    1]]\n",
            "[[ 52   7 174 130   2 139   5 275 149   2]]\n",
            "epoch: 8 step:   37/60, lr:0.000071, giou_loss:   1.00, conf_loss:  34.36, prob_loss:   1.26, total_loss:  36.62\n",
            "[[ 70  61 191 204   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "[[244 179 549 485   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[466  75 636 363   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[220  99 784 679   0  78 176 590 665   0]]\n",
            "epoch: 8 step:   38/60, lr:0.000070, giou_loss:   1.35, conf_loss:  35.83, prob_loss:   1.58, total_loss:  38.77\n",
            "[[ 297  294 1137 1130    2]]\n",
            "[[  9  15 348 217   1]]\n",
            "[[125 196 295 374   0 197 144 363 328   0]]\n",
            "[[ 84  71 446 398   2]]\n",
            "epoch: 8 step:   39/60, lr:0.000070, giou_loss:   1.27, conf_loss:  34.13, prob_loss:   1.84, total_loss:  37.25\n",
            "[[339 181 651 469   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[ 57  22 581 380   1]]\n",
            "[[199 162 423 404   2]]\n",
            "[[151  64 324 236   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "epoch: 8 step:   40/60, lr:0.000070, giou_loss:   1.51, conf_loss:  33.55, prob_loss:   1.71, total_loss:  36.76\n",
            "[[ 10  85 245 329   0]]\n",
            "[[  1  55 194 257   0]]\n",
            "[[108 277 223 376   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "[[ 13 101 235 320   2 213  98 413 320   2]]\n",
            "epoch: 8 step:   41/60, lr:0.000070, giou_loss:   1.86, conf_loss:  35.25, prob_loss:   2.04, total_loss:  39.15\n",
            "[[ 266  363  883  983    2  990  370 1578  974    2]]\n",
            "[[  6  85 331 414   0 304  62 575 353   0]]\n",
            "[[ 77   5 309 175   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "[[ 75  26 506 467   2]]\n",
            "epoch: 8 step:   42/60, lr:0.000070, giou_loss:   1.01, conf_loss:  34.17, prob_loss:   0.50, total_loss:  35.69\n",
            "[[ 97   4 693 671   1]]\n",
            "[[ 40   7 226 189   0]]\n",
            "[[156 237 338 448   0]]\n",
            "[[110  43 641 548   2]]\n",
            "epoch: 8 step:   43/60, lr:0.000070, giou_loss:   1.32, conf_loss:  34.04, prob_loss:   2.00, total_loss:  37.36\n",
            "[[  1  22 180 192   2]]\n",
            "[[188  22 451 310   0  79  18 338 270   0]]\n",
            "[[171  54 607 291   1]]\n",
            "[[236 207 482 432   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "epoch: 8 step:   44/60, lr:0.000070, giou_loss:   1.45, conf_loss:  33.83, prob_loss:   1.50, total_loss:  36.77\n",
            "[[ 43  71 219 260   0]]\n",
            "[[  4  29 183 201   0]]\n",
            "[[ 31  27 633 480   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[ 63  63 565 266   1]]\n",
            "epoch: 8 step:   45/60, lr:0.000069, giou_loss:   1.28, conf_loss:  33.64, prob_loss:   3.80, total_loss:  38.73\n",
            "[[109 273 579 715   0]]\n",
            "[[  4  12 101 117   2]]\n",
            "[[ 73  12 405 341   2]]\n",
            "[[ 47  11 939 498   1]]\n",
            "epoch: 8 step:   46/60, lr:0.000069, giou_loss:   1.05, conf_loss:  33.85, prob_loss:   1.26, total_loss:  36.16\n",
            "[[  35    2 1348 1069    1]]\n",
            "[[ 48  31 134 119   2  83 107 161 198   2]]\n",
            "[[ 10  46 323 172   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "[[ 73 171 296 385   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "epoch: 8 step:   47/60, lr:0.000069, giou_loss:   1.62, conf_loss:  34.49, prob_loss:   2.40, total_loss:  38.52\n",
            "[[  3   9 310 341   0]]\n",
            "[[124 188 357 407   2]]\n",
            "[[ 10  26 504 273   1 272  80 498 306   1]]\n",
            "[[ 391   89 1497 1116    0]]\n",
            "epoch: 8 step:   48/60, lr:0.000069, giou_loss:   0.77, conf_loss:  33.28, prob_loss:   0.64, total_loss:  34.69\n",
            "[[103 210 655 484   1]]\n",
            "[[209   1 516 299   2]]\n",
            "[[192  24 615 353   1]]\n",
            "[[  5   3 409 147   1]]\n",
            "epoch: 8 step:   49/60, lr:0.000069, giou_loss:   1.12, conf_loss:  33.16, prob_loss:   1.39, total_loss:  35.67\n",
            "[[294   1 692 416   0]]\n",
            "[[ 59  16 462 424   2]]\n",
            "[[ 19  21 337 325   1]]\n",
            "[[ 16  88 346 420   2]]\n",
            "epoch: 8 step:   50/60, lr:0.000069, giou_loss:   0.79, conf_loss:  32.83, prob_loss:   1.23, total_loss:  34.84\n",
            "[[ 67 132 149 220   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "[[ 46  51 725 799   0]]\n",
            "[[120   0 808 520   1   4 236 433 756   1]]\n",
            "[[ 773  258 1196  684    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "epoch: 8 step:   51/60, lr:0.000069, giou_loss:   1.38, conf_loss:  34.28, prob_loss:   1.31, total_loss:  36.98\n",
            "[[1325  140 1897 1140    1]]\n",
            "[[ 67  35 250 225   2]]\n",
            "[[ 72   0 433 291   2]]\n",
            "[[ 11  43 592 448   1  24 174 599 657   1]]\n",
            "epoch: 8 step:   52/60, lr:0.000069, giou_loss:   1.25, conf_loss:  33.06, prob_loss:   3.23, total_loss:  37.54\n",
            "[[ 50 508 452 597   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "[[ 42  10 530 317   1]]\n",
            "[[ 244   18 1098  896    0]]\n",
            "[[158 109 386 342   2]]\n",
            "epoch: 8 step:   53/60, lr:0.000068, giou_loss:   1.09, conf_loss:  32.50, prob_loss:   0.93, total_loss:  34.52\n",
            "[[127 369 505 780   2 147  48 869 476   1 169 180 839 798   1]]\n",
            "[[110 289 406 607   0]]\n",
            "[[137  90 393 320   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "[[ 56  73 314 321   2 357  35 656 334   2]]\n",
            "epoch: 8 step:   54/60, lr:0.000068, giou_loss:   1.43, conf_loss:  33.57, prob_loss:   1.39, total_loss:  36.39\n",
            "[[159 132 348 326   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "[[187  33 607 475   0]]\n",
            "[[174 206 777 817   2]]\n",
            "[[280  27 745 483   2]]\n",
            "epoch: 8 step:   55/60, lr:0.000068, giou_loss:   1.05, conf_loss:  32.34, prob_loss:   0.72, total_loss:  34.10\n",
            "[[ 51 151 389 512   2  58  69 410 432   2]]\n",
            "[[160 257 894 978   2 172 181 860 843   2]]\n",
            "[[222   8 486 280   2]]\n",
            "[[  1  23 283 268   0]]\n",
            "epoch: 8 step:   56/60, lr:0.000068, giou_loss:   0.61, conf_loss:  32.37, prob_loss:   0.85, total_loss:  33.83\n",
            "[[404 173 827 595   0 401 115 725 441   0]]\n",
            "[[  2   1 338 364   2]]\n",
            "[[244 175 411 352   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "[[144 136 470 411   0]]\n",
            "epoch: 8 step:   57/60, lr:0.000068, giou_loss:   1.55, conf_loss:  33.43, prob_loss:   2.64, total_loss:  37.62\n",
            "[[117   3 360 226   2]]\n",
            "[[381 186 500 301   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "[[ 43 128 714 385   1]]\n",
            "[[166  63 529 552   1]]\n",
            "epoch: 8 step:   58/60, lr:0.000068, giou_loss:   1.99, conf_loss:  33.78, prob_loss:   2.63, total_loss:  38.39\n",
            "[[483  60 939 546   2]]\n",
            "[[119  31 447 333   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "[[159 150 962 971   0   1   1 407 617   0 104   4 624 557   0]]\n",
            "[[ 39   7 279 251   2 291  81 545 348   2]]\n",
            "epoch: 8 step:   59/60, lr:0.000068, giou_loss:   0.78, conf_loss:  31.76, prob_loss:   2.62, total_loss:  35.16\n",
            "[[ 16   8 171 155   0]]\n",
            "[[186  57 362 227   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "[[  90  159 2546 4049    1]]\n",
            "[[202  37 470 314   0]]\n",
            "epoch: 8 step:    0/60, lr:0.000067, giou_loss:   1.27, conf_loss:  32.65, prob_loss:   4.90, total_loss:  38.82\n",
            "[[ 625  196 1378  878    0]]\n",
            "[[ 18  19 195 186   0]]\n",
            "[[ 64   2 233 110   1]]\n",
            "[[   8    4 1188  470    1]]\n",
            "epoch: 8 step:    1/60, lr:0.000067, giou_loss:   1.03, conf_loss:  31.71, prob_loss:   3.42, total_loss:  36.17\n",
            "[[ 43 134 358 455   0]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[145  36 421 286   2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[227  62 723 500   0]]\n",
            "[[111  51 372 277   2]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[155 105 453 436   0]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[133 165 922 525   1]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[150 165 923 983   2]]\n",
            "[[203 115 584 452   1]]\n",
            "[[279  14 562 295   2]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "\n",
            "\n",
            "giou_val_loss:   1.49, conf_val_loss:  40.39, prob_val_loss:   3.30, total_val_loss:  45.19\n",
            "\n",
            "\n",
            "[[ 89 239 309 440   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "[[560 350 872 638   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[368  73 626 321   2 357  35 656 334   2]]\n",
            "[[ 73  82 435 409   2]]\n",
            "epoch: 9 step:    2/60, lr:0.000067, giou_loss:   1.25, conf_loss:  33.45, prob_loss:   2.64, total_loss:  37.33\n",
            "[[ 62  83 681 697   0]]\n",
            "[[  1  99 532 604   2]]\n",
            "[[ 89  48 613 406   1]]\n",
            "[[ 65  52 311 313   2]]\n",
            "epoch: 9 step:    3/60, lr:0.000067, giou_loss:   0.78, conf_loss:  31.43, prob_loss:   0.87, total_loss:  33.08\n",
            "[[ 415  116 1149  837    2  172  181  860  843    2]]\n",
            "[[153  11 815 714   2]]\n",
            "[[ 21  13 456 455   0]]\n",
            "[[ 50  98 248 289   2 214  75 383 248   2]]\n",
            "epoch: 9 step:    4/60, lr:0.000067, giou_loss:   0.72, conf_loss:  31.66, prob_loss:   0.99, total_loss:  33.37\n",
            "[[158   4 584 335   1]]\n",
            "[[  78   54 1732  951    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[ 12  45 153 202   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[120 171 604 647   0   1 132 368 605   0]]\n",
            "epoch: 9 step:    5/60, lr:0.000067, giou_loss:   0.84, conf_loss:  31.67, prob_loss:   2.57, total_loss:  35.08\n",
            "[[ 11  23 180 131   1]]\n",
            "[[180 161 647 436   1 190 100 623 330   1]]\n",
            "[[ 342    1 1147  214    1]]\n",
            "[[ 353  305 1232 1191    2]]\n",
            "epoch: 9 step:    6/60, lr:0.000067, giou_loss:   1.28, conf_loss:  31.68, prob_loss:   2.83, total_loss:  35.80\n",
            "[[567 189 846 470   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "[[ 224   75 1625 1466    2]]\n",
            "[[223  74 775 348   1]]\n",
            "[[ 15  16 403 191   1]]\n",
            "epoch: 9 step:    7/60, lr:0.000066, giou_loss:   1.14, conf_loss:  32.25, prob_loss:   2.40, total_loss:  35.79\n",
            "[[  4  61 430 275   1]]\n",
            "[[178 291 506 615   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "[[ 15   1 378 348   0]]\n",
            "[[164  36 595 477   2]]\n",
            "epoch: 9 step:    8/60, lr:0.000066, giou_loss:   0.78, conf_loss:  31.31, prob_loss:   0.55, total_loss:  32.63\n",
            "[[141 155 499 513   2  77 133 674 590   1]]\n",
            "[[602  16 898 288   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "[[347 485 512 680   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[255  60 451 305   1]]\n",
            "epoch: 9 step:    9/60, lr:0.000066, giou_loss:   2.03, conf_loss:  32.62, prob_loss:   3.73, total_loss:  38.38\n",
            "[[ 70   1 247 164   0]]\n",
            "[[ 652  210  916  462    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "[[372 364 975 975   2]]\n",
            "[[194 184 376 395   0]]\n",
            "epoch: 9 step:   10/60, lr:0.000066, giou_loss:   1.61, conf_loss:  31.80, prob_loss:   1.76, total_loss:  35.17\n",
            "[[ 485   43  908  469    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "[[114 106 627 618   2]]\n",
            "[[247  72 548 392   2]]\n",
            "[[ 144   89 1651  865    1]]\n",
            "epoch: 9 step:   11/60, lr:0.000066, giou_loss:   1.33, conf_loss:  31.03, prob_loss:   4.03, total_loss:  36.40\n",
            "[[ 18   5 205 207   2]]\n",
            "[[  20   18 2752 2501    1]]\n",
            "[[ 71 470 973 948   1]]\n",
            "[[  3  25 243 229   0]]\n",
            "epoch: 9 step:   12/60, lr:0.000066, giou_loss:   0.99, conf_loss:  31.24, prob_loss:   3.43, total_loss:  35.66\n",
            "[[301  57 897 724   1]]\n",
            "[[ 36  78 387 451   0 363  83 770 514   0]]\n",
            "[[  31    2 1845  925    1]]\n",
            "[[ 20  48 333 174   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "epoch: 9 step:   13/60, lr:0.000066, giou_loss:   1.22, conf_loss:  31.34, prob_loss:   0.63, total_loss:  33.20\n",
            "[[ 231   44  803 1044    1]]\n",
            "[[  3  33 124 176   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "[[302 182 721 580   2 664 128 919 369   2]]\n",
            "[[345 137 583 415   2 347 127 603 398   2]]\n",
            "epoch: 9 step:   14/60, lr:0.000066, giou_loss:   1.07, conf_loss:  30.72, prob_loss:   3.69, total_loss:  35.49\n",
            "[[ 70 238 419 606   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "[[167 503 569 592   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "[[ 17  54 213 237   0]]\n",
            "[[ 54   9 231 176   0]]\n",
            "epoch: 9 step:   15/60, lr:0.000065, giou_loss:   1.32, conf_loss:  31.78, prob_loss:   1.52, total_loss:  34.61\n",
            "[[ 19  21 337 325   1]]\n",
            "[[341 337 766 746   2]]\n",
            "[[451  30 621 318   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[ 16   9 170 154   2]]\n",
            "epoch: 9 step:   16/60, lr:0.000065, giou_loss:   1.93, conf_loss:  33.27, prob_loss:   7.73, total_loss:  42.93\n",
            "[[107 231 470 720   1]]\n",
            "[[ 68   3 406 364   2  58  69 410 432   2]]\n",
            "[[ 26  30 918 517   1]]\n",
            "[[ 18  52 182 202   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "epoch: 9 step:   17/60, lr:0.000065, giou_loss:   1.50, conf_loss:  31.49, prob_loss:   3.50, total_loss:  36.49\n",
            "[[239  97 565 372   0]]\n",
            "[[221  64 454 283   2]]\n",
            "[[ 537  164 1082  681    2]]\n",
            "[[ 37 178 708 435   1]]\n",
            "epoch: 9 step:   18/60, lr:0.000065, giou_loss:   1.31, conf_loss:  30.64, prob_loss:   1.36, total_loss:  33.32\n",
            "[[ 95  63 597 266   1]]\n",
            "[[ 553  120 1232  868    0]]\n",
            "[[344  76 640 394   0]]\n",
            "[[ 51   3 206 150   0]]\n",
            "epoch: 9 step:   19/60, lr:0.000065, giou_loss:   1.11, conf_loss:  31.37, prob_loss:   2.64, total_loss:  35.11\n",
            "[[ 25  14 431 433   0]]\n",
            "[[ 102   99 1969 1919    2]]\n",
            "[[220  17 398 195   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "[[165  86 377 291   2]]\n",
            "epoch: 9 step:   20/60, lr:0.000065, giou_loss:   0.68, conf_loss:  30.65, prob_loss:   1.40, total_loss:  32.73\n",
            "[[ 81  63 524 458   1]]\n",
            "[[108  78 419 349   1 100  65 361 353   1]]\n",
            "[[ 38  14 526 321   1]]\n",
            "[[ 67   1 561 248   1 272  80 498 306   1]]\n",
            "epoch: 9 step:   21/60, lr:0.000065, giou_loss:   0.87, conf_loss:  30.64, prob_loss:   3.20, total_loss:  34.71\n",
            "[[644 241 927 511   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[ 34  25 284 280   0]]\n",
            "[[ 78  22 476 437   0]]\n",
            "[[ 34  36 298 349   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "epoch: 9 step:   22/60, lr:0.000064, giou_loss:   1.33, conf_loss:  31.58, prob_loss:   2.86, total_loss:  35.77\n",
            "[[  27  575  834 1514    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "[[ 203 1409  829 2547    1]]\n",
            "[[ 645  456 1600 1092    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "[[295  32 715 408   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "epoch: 9 step:   23/60, lr:0.000064, giou_loss:   1.66, conf_loss:  31.07, prob_loss:   6.10, total_loss:  38.83\n",
            "[[  1  10 275 277   2]]\n",
            "[[  83  181 1548 1755    2]]\n",
            "[[ 809   55 1188  404    0]]\n",
            "[[216   5 532 298   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "epoch: 9 step:   24/60, lr:0.000064, giou_loss:   1.60, conf_loss:  30.58, prob_loss:   1.45, total_loss:  33.64\n",
            "[[173  62 606 387   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[216  62 335 177   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "[[ 60 115 340 416   2]]\n",
            "[[ 64  17 425 308   2]]\n",
            "epoch: 9 step:   25/60, lr:0.000064, giou_loss:   1.37, conf_loss:  30.35, prob_loss:   1.98, total_loss:  33.71\n",
            "[[243  95 568 424   0 304  62 575 353   0]]\n",
            "[[  8  24 347 226   1]]\n",
            "[[ 64  60 423 393   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[  5  68 424 262   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "epoch: 9 step:   26/60, lr:0.000064, giou_loss:   0.94, conf_loss:  30.55, prob_loss:   1.67, total_loss:  33.16\n",
            "[[394  52 570 222   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "[[ 31  27 633 480   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[ 18  24 337 136   1]]\n",
            "[[ 18  92 771 774   0]]\n",
            "epoch: 9 step:   27/60, lr:0.000064, giou_loss:   1.03, conf_loss:  30.20, prob_loss:   3.17, total_loss:  34.39\n",
            "[[ 32  81 486 537   2]]\n",
            "[[ 22  90 314 206   1]]\n",
            "[[ 596   69 1174  581    2]]\n",
            "[[250   7 557 305   2]]\n",
            "epoch: 9 step:   28/60, lr:0.000064, giou_loss:   0.72, conf_loss:  29.25, prob_loss:   0.74, total_loss:  30.70\n",
            "[[ 284  157 1017  932    2  580   21 1273  753    2]]\n",
            "[[124 108 583 358   1]]\n",
            "[[127 369 505 780   2 147  48 869 476   1 169 180 839 798   1]]\n",
            "[[  1  20 294 136   1]]\n",
            "epoch: 9 step:   29/60, lr:0.000063, giou_loss:   1.42, conf_loss:  30.02, prob_loss:   1.41, total_loss:  32.85\n",
            "[[ 82 132 562 336   1]]\n",
            "[[ 19  59 423 203   1]]\n",
            "[[ 596  800 1230 1800    1  546  621 1071 1679    1]]\n",
            "[[ 76   6 162  94   2  83 107 161 198   2]]\n",
            "epoch: 9 step:   30/60, lr:0.000063, giou_loss:   1.42, conf_loss:  30.32, prob_loss:   2.03, total_loss:  33.77\n",
            "[[ 142   27 1121 1022    0]]\n",
            "[[238  27 506 304   0]]\n",
            "[[ 74  58 497 387   1]]\n",
            "[[341  29 764 451   0 401 115 725 441   0]]\n",
            "epoch: 9 step:   31/60, lr:0.000063, giou_loss:   0.76, conf_loss:  29.52, prob_loss:   2.04, total_loss:  32.32\n",
            "[[277 110 645 499   2]]\n",
            "[[315 231 538 445   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "[[ 50  77 553 323   1]]\n",
            "[[247 259 414 436   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "epoch: 9 step:   32/60, lr:0.000063, giou_loss:   2.03, conf_loss:  31.98, prob_loss:   2.08, total_loss:  36.09\n",
            "[[ 99 156 323 387   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "[[ 57 129 246 323   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "[[  4  27 130 167   0]]\n",
            "[[ 35  33 281 258   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "epoch: 9 step:   33/60, lr:0.000063, giou_loss:   1.17, conf_loss:  30.31, prob_loss:   3.17, total_loss:  34.65\n",
            "[[ 137  112  754  732    2  990  370 1578  974    2]]\n",
            "[[ 40 108 231 297   2 328 173 488 351   2]]\n",
            "[[ 245  421 1007 1156    2]]\n",
            "[[ 45  52 213 222   0]]\n",
            "epoch: 9 step:   34/60, lr:0.000063, giou_loss:   0.93, conf_loss:  29.14, prob_loss:   1.83, total_loss:  31.89\n",
            "[[  13  397  867 1275    0]]\n",
            "[[142  37 470 339   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "[[  4 206 561 440   1]]\n",
            "[[ 91  58 354 346   0  79  18 338 270   0]]\n",
            "epoch: 9 step:   35/60, lr:0.000063, giou_loss:   0.68, conf_loss:  28.54, prob_loss:   2.28, total_loss:  31.50\n",
            "[[ 12  86 322 402   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[515  24 817 338   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "[[  4  92 409 434   1]]\n",
            "[[ 69  43 218 258   1]]\n",
            "epoch: 9 step:   36/60, lr:0.000062, giou_loss:   0.93, conf_loss:  28.83, prob_loss:   0.83, total_loss:  30.59\n",
            "[[  7 105 476 439   0   3  40 475 439   0]]\n",
            "[[  18   26 1375 1346    0]]\n",
            "[[ 17 155 857 991   2]]\n",
            "[[265 174 562 461   0]]\n",
            "epoch: 9 step:   37/60, lr:0.000062, giou_loss:   0.92, conf_loss:  29.00, prob_loss:   1.22, total_loss:  31.14\n",
            "[[  4   4 151 148   0]]\n",
            "[[ 61  74 481 516   0]]\n",
            "[[489 235 794 541   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[531 139 905 517   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "epoch: 9 step:   38/60, lr:0.000062, giou_loss:   0.76, conf_loss:  28.24, prob_loss:   3.70, total_loss:  32.70\n",
            "[[ 315  289 1422 1044    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "[[  1  13 299 301   0]]\n",
            "[[131 107 213 195   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "[[ 65  18 693 192   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "epoch: 9 step:   39/60, lr:0.000062, giou_loss:   1.38, conf_loss:  29.19, prob_loss:   0.96, total_loss:  31.53\n",
            "[[294 127 543 379   0 314 112 594 385   0]]\n",
            "[[162 113 384 332   2 213  98 413 320   2]]\n",
            "[[ 54  15 386 344   2]]\n",
            "[[ 90  86 299 307   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "epoch: 9 step:   40/60, lr:0.000062, giou_loss:   0.97, conf_loss:  28.94, prob_loss:   5.23, total_loss:  35.15\n",
            "[[ 14   8 278 280   2]]\n",
            "[[245 237 826 642   1  24 174 599 657   1]]\n",
            "[[ 74 178 468 559   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "[[101  13 275 188   0  18  22 168 190   0]]\n",
            "epoch: 9 step:   41/60, lr:0.000062, giou_loss:   0.67, conf_loss:  28.49, prob_loss:   1.07, total_loss:  30.24\n",
            "[[115  30 627 587   0]]\n",
            "[[172  95 341 250   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "[[185 147 409 389   2]]\n",
            "[[197  94 398 299   2]]\n",
            "epoch: 9 step:   42/60, lr:0.000062, giou_loss:   0.69, conf_loss:  28.59, prob_loss:   1.54, total_loss:  30.82\n",
            "[[ 311   23 1417 1050    0]]\n",
            "[[ 45  53 252 261   0]]\n",
            "[[ 17  21 227 283   1]]\n",
            "[[ 31  21 259 254   2]]\n",
            "epoch: 9 step:   43/60, lr:0.000061, giou_loss:   0.64, conf_loss:  29.04, prob_loss:   0.87, total_loss:  30.54\n",
            "[[ 71 208 241 386   0 197 144 363 328   0]]\n",
            "[[ 49  61 186 220   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "[[207  55 663 424   2]]\n",
            "[[331 247 980 951   0]]\n",
            "epoch: 9 step:   44/60, lr:0.000061, giou_loss:   1.64, conf_loss:  29.35, prob_loss:   1.67, total_loss:  32.67\n",
            "[[318  86 558 330   2 291  81 545 348   2]]\n",
            "[[ 461   13 1774 1080    1]]\n",
            "[[ 55  77 238 267   2]]\n",
            "[[161 194 394 461   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "epoch: 9 step:   45/60, lr:0.000061, giou_loss:   0.80, conf_loss:  28.58, prob_loss:   0.86, total_loss:  30.25\n",
            "[[ 29  12 336 344   0]]\n",
            "[[223  76 327 183   2]]\n",
            "[[ 143   87 1716  710    1]]\n",
            "[[179  38 414 282   0]]\n",
            "epoch: 9 step:   46/60, lr:0.000061, giou_loss:   1.12, conf_loss:  28.05, prob_loss:   1.01, total_loss:  30.18\n",
            "[[178  72 389 268   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[ 41  67 151 177   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[ 653   16 1175  350    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "[[324  34 789 490   2]]\n",
            "epoch: 9 step:   47/60, lr:0.000061, giou_loss:   1.50, conf_loss:  29.99, prob_loss:   2.49, total_loss:  33.98\n",
            "[[ 16  37 285 301   0]]\n",
            "[[  19  283  990 1067    1  445  447 1069 1046    0]]\n",
            "[[190  12 422 182   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "[[ 56  49 526 491   0]]\n",
            "epoch: 9 step:   48/60, lr:0.000061, giou_loss:   0.69, conf_loss:  29.44, prob_loss:   2.44, total_loss:  32.57\n",
            "[[159 270 521 635   0]]\n",
            "[[ 87  80 423 443   2]]\n",
            "[[ 14  44 270 300   0]]\n",
            "[[ 55   8 241 190   0]]\n",
            "epoch: 9 step:   49/60, lr:0.000061, giou_loss:   0.81, conf_loss:  27.62, prob_loss:   2.50, total_loss:  30.92\n",
            "[[  3  31 285 276   0]]\n",
            "[[ 51  44 338 330   0]]\n",
            "[[269  53 916 417   1]]\n",
            "[[  95    6  859  348    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "epoch: 9 step:   50/60, lr:0.000061, giou_loss:   0.97, conf_loss:  29.25, prob_loss:   1.20, total_loss:  31.42\n",
            "[[183  68 368 160   1 305  81 489 175   1]]\n",
            "[[ 77 230 684 833   0]]\n",
            "[[  2  45 258 275   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "[[ 20  13 269 245   2]]\n",
            "epoch: 9 step:   51/60, lr:0.000060, giou_loss:   1.46, conf_loss:  29.38, prob_loss:   2.94, total_loss:  33.78\n",
            "[[ 85 137 488 545   2]]\n",
            "[[  4  14 204 202   0]]\n",
            "[[385  35 526 276   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[ 24  11 253 235   2]]\n",
            "epoch: 9 step:   52/60, lr:0.000060, giou_loss:   1.29, conf_loss:  28.55, prob_loss:   1.56, total_loss:  31.39\n",
            "[[241 236 929 756   1   4 236 433 756   1]]\n",
            "[[ 46   4 294 260   2]]\n",
            "[[105 124 435 456   2]]\n",
            "[[ 24 187 588 767   0  78 176 590 665   0]]\n",
            "epoch: 9 step:   53/60, lr:0.000060, giou_loss:   0.81, conf_loss:  27.63, prob_loss:   1.38, total_loss:  29.82\n",
            "[[ 383    1 1083  708    0]]\n",
            "[[ 126  135 1168  626    1   68  224 1011  751    1]]\n",
            "[[147  18 262 117   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "[[ 27  57 173 200   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "epoch: 9 step:   54/60, lr:0.000060, giou_loss:   1.98, conf_loss:  29.29, prob_loss:   2.03, total_loss:  33.31\n",
            "[[ 23  69 507 359   1]]\n",
            "[[191  71 584 473   2]]\n",
            "[[  1  21 194 223   0]]\n",
            "[[150  42 583 471   0]]\n",
            "epoch: 9 step:   55/60, lr:0.000060, giou_loss:   0.91, conf_loss:  27.38, prob_loss:   1.56, total_loss:  29.85\n",
            "[[  6   6 329 340   0]]\n",
            "[[ 47   0 551 527   0]]\n",
            "[[282  37 479 807   1]]\n",
            "[[ 32  87 488 573   2]]\n",
            "epoch: 9 step:   56/60, lr:0.000060, giou_loss:   1.10, conf_loss:  27.16, prob_loss:   0.93, total_loss:  29.20\n",
            "[[287  74 529 329   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "[[202  61 445 284   2]]\n",
            "[[ 607  133 1030  540    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "[[ 36  17 288 276   2]]\n",
            "epoch: 9 step:   57/60, lr:0.000060, giou_loss:   1.30, conf_loss:  29.39, prob_loss:   3.61, total_loss:  34.29\n",
            "[[ 27  24 124 129   2]]\n",
            "[[ 481    1 1439  741    2  822  643 1620 1066    2]]\n",
            "[[  5 108 181 297   0]]\n",
            "[[356  72 906 624   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "epoch: 9 step:   58/60, lr:0.000059, giou_loss:   1.06, conf_loss:  28.00, prob_loss:   1.21, total_loss:  30.27\n",
            "[[ 28   1 113  95   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "[[248  75 684 312   1]]\n",
            "[[ 30  36 203 208   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "[[ 270   18 1073  839    0    1    1  407  617    0  104    4  624  557\n",
            "     0]]\n",
            "epoch: 9 step:   59/60, lr:0.000059, giou_loss:   1.26, conf_loss:  27.68, prob_loss:   0.76, total_loss:  29.69\n",
            "[[104 108 491 301   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[  19  135 1199  601    1]]\n",
            "[[104 145 246 290   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "[[168  23 347 195   0]]\n",
            "epoch: 9 step:    0/60, lr:0.000059, giou_loss:   1.70, conf_loss:  28.65, prob_loss:   1.70, total_loss:  32.04\n",
            "[[ 75  20 197 143   2 139   5 275 149   2]]\n",
            "[[  83  237 2539 4127    1]]\n",
            "[[271  49 607 419   2 430  58 759 380   2]]\n",
            "[[ 68  17 247 187   2]]\n",
            "epoch: 9 step:    1/60, lr:0.000059, giou_loss:   1.08, conf_loss:  27.51, prob_loss:   1.88, total_loss:  30.46\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[279  14 562 295   2]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[145  36 421 286   2]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[166  92 638 559   2]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[155 105 453 436   0]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[150 165 923 983   2]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[203 115 584 452   1]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[133 165 922 525   1]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "[[111  51 372 277   2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "\n",
            "\n",
            "giou_val_loss:   1.49, conf_val_loss:  30.66, prob_val_loss:   2.92, total_val_loss:  35.07\n",
            "\n",
            "\n",
            "[[ 90  16 205 115   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "[[  49   34 1863  957    1]]\n",
            "[[ 81  91 535 547   2]]\n",
            "[[249 106 856 709   0]]\n",
            "epoch:10 step:    2/60, lr:0.000059, giou_loss:   1.46, conf_loss:  28.92, prob_loss:   1.36, total_loss:  31.74\n",
            "[[ 791  317 1170  666    0]]\n",
            "[[165  86 377 291   2]]\n",
            "[[ 59 205 410 578   0 363  83 770 514   0]]\n",
            "[[ 94   3 492 418   0]]\n",
            "epoch:10 step:    3/60, lr:0.000059, giou_loss:   1.18, conf_loss:  27.26, prob_loss:   1.41, total_loss:  29.84\n",
            "[[ 379  826 1005 1964    1]]\n",
            "[[223  19 584 310   2]]\n",
            "[[ 97 127 346 379   0 314 112 594 385   0]]\n",
            "[[ 38   8 217 180   0]]\n",
            "epoch:10 step:    4/60, lr:0.000059, giou_loss:   1.61, conf_loss:  28.97, prob_loss:   5.49, total_loss:  36.07\n",
            "[[ 10   8 207 778   1]]\n",
            "[[198  30 431 249   2]]\n",
            "[[106   7 228 130   2 139   5 275 149   2]]\n",
            "[[393  42 858 498   2]]\n",
            "epoch:10 step:    5/60, lr:0.000058, giou_loss:   0.98, conf_loss:  27.17, prob_loss:   1.23, total_loss:  29.38\n",
            "[[ 37   1 266 225   2]]\n",
            "[[202  37 470 314   0]]\n",
            "[[ 50   5 239 199   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "[[  38  189 1017 1184    0]]\n",
            "epoch:10 step:    6/60, lr:0.000058, giou_loss:   0.80, conf_loss:  26.94, prob_loss:   0.70, total_loss:  28.44\n",
            "[[  6   2 206 190   0]]\n",
            "[[ 184  221 1226  712    1   68  224 1011  751    1]]\n",
            "[[  1  96  83 184   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "[[  2  86 139 245   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "epoch:10 step:    7/60, lr:0.000058, giou_loss:   1.16, conf_loss:  29.36, prob_loss:   1.24, total_loss:  31.76\n",
            "[[ 56  35 182 175   0]]\n",
            "[[ 326  229 1059 1004    2  580   21 1273  753    2]]\n",
            "[[ 15  55 419 199   1]]\n",
            "[[1223  217 2688 1791    2]]\n",
            "epoch:10 step:    8/60, lr:0.000058, giou_loss:   0.95, conf_loss:  26.73, prob_loss:   0.72, total_loss:  28.39\n",
            "[[  2   6 433 447   2]]\n",
            "[[   1  685 2457 4575    1]]\n",
            "[[121  64 423 378   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "[[468 158 924 644   2]]\n",
            "epoch:10 step:    9/60, lr:0.000058, giou_loss:   1.10, conf_loss:  27.38, prob_loss:   3.81, total_loss:  32.30\n",
            "[[ 76   6 173 111   2]]\n",
            "[[121 161 588 436   1 190 100 623 330   1]]\n",
            "[[  1  13 194 215   0]]\n",
            "[[ 568  282 1185  902    2  990  370 1578  974    2]]\n",
            "epoch:10 step:   10/60, lr:0.000058, giou_loss:   1.25, conf_loss:  26.66, prob_loss:   0.72, total_loss:  28.63\n",
            "[[280 146 642 511   0]]\n",
            "[[ 46  48 309 336   0  79  18 338 270   0]]\n",
            "[[ 30  14 140 124   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[ 12  23 492 227   1]]\n",
            "epoch:10 step:   11/60, lr:0.000058, giou_loss:   1.68, conf_loss:  27.60, prob_loss:   2.44, total_loss:  31.72\n",
            "[[ 81  10 781 717   0]]\n",
            "[[   0   81 1867 1901    2]]\n",
            "[[221  83 463 338   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "[[297 202 716 600   2 664 128 919 369   2]]\n",
            "epoch:10 step:   12/60, lr:0.000057, giou_loss:   1.13, conf_loss:  27.14, prob_loss:   1.48, total_loss:  29.75\n",
            "[[218   2 651 327   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[112 137 515 545   2]]\n",
            "[[139   7 367 240   2]]\n",
            "[[163  47 432 311   0]]\n",
            "epoch:10 step:   13/60, lr:0.000057, giou_loss:   0.91, conf_loss:  26.28, prob_loss:   0.66, total_loss:  27.85\n",
            "[[  2   9 320 313   1]]\n",
            "[[  47   11 1360 1078    1]]\n",
            "[[340 258 652 546   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[ 84   1 447 348   0]]\n",
            "epoch:10 step:   14/60, lr:0.000057, giou_loss:   0.82, conf_loss:  26.72, prob_loss:   1.32, total_loss:  28.86\n",
            "[[ 29  45 225 228   0]]\n",
            "[[130  61 276 204   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "[[ 130  800  764 1800    1  546  621 1071 1679    1]]\n",
            "[[ 13  29 261 285   2]]\n",
            "epoch:10 step:   15/60, lr:0.000057, giou_loss:   0.99, conf_loss:  27.21, prob_loss:   0.87, total_loss:  29.07\n",
            "[[ 49 186 417 575   2]]\n",
            "[[ 14  82 344 414   2]]\n",
            "[[ 41  35 210 190   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "[[ 88  10 638 562   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "epoch:10 step:   16/60, lr:0.000057, giou_loss:   0.97, conf_loss:  26.58, prob_loss:   2.57, total_loss:  30.13\n",
            "[[ 19  62 357 423   2  58  69 410 432   2]]\n",
            "[[353 124 472 239   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "[[ 733  247  997  499    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "[[   8  217 1581  840    1]]\n",
            "epoch:10 step:   17/60, lr:0.000057, giou_loss:   2.02, conf_loss:  29.04, prob_loss:   3.42, total_loss:  34.48\n",
            "[[114   7 573 257   1]]\n",
            "[[ 54   1 558 528   0]]\n",
            "[[ 61  73 740 821   0]]\n",
            "[[  35  211 1006  995    1  445  447 1069 1046    0]]\n",
            "epoch:10 step:   18/60, lr:0.000057, giou_loss:   0.85, conf_loss:  26.69, prob_loss:   3.60, total_loss:  31.14\n",
            "[[ 28  33 277 265   2]]\n",
            "[[ 36 121 593 355   1]]\n",
            "[[223 207 646 629   0 401 115 725 441   0]]\n",
            "[[268 166 564 438   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "epoch:10 step:   19/60, lr:0.000056, giou_loss:   1.21, conf_loss:  26.23, prob_loss:   0.60, total_loss:  28.05\n",
            "[[213  33 459 258   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "[[356 105 526 393   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[171 236 859 756   1   4 236 433 756   1]]\n",
            "[[575  19 995 395   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "epoch:10 step:   20/60, lr:0.000056, giou_loss:   1.72, conf_loss:  27.89, prob_loss:   3.03, total_loss:  32.65\n",
            "[[  3  61 342 263   1]]\n",
            "[[114 289 290 459   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "[[291  55 588 342   0]]\n",
            "[[ 21  11 692 268   1]]\n",
            "epoch:10 step:   21/60, lr:0.000056, giou_loss:   1.52, conf_loss:  26.65, prob_loss:   2.08, total_loss:  30.26\n",
            "[[ 95  63 597 266   1]]\n",
            "[[  46  119  810  461    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "[[  8  15 331 349   0]]\n",
            "[[  10    1 1517  777    1]]\n",
            "epoch:10 step:   22/60, lr:0.000056, giou_loss:   0.90, conf_loss:  26.08, prob_loss:   1.32, total_loss:  28.30\n",
            "[[ 40  24 250 286   1]]\n",
            "[[ 43 135 219 324   0]]\n",
            "[[ 485   43  908  469    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "[[ 64   7 507 402   1]]\n",
            "epoch:10 step:   23/60, lr:0.000056, giou_loss:   1.14, conf_loss:  26.43, prob_loss:   0.78, total_loss:  28.34\n",
            "[[ 50  73 241 262   2 328 173 488 351   2]]\n",
            "[[ 661  111 1310  815    0]]\n",
            "[[ 57 120 143 208   2  83 107 161 198   2]]\n",
            "[[ 41   7 403 334   2]]\n",
            "epoch:10 step:   24/60, lr:0.000056, giou_loss:   1.00, conf_loss:  26.54, prob_loss:   1.20, total_loss:  28.74\n",
            "[[ 43  21 350 353   0]]\n",
            "[[ 41   7 367 282   0]]\n",
            "[[ 497    2 1075  514    2]]\n",
            "[[ 608  338 1033  747    2]]\n",
            "epoch:10 step:   25/60, lr:0.000056, giou_loss:   1.06, conf_loss:  26.66, prob_loss:   2.81, total_loss:  30.53\n",
            "[[ 31  27 633 480   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[ 70  31 583 543   2]]\n",
            "[[  3  19 259 275   0]]\n",
            "[[ 371   98 1018  462    1]]\n",
            "epoch:10 step:   26/60, lr:0.000055, giou_loss:   0.65, conf_loss:  25.25, prob_loss:   0.44, total_loss:  26.34\n",
            "[[301 155 659 513   2  77 133 674 590   1]]\n",
            "[[192  97 788 764   1]]\n",
            "[[311  14 569 262   2 357  35 656 334   2]]\n",
            "[[ 66  99 499 528   0]]\n",
            "epoch:10 step:   27/60, lr:0.000055, giou_loss:   0.83, conf_loss:  25.70, prob_loss:   0.82, total_loss:  27.35\n",
            "[[125  36 267 181   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "[[342  83 701 416   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[103  34 414 305   1 100  65 361 353   1]]\n",
            "[[ 62 103 398 466   2]]\n",
            "epoch:10 step:   28/60, lr:0.000055, giou_loss:   1.25, conf_loss:  27.83, prob_loss:   3.01, total_loss:  32.10\n",
            "[[262 608 427 803   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[602 238 881 519   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "[[ 20  13 188 183   0]]\n",
            "[[ 334    7 1289  643    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "epoch:10 step:   29/60, lr:0.000055, giou_loss:   1.84, conf_loss:  27.59, prob_loss:   1.95, total_loss:  31.38\n",
            "[[ 54   8 209 155   0]]\n",
            "[[  19  135 1199  601    1]]\n",
            "[[ 21 457 232 653   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[ 86  54 638 328   1]]\n",
            "epoch:10 step:   30/60, lr:0.000055, giou_loss:   1.11, conf_loss:  26.06, prob_loss:   0.90, total_loss:  28.07\n",
            "[[282 100 827 617   2]]\n",
            "[[122  41 625 287   1]]\n",
            "[[ 25  26 226 231   2]]\n",
            "[[  2   7 149 151   0]]\n",
            "epoch:10 step:   31/60, lr:0.000055, giou_loss:   0.60, conf_loss:  25.16, prob_loss:   0.87, total_loss:  26.63\n",
            "[[209  48 432 262   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "[[  6 159 432 373   1]]\n",
            "[[ 58  39 582 397   1]]\n",
            "[[349 227 861 784   0]]\n",
            "epoch:10 step:   32/60, lr:0.000055, giou_loss:   1.16, conf_loss:  26.07, prob_loss:   1.97, total_loss:  29.19\n",
            "[[ 68  84 552 374   1]]\n",
            "[[  8  19 194 201   0]]\n",
            "[[  6  60 299 176   1]]\n",
            "[[322  60 546 291   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "epoch:10 step:   33/60, lr:0.000054, giou_loss:   0.81, conf_loss:  25.91, prob_loss:   0.83, total_loss:  27.55\n",
            "[[ 55  77 238 267   2]]\n",
            "[[177  66 540 555   1]]\n",
            "[[ 352   80 1159 1019    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "[[674   9 957 279   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "epoch:10 step:   34/60, lr:0.000054, giou_loss:   1.58, conf_loss:  25.67, prob_loss:   4.03, total_loss:  31.28\n",
            "[[253  87 426 259   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "[[  5  82 297 198   1]]\n",
            "[[  5  63 240 307   0]]\n",
            "[[152  10 384 180   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "epoch:10 step:   35/60, lr:0.000054, giou_loss:   1.05, conf_loss:  25.60, prob_loss:   0.99, total_loss:  27.64\n",
            "[[ 48  58 225 221   0]]\n",
            "[[520 142 684 292   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "[[150   7 812 710   2]]\n",
            "[[ 64  28 262 219   2 214  75 383 248   2]]\n",
            "epoch:10 step:   36/60, lr:0.000054, giou_loss:   1.21, conf_loss:  26.10, prob_loss:   3.66, total_loss:  30.97\n",
            "[[189 239 673 715   0   1 132 368 605   0]]\n",
            "[[  29  181  452  588    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "[[140  26 527 219   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[308 261 889 666   1  24 174 599 657   1]]\n",
            "epoch:10 step:   37/60, lr:0.000054, giou_loss:   1.26, conf_loss:  26.97, prob_loss:   1.31, total_loss:  29.54\n",
            "[[  8  28 248 232   0]]\n",
            "[[  65  134 1422 1454    0]]\n",
            "[[ 43  47 375 376   2]]\n",
            "[[ 10 272 338 596   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "epoch:10 step:   38/60, lr:0.000054, giou_loss:   0.87, conf_loss:  25.84, prob_loss:   2.88, total_loss:  29.60\n",
            "[[218  36 543 365   0 304  62 575 353   0]]\n",
            "[[ 147  381 1001 1259    0]]\n",
            "[[ 18  30 172 175   2]]\n",
            "[[  6  19 537 524   2]]\n",
            "epoch:10 step:   39/60, lr:0.000054, giou_loss:   0.94, conf_loss:  25.47, prob_loss:   1.69, total_loss:  28.10\n",
            "[[209 125 455 386   2]]\n",
            "[[ 33  60 313 361   2]]\n",
            "[[ 12  45 153 202   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[ 38  27 216 205   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "epoch:10 step:   40/60, lr:0.000053, giou_loss:   0.91, conf_loss:  25.39, prob_loss:   2.79, total_loss:  29.08\n",
            "[[ 87  29 543 398   2]]\n",
            "[[ 728    5 1077  373    2  389  337  818  688    2  621  277  965  620\n",
            "     2  365   61  805  419    2]]\n",
            "[[ 102  264  942 1100    2]]\n",
            "[[ 353  305 1232 1191    2]]\n",
            "epoch:10 step:   41/60, lr:0.000053, giou_loss:   0.89, conf_loss:  25.62, prob_loss:   1.98, total_loss:  28.49\n",
            "[[ 11  12 446 454   0]]\n",
            "[[302 167 469 344   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "[[ 70  15 257 217   2]]\n",
            "[[ 42  85 416 463   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "epoch:10 step:   42/60, lr:0.000053, giou_loss:   0.82, conf_loss:  25.12, prob_loss:   1.38, total_loss:  27.32\n",
            "[[ 76 116 180 223   2]]\n",
            "[[ 283  244  855 1244    1]]\n",
            "[[ 42  22 355 148   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "[[258  30 684 361   1]]\n",
            "epoch:10 step:   43/60, lr:0.000053, giou_loss:   1.37, conf_loss:  26.61, prob_loss:   2.76, total_loss:  30.73\n",
            "[[142  77 338 322   1]]\n",
            "[[ 69  43 218 258   1]]\n",
            "[[ 635  554 1238 1165    2]]\n",
            "[[ 13 104 134 247   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "epoch:10 step:   44/60, lr:0.000053, giou_loss:   0.84, conf_loss:  25.17, prob_loss:   1.09, total_loss:  27.10\n",
            "[[415 261 720 567   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[ 25  46 495 488   0]]\n",
            "[[ 14  15 321 313   2]]\n",
            "[[ 94   7 381 293   0]]\n",
            "epoch:10 step:   45/60, lr:0.000053, giou_loss:   0.76, conf_loss:  24.76, prob_loss:   2.74, total_loss:  28.26\n",
            "[[ 48 112 288 356   2 291  81 545 348   2]]\n",
            "[[286 119 596 435   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[192  44 508 337   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "[[ 17 154 436 348   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "epoch:10 step:   46/60, lr:0.000053, giou_loss:   1.03, conf_loss:  25.58, prob_loss:   0.82, total_loss:  27.42\n",
            "[[171   4 356  96   1 305  81 489 175   1]]\n",
            "[[  0   2 405 344   1]]\n",
            "[[ 25  42 275 297   0]]\n",
            "[[ 43  67 220 234   0]]\n",
            "epoch:10 step:   47/60, lr:0.000052, giou_loss:   0.80, conf_loss:  25.38, prob_loss:   0.76, total_loss:  26.94\n",
            "[[  5  21 287 266   0]]\n",
            "[[ 207  161  729  495    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "[[   4  364  757 1046    0]]\n",
            "[[ 317  168 1424  923    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "epoch:10 step:   48/60, lr:0.000052, giou_loss:   1.55, conf_loss:  25.64, prob_loss:   4.27, total_loss:  31.46\n",
            "[[  1  37 253 296   2]]\n",
            "[[ 706  257 1440  978    2  172  181  860  843    2]]\n",
            "[[ 795  369 1173  780    2  147   48  869  476    1  169  180  839  798\n",
            "     1]]\n",
            "[[202 188 372 366   0 197 144 363 328   0]]\n",
            "epoch:10 step:   49/60, lr:0.000052, giou_loss:   1.14, conf_loss:  25.85, prob_loss:   1.03, total_loss:  28.01\n",
            "[[ 65  18 693 192   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "[[ 75  21 563 328   1]]\n",
            "[[  26  188 1680 1085    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[ 56 113 278 332   2 213  98 413 320   2]]\n",
            "epoch:10 step:   50/60, lr:0.000052, giou_loss:   1.10, conf_loss:  25.72, prob_loss:   2.46, total_loss:  29.28\n",
            "[[ 20  77 426 496   0]]\n",
            "[[  12  310 2744 2793    1]]\n",
            "[[  97  108 1203 1135    0]]\n",
            "[[ 20  53 189 161   1]]\n",
            "epoch:10 step:   51/60, lr:0.000052, giou_loss:   0.52, conf_loss:  25.23, prob_loss:   0.84, total_loss:  26.59\n",
            "[[202  61 445 284   2]]\n",
            "[[499  68 732 335   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "[[ 248  368 1649 1759    2]]\n",
            "[[ 324  499 1127 1320    0    1    1  407  617    0  104    4  624  557\n",
            "     0]]\n",
            "epoch:10 step:   52/60, lr:0.000052, giou_loss:   1.49, conf_loss:  25.38, prob_loss:   2.13, total_loss:  29.00\n",
            "[[ 50  67 314 380   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "[[ 38 107 179 348   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[128  83 456 385   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "[[ 71  58 635 638   0  78 176 590 665   0]]\n",
            "epoch:10 step:   53/60, lr:0.000052, giou_loss:   1.25, conf_loss:  25.29, prob_loss:   3.06, total_loss:  29.59\n",
            "[[ 29   4 465 241   1]]\n",
            "[[ 31  47 424 449   2]]\n",
            "[[ 263  226 1068  439    1]]\n",
            "[[ 45  22 346 342   2]]\n",
            "epoch:10 step:   54/60, lr:0.000051, giou_loss:   0.77, conf_loss:  24.74, prob_loss:   2.50, total_loss:  28.01\n",
            "[[  0  76 296 394   0]]\n",
            "[[ 24  18 198 193   0  18  22 168 190   0]]\n",
            "[[ 26  42 324 330   0]]\n",
            "[[556 178 950 559   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "epoch:10 step:   55/60, lr:0.000051, giou_loss:   0.84, conf_loss:  25.74, prob_loss:   2.30, total_loss:  28.88\n",
            "[[173  93 429 323   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "[[137  92 456 204   1]]\n",
            "[[ 84  14 263 184   2]]\n",
            "[[  2  59 240 337   2 347 127 603 398   2]]\n",
            "epoch:10 step:   56/60, lr:0.000051, giou_loss:   1.26, conf_loss:  25.71, prob_loss:   1.34, total_loss:  28.31\n",
            "[[130  51 892 786   2]]\n",
            "[[  5  12 279 279   2]]\n",
            "[[197   5 461 277   2]]\n",
            "[[ 85 239 505 681   0]]\n",
            "epoch:10 step:   57/60, lr:0.000051, giou_loss:   0.84, conf_loss:  24.75, prob_loss:   1.88, total_loss:  27.47\n",
            "[[ 78 263 980 741   1]]\n",
            "[[ 99  10 568 344   0   3  40 475 439   0]]\n",
            "[[206 179 388 390   0]]\n",
            "[[ 168    1 1126  741    2  822  643 1620 1066    2]]\n",
            "epoch:10 step:   58/60, lr:0.000051, giou_loss:   0.94, conf_loss:  23.72, prob_loss:   1.13, total_loss:  25.79\n",
            "[[ 76  69 412 439   2 430  58 759 380   2]]\n",
            "[[ 27  33 112 127   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "[[ 68  35 491 364   1]]\n",
            "[[ 41  13 535 260   1 272  80 498 306   1]]\n",
            "epoch:10 step:   59/60, lr:0.000051, giou_loss:   0.76, conf_loss:  24.40, prob_loss:   1.27, total_loss:  26.43\n",
            "[[233 137 442 358   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "[[178   4 566 179   1]]\n",
            "[[ 14 254 416 343   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "[[149 125 768 739   0]]\n",
            "epoch:10 step:    0/60, lr:0.000050, giou_loss:   1.08, conf_loss:  24.72, prob_loss:   0.73, total_loss:  26.52\n",
            "[[208 149 432 391   2]]\n",
            "[[ 70  25 290 226   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "[[ 20  33 227 241   0]]\n",
            "[[ 15  21 907 508   1]]\n",
            "epoch:10 step:    1/60, lr:0.000050, giou_loss:   1.14, conf_loss:  25.04, prob_loss:   1.79, total_loss:  27.96\n",
            "[[ 19  68 278 343   0]]\n",
            "[[143  40 415 346   0]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[203 115 584 452   1]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[145  36 421 286   2]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[133 165 922 525   1]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[111  51 372 277   2]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[166  92 638 559   2]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[150 165 923 983   2]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[279  14 562 295   2]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[155 105 453 436   0]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "\n",
            "\n",
            "giou_val_loss:    nan, conf_val_loss:  27.64, prob_val_loss:   1.90, total_val_loss:    nan\n",
            "\n",
            "\n",
            "[[ 107  154 2839 2637    1]]\n",
            "[[ 13   1 269 257   0]]\n",
            "[[  2  85 307 391   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[ 287  233 1141 1111    0]]\n",
            "epoch:11 step:    2/60, lr:0.000050, giou_loss:   0.85, conf_loss:  24.27, prob_loss:   1.95, total_loss:  27.07\n",
            "[[ 92 150 711 764   0]]\n",
            "[[422   4 718 276   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "[[  2  86 282 387   2]]\n",
            "[[ 82  13 541 263   1]]\n",
            "epoch:11 step:    3/60, lr:0.000050, giou_loss:   1.22, conf_loss:  24.92, prob_loss:   1.50, total_loss:  27.63\n",
            "[[  63  105 1570  881    1]]\n",
            "[[142  37 470 339   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "[[  1   5 283 250   0]]\n",
            "[[ 16   8 171 155   0]]\n",
            "epoch:11 step:    4/60, lr:0.000050, giou_loss:   1.04, conf_loss:  24.27, prob_loss:   2.24, total_loss:  27.55\n",
            "[[ 118  128  541  554    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "[[127 101 530 509   2]]\n",
            "[[ 13  18 419 437   0]]\n",
            "[[167  29 346 201   0]]\n",
            "epoch:11 step:    5/60, lr:0.000050, giou_loss:   0.70, conf_loss:  24.28, prob_loss:   0.79, total_loss:  25.76\n",
            "[[ 325   35 1004  783    0]]\n",
            "[[  80  104 1947 1924    2]]\n",
            "[[159  45 520 336   2]]\n",
            "[[115 100 280 295   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "epoch:11 step:    6/60, lr:0.000050, giou_loss:   1.10, conf_loss:  24.74, prob_loss:   1.46, total_loss:  27.30\n",
            "[[ 21  49 198 216   0]]\n",
            "[[ 568   46 1146  558    2]]\n",
            "[[174 204 502 528   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "[[ 39  63 235 246   0]]\n",
            "epoch:11 step:    7/60, lr:0.000049, giou_loss:   0.84, conf_loss:  24.33, prob_loss:   1.22, total_loss:  26.40\n",
            "[[159  25 484 354   0 304  62 575 353   0]]\n",
            "[[ 109    1 1067  741    2  822  643 1620 1066    2]]\n",
            "[[152  18 350 209   2 214  75 383 248   2]]\n",
            "[[ 86 223 277 412   2 328 173 488 351   2]]\n",
            "epoch:11 step:    8/60, lr:0.000049, giou_loss:   1.09, conf_loss:  24.54, prob_loss:   2.26, total_loss:  27.90\n",
            "[[119  72 613 319   1 272  80 498 306   1]]\n",
            "[[ 44 126 946 604   1]]\n",
            "[[ 19   1 507 308   1]]\n",
            "[[119  85 457 446   2  58  69 410 432   2]]\n",
            "epoch:11 step:    9/60, lr:0.000049, giou_loss:   1.24, conf_loss:  24.23, prob_loss:   3.36, total_loss:  28.83\n",
            "[[385  38 664 319   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "[[125 133 772 497   1]]\n",
            "[[ 29   1 452 330   1]]\n",
            "[[ 491  212 1013  546    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "epoch:11 step:   10/60, lr:0.000049, giou_loss:   1.80, conf_loss:  26.19, prob_loss:   2.32, total_loss:  30.31\n",
            "[[  5  14 298 130   1]]\n",
            "[[234 160 412 338   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "[[   7  176  740  951    2  580   21 1273  753    2]]\n",
            "[[ 83 245 553 687   0]]\n",
            "epoch:11 step:   11/60, lr:0.000049, giou_loss:   0.95, conf_loss:  23.05, prob_loss:   0.43, total_loss:  24.43\n",
            "[[231  57 547 350   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "[[ 90  26 574 316   1]]\n",
            "[[ 93  79 272 249   2]]\n",
            "[[322 441 716 822   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "epoch:11 step:   12/60, lr:0.000049, giou_loss:   0.81, conf_loss:  23.59, prob_loss:   2.01, total_loss:  26.41\n",
            "[[174 101 576 190   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "[[ 14 103 350 466   2]]\n",
            "[[   4    3 1818  926    1]]\n",
            "[[ 19  19 351 348   2]]\n",
            "epoch:11 step:   13/60, lr:0.000049, giou_loss:   1.05, conf_loss:  24.11, prob_loss:   2.28, total_loss:  27.44\n",
            "[[  8 114 230 333   2 213  98 413 320   2]]\n",
            "[[  6  18 206 206   0]]\n",
            "[[ 17  36 227 298   1]]\n",
            "[[ 75  87 251 276   0]]\n",
            "epoch:11 step:   14/60, lr:0.000048, giou_loss:   0.41, conf_loss:  23.66, prob_loss:   0.72, total_loss:  24.79\n",
            "[[ 38  43 339 363   2]]\n",
            "[[103  57 699 724   1]]\n",
            "[[ 30  44 465 486   0]]\n",
            "[[ 35  96 268 315   2]]\n",
            "epoch:11 step:   15/60, lr:0.000048, giou_loss:   0.57, conf_loss:  23.14, prob_loss:   0.52, total_loss:  24.23\n",
            "[[136 117 240 224   2]]\n",
            "[[136 107 379 330   2]]\n",
            "[[151  18 397 243   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "[[108  11 434 286   0]]\n",
            "epoch:11 step:   16/60, lr:0.000048, giou_loss:   1.37, conf_loss:  24.25, prob_loss:   1.21, total_loss:  26.84\n",
            "[[395 113 510 212   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "[[188  38 631 433   1]]\n",
            "[[  4 216 237 483   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "[[ 40  71 226 253   0]]\n",
            "epoch:11 step:   17/60, lr:0.000048, giou_loss:   1.50, conf_loss:  24.37, prob_loss:   2.39, total_loss:  28.26\n",
            "[[546 340 766 541   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "[[ 97  62 366 326   0]]\n",
            "[[  8  15 331 349   0]]\n",
            "[[115  62 212 167   2]]\n",
            "epoch:11 step:   18/60, lr:0.000048, giou_loss:   1.11, conf_loss:  23.55, prob_loss:   3.58, total_loss:  28.24\n",
            "[[ 55  24 177 147   2 139   5 275 149   2]]\n",
            "[[ 56  12 663 615   0]]\n",
            "[[ 435   46  858  453    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "[[  8  23 155 167   0]]\n",
            "epoch:11 step:   19/60, lr:0.000048, giou_loss:   0.81, conf_loss:  24.75, prob_loss:   1.76, total_loss:  27.32\n",
            "[[  3 134  85 222   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "[[ 43  17 217 192   0  18  22 168 190   0]]\n",
            "[[140 300 489 668   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "[[192 104 617 513   2]]\n",
            "epoch:11 step:   20/60, lr:0.000048, giou_loss:   1.32, conf_loss:  26.73, prob_loss:   1.73, total_loss:  29.78\n",
            "[[ 80  48 221 289   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[109  48 633 406   1]]\n",
            "[[  2  23 407 365   1]]\n",
            "[[  2  60 421 254   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "epoch:11 step:   21/60, lr:0.000047, giou_loss:   1.59, conf_loss:  24.40, prob_loss:   4.85, total_loss:  30.85\n",
            "[[263 193 501 471   2 347 127 603 398   2]]\n",
            "[[ 50 135 232 346   0]]\n",
            "[[104  71 434 403   2]]\n",
            "[[251  18 614 507   1]]\n",
            "epoch:11 step:   22/60, lr:0.000047, giou_loss:   0.87, conf_loss:  23.39, prob_loss:   1.53, total_loss:  25.79\n",
            "[[  65   86 1422 1406    0]]\n",
            "[[ 38   2 312 269   2]]\n",
            "[[ 77  49 254 212   0]]\n",
            "[[ 62  78 238 248   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "epoch:11 step:   23/60, lr:0.000047, giou_loss:   0.84, conf_loss:  23.69, prob_loss:   1.22, total_loss:  25.75\n",
            "[[ 257  388 1228 1172    1  445  447 1069 1046    0]]\n",
            "[[ 66  44 453 237   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[228 258 877 962   0]]\n",
            "[[1426   68 1998 1068    1]]\n",
            "epoch:11 step:   24/60, lr:0.000047, giou_loss:   1.15, conf_loss:  23.82, prob_loss:   4.00, total_loss:  28.97\n",
            "[[ 28   7 278 262   0]]\n",
            "[[134 208 896 943   2]]\n",
            "[[173 208 346 380   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "[[ 24  95 504 299   1]]\n",
            "epoch:11 step:   25/60, lr:0.000047, giou_loss:   1.01, conf_loss:  23.05, prob_loss:   0.74, total_loss:  24.81\n",
            "[[177  52 374 822   1]]\n",
            "[[  7  65 400 467   2]]\n",
            "[[143   9 541 424   0]]\n",
            "[[144  38 390 299   2]]\n",
            "epoch:11 step:   26/60, lr:0.000047, giou_loss:   0.81, conf_loss:  22.89, prob_loss:   1.72, total_loss:  25.42\n",
            "[[121 161 588 436   1 190 100 623 330   1]]\n",
            "[[299 124 539 368   2 291  81 545 348   2]]\n",
            "[[ 158  116 1264 1143    0]]\n",
            "[[ 15  10 141 150   0]]\n",
            "epoch:11 step:   27/60, lr:0.000047, giou_loss:   0.82, conf_loss:  23.92, prob_loss:   0.73, total_loss:  25.47\n",
            "[[111  89 447 459   2 430  58 759 380   2]]\n",
            "[[ 24  12 342 316   1]]\n",
            "[[ 23  45 477 501   2]]\n",
            "[[ 23 373 315 489   1]]\n",
            "epoch:11 step:   28/60, lr:0.000046, giou_loss:   1.06, conf_loss:  23.83, prob_loss:   1.13, total_loss:  26.02\n",
            "[[ 340  219  966 1357    1]]\n",
            "[[ 56  58 489 383   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[  4 148 561 382   1]]\n",
            "[[ 88 123 225 282   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "epoch:11 step:   29/60, lr:0.000046, giou_loss:   1.93, conf_loss:  25.38, prob_loss:   4.47, total_loss:  31.78\n",
            "[[ 409   58 1249  894    2]]\n",
            "[[102  45 314 250   2]]\n",
            "[[179  38 414 282   0]]\n",
            "[[  56  194 1457 1585    2]]\n",
            "epoch:11 step:   30/60, lr:0.000046, giou_loss:   0.60, conf_loss:  23.30, prob_loss:   1.26, total_loss:  25.15\n",
            "[[ 47 224 531 700   0   1 132 368 605   0]]\n",
            "[[  1 213 427 427   1]]\n",
            "[[221  83 463 338   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "[[ 27   2 334 300   2]]\n",
            "epoch:11 step:   31/60, lr:0.000046, giou_loss:   0.98, conf_loss:  23.21, prob_loss:   0.65, total_loss:  24.84\n",
            "[[100  21 407 353   0]]\n",
            "[[ 92  48 405 174   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "[[167  52 450 322   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[349  51 651 365   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "epoch:11 step:   32/60, lr:0.000046, giou_loss:   1.23, conf_loss:  23.41, prob_loss:   3.53, total_loss:  28.17\n",
            "[[ 141  143 1248  898    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "[[377   6 541 156   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "[[ 27  17 325 305   0]]\n",
            "[[188 162 485 449   0]]\n",
            "epoch:11 step:   33/60, lr:0.000046, giou_loss:   1.11, conf_loss:  23.43, prob_loss:   2.61, total_loss:  27.14\n",
            "[[ 77   5 260 195   2]]\n",
            "[[113   8 409 326   0]]\n",
            "[[ 14   8 278 280   2]]\n",
            "[[147 113 317 291   0 197 144 363 328   0]]\n",
            "epoch:11 step:   34/60, lr:0.000046, giou_loss:   0.98, conf_loss:  23.45, prob_loss:   0.74, total_loss:  25.18\n",
            "[[319  51 582 339   0  79  18 338 270   0]]\n",
            "[[ 28  46 239 242   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[  74  221 1116  712    1   68  224 1011  751    1]]\n",
            "[[142  77 309 254   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "epoch:11 step:   35/60, lr:0.000045, giou_loss:   1.00, conf_loss:  24.56, prob_loss:   1.29, total_loss:  26.85\n",
            "[[ 58  36 571 548   2]]\n",
            "[[ 22  19 270 275   2]]\n",
            "[[359 189 733 567   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "[[ 59  64 228 172   1]]\n",
            "epoch:11 step:   36/60, lr:0.000045, giou_loss:   0.81, conf_loss:  22.97, prob_loss:   1.37, total_loss:  25.16\n",
            "[[ 28  37 251 251   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "[[460  36 724 349   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "[[ 13   1  98  95   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "[[  54  426  857 1247    0    1    1  407  617    0  104    4  624  557\n",
            "     0]]\n",
            "epoch:11 step:   37/60, lr:0.000045, giou_loss:   1.05, conf_loss:  22.89, prob_loss:   1.06, total_loss:  24.99\n",
            "[[ 54  25 807 707   0]]\n",
            "[[  2  12 156 157   2]]\n",
            "[[ 59  96 571 653   0]]\n",
            "[[463   5 842 354   0]]\n",
            "epoch:11 step:   38/60, lr:0.000045, giou_loss:   1.15, conf_loss:  23.27, prob_loss:   1.78, total_loss:  26.20\n",
            "[[ 27  72 463 309   1]]\n",
            "[[  4 128 675 385   1]]\n",
            "[[586 248 954 637   2]]\n",
            "[[ 576   54 1210 1054    1  546  621 1071 1679    1]]\n",
            "epoch:11 step:   39/60, lr:0.000045, giou_loss:   1.23, conf_loss:  23.42, prob_loss:   3.42, total_loss:  28.06\n",
            "[[ 87 114 315 347   2]]\n",
            "[[164  36 595 477   2]]\n",
            "[[218  62 442 304   2]]\n",
            "[[461 197 881 573   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "epoch:11 step:   40/60, lr:0.000045, giou_loss:   0.88, conf_loss:  22.83, prob_loss:   1.99, total_loss:  25.70\n",
            "[[174   1 320 144   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "[[   1  135 1181  601    1]]\n",
            "[[1103  182 2568 1756    2]]\n",
            "[[168  84 337 239   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "epoch:11 step:   41/60, lr:0.000045, giou_loss:   0.90, conf_loss:  22.26, prob_loss:   0.56, total_loss:  23.72\n",
            "[[ 47  11 366 123   1]]\n",
            "[[316   7 978 710   2]]\n",
            "[[ 227  318 1206 1313    0]]\n",
            "[[ 99 156 323 387   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "epoch:11 step:   42/60, lr:0.000044, giou_loss:   1.28, conf_loss:  23.83, prob_loss:   1.37, total_loss:  26.48\n",
            "[[130 173 442 461   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[ 45  53 252 261   0]]\n",
            "[[121  45 547 376   1]]\n",
            "[[282 266 863 671   1  24 174 599 657   1]]\n",
            "epoch:11 step:   43/60, lr:0.000044, giou_loss:   0.83, conf_loss:  23.12, prob_loss:   1.70, total_loss:  25.65\n",
            "[[202  37 470 314   0]]\n",
            "[[ 19   1 719 708   0]]\n",
            "[[144  55 337 257   0]]\n",
            "[[ 69 169 211 314   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "epoch:11 step:   44/60, lr:0.000044, giou_loss:   1.11, conf_loss:  24.09, prob_loss:   4.73, total_loss:  29.93\n",
            "[[ 78   8 680 461   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[ 51  25 303 284   2]]\n",
            "[[108  27 466 385   2  77 133 674 590   1]]\n",
            "[[ 191  348 1845 1245    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "epoch:11 step:   45/60, lr:0.000044, giou_loss:   1.00, conf_loss:  23.00, prob_loss:   0.95, total_loss:  24.96\n",
            "[[199  82 384 174   1 305  81 489 175   1]]\n",
            "[[ 635  180 1013  591    2  147   48  869  476    1  169  180  839  798\n",
            "     1]]\n",
            "[[ 21   6 649 180   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "[[ 25  40 917 527   1]]\n",
            "epoch:11 step:   46/60, lr:0.000044, giou_loss:   2.16, conf_loss:  24.05, prob_loss:   1.93, total_loss:  28.15\n",
            "[[167  16 526 349   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[ 541  481 1086  998    2]]\n",
            "[[ 83 209 202 324   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "[[  27  247  291  499    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "epoch:11 step:   47/60, lr:0.000044, giou_loss:   1.98, conf_loss:  24.92, prob_loss:   2.82, total_loss:  29.72\n",
            "[[ 14  61 418 205   1]]\n",
            "[[129 217 299 505   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[  9   4 348 206   1]]\n",
            "[[106 189 637 694   2]]\n",
            "epoch:11 step:   48/60, lr:0.000044, giou_loss:   1.45, conf_loss:  23.40, prob_loss:   1.24, total_loss:  26.09\n",
            "[[ 23 123 232 344   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "[[172  93 906 814   2 172 181 860 843   2]]\n",
            "[[ 80 114 500 556   0]]\n",
            "[[136  10 368 180   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "epoch:11 step:   49/60, lr:0.000043, giou_loss:   0.51, conf_loss:  22.98, prob_loss:   0.45, total_loss:  23.93\n",
            "[[ 38  47 206 217   0]]\n",
            "[[ 54  47 487 476   0]]\n",
            "[[ 15  87 255 291   0]]\n",
            "[[126  54 275 269   1]]\n",
            "epoch:11 step:   50/60, lr:0.000043, giou_loss:   0.77, conf_loss:  23.33, prob_loss:   2.43, total_loss:  26.53\n",
            "[[ 26  23 275 255   2]]\n",
            "[[343 131 907 711   0  78 176 590 665   0]]\n",
            "[[   0   12  955  648    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "[[ 333  207  950  827    2  990  370 1578  974    2]]\n",
            "epoch:11 step:   51/60, lr:0.000043, giou_loss:   0.94, conf_loss:  21.96, prob_loss:   0.51, total_loss:  23.40\n",
            "[[115   4 619 531   0]]\n",
            "[[156 121 507 494   0 363  83 770 514   0]]\n",
            "[[ 12  45 153 202   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[ 40  81 351 352   1 100  65 361 353   1]]\n",
            "epoch:11 step:   52/60, lr:0.000043, giou_loss:   1.08, conf_loss:  22.37, prob_loss:   1.97, total_loss:  25.42\n",
            "[[259 110 678 508   2 664 128 919 369   2]]\n",
            "[[ 19   1 484 457   2]]\n",
            "[[ 53   3 163 113   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[293  27 655 392   0]]\n",
            "epoch:11 step:   53/60, lr:0.000043, giou_loss:   0.99, conf_loss:  22.46, prob_loss:   1.96, total_loss:  25.42\n",
            "[[161  53 764 664   2]]\n",
            "[[129 109 632 355   1]]\n",
            "[[ 57 120 143 208   2  83 107 161 198   2]]\n",
            "[[ 63  63 565 266   1]]\n",
            "epoch:11 step:   54/60, lr:0.000043, giou_loss:   1.31, conf_loss:  22.35, prob_loss:   0.73, total_loss:  24.39\n",
            "[[  81   48  845  390    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "[[ 36 158 492 644   2]]\n",
            "[[133 173 556 595   0 401 115 725 441   0]]\n",
            "[[ 29  30 316 316   0]]\n",
            "epoch:11 step:   55/60, lr:0.000043, giou_loss:   0.78, conf_loss:  22.56, prob_loss:   1.29, total_loss:  24.63\n",
            "[[  6  17 462 386   2]]\n",
            "[[147 218 336 412   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "[[228 130 538 446   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[  19  785  826 1724    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "epoch:11 step:   56/60, lr:0.000042, giou_loss:   0.95, conf_loss:  23.26, prob_loss:   3.49, total_loss:  27.70\n",
            "[[241 236 929 756   1   4 236 433 756   1]]\n",
            "[[183  39 379 284   1]]\n",
            "[[ 99  48 568 382   0   3  40 475 439   0]]\n",
            "[[ 64  57 185 200   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "epoch:11 step:   57/60, lr:0.000042, giou_loss:   1.09, conf_loss:  22.47, prob_loss:   0.55, total_loss:  24.11\n",
            "[[145 210 697 484   1]]\n",
            "[[  17    1 1590  624    1]]\n",
            "[[149 138 537 313   1]]\n",
            "[[  2  16 203 221   2]]\n",
            "epoch:11 step:   58/60, lr:0.000042, giou_loss:   1.02, conf_loss:  22.71, prob_loss:   0.94, total_loss:  24.67\n",
            "[[ 69  12 256 214   2]]\n",
            "[[ 18  63 274 293   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "[[ 51   3 414 350   0]]\n",
            "[[ 290    3 1603 1070    1]]\n",
            "epoch:11 step:   59/60, lr:0.000042, giou_loss:   0.81, conf_loss:  21.89, prob_loss:   0.87, total_loss:  23.57\n",
            "[[ 260   43 1139  929    2]]\n",
            "[[  4  71 366 398   2]]\n",
            "[[189  10 447 258   2 357  35 656 334   2]]\n",
            "[[  83  288 2539 4178    1]]\n",
            "epoch:11 step:    0/60, lr:0.000042, giou_loss:   0.92, conf_loss:  21.82, prob_loss:   0.80, total_loss:  23.54\n",
            "[[ 54  72 604 624   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "[[169 394 974 607   1]]\n",
            "[[158  69 407 321   0 314 112 594 385   0]]\n",
            "[[ 24  11 253 235   2]]\n",
            "epoch:11 step:    1/60, lr:0.000042, giou_loss:   0.68, conf_loss:  21.79, prob_loss:   0.43, total_loss:  22.90\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[174  20 364 199   2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[155 105 453 436   0]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[203 115 584 452   1]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[227  62 723 500   0]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[133 165 922 525   1]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[150 165 923 983   2]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[145  36 421 286   2]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[111  51 372 277   2]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "\n",
            "\n",
            "giou_val_loss:    nan, conf_val_loss:  24.38, prob_val_loss:   1.67, total_val_loss:    nan\n",
            "\n",
            "\n",
            "[[  2   8 157 155   0]]\n",
            "[[164  36 595 477   2]]\n",
            "[[140  72 368 305   2]]\n",
            "[[ 13   5 261 261   2]]\n",
            "epoch:12 step:    2/60, lr:0.000042, giou_loss:   0.61, conf_loss:  21.82, prob_loss:   1.74, total_loss:  24.17\n",
            "[[162 113 384 332   2 213  98 413 320   2]]\n",
            "[[103  78 343 322   2 291  81 545 348   2]]\n",
            "[[ 37 131 594 365   1]]\n",
            "[[280 146 642 511   0]]\n",
            "epoch:12 step:    3/60, lr:0.000041, giou_loss:   0.99, conf_loss:  21.94, prob_loss:   0.95, total_loss:  23.88\n",
            "[[104  36 226 159   2 139   5 275 149   2]]\n",
            "[[ 34 114 266 284   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "[[ 99   6 362 294   0  79  18 338 270   0]]\n",
            "[[ 62  77 245 267   2]]\n",
            "epoch:12 step:    4/60, lr:0.000041, giou_loss:   0.70, conf_loss:  21.03, prob_loss:   0.38, total_loss:  22.11\n",
            "[[ 10   3 187 170   0]]\n",
            "[[ 436  278 1039  889    2]]\n",
            "[[112   2 555 397   1]]\n",
            "[[216  78 797 483   1  24 174 599 657   1]]\n",
            "epoch:12 step:    5/60, lr:0.000041, giou_loss:   0.97, conf_loss:  21.98, prob_loss:   0.31, total_loss:  23.26\n",
            "[[118 110 516 525   0]]\n",
            "[[ 28  57 204 246   0]]\n",
            "[[136  34 529 436   2]]\n",
            "[[ 28  56 224 239   0]]\n",
            "epoch:12 step:    6/60, lr:0.000041, giou_loss:   0.57, conf_loss:  21.39, prob_loss:   0.66, total_loss:  22.62\n",
            "[[467 341 777 657   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[ 16   8 435 202   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "[[  8  15 331 349   0]]\n",
            "[[ 17  45 497 249   1]]\n",
            "epoch:12 step:    7/60, lr:0.000041, giou_loss:   0.92, conf_loss:  21.81, prob_loss:   0.46, total_loss:  23.19\n",
            "[[188  32 384 277   1]]\n",
            "[[ 80 261 385 567   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[  7  81 192 173   1 305  81 489 175   1]]\n",
            "[[ 474    1 1174  708    0]]\n",
            "epoch:12 step:    8/60, lr:0.000041, giou_loss:   1.09, conf_loss:  22.38, prob_loss:   2.81, total_loss:  26.28\n",
            "[[ 41  47 529 354   1]]\n",
            "[[133 438 416 708   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[380  42 638 290   2 357  35 656 334   2]]\n",
            "[[ 418   73 1097  821    0]]\n",
            "epoch:12 step:    9/60, lr:0.000041, giou_loss:   1.14, conf_loss:  22.45, prob_loss:   0.99, total_loss:  24.58\n",
            "[[ 86  61 599 573   2]]\n",
            "[[146   6 650 533   0]]\n",
            "[[220 101 458 379   2 347 127 603 398   2]]\n",
            "[[ 79  64 164 158   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "epoch:12 step:   10/60, lr:0.000040, giou_loss:   0.44, conf_loss:  21.12, prob_loss:   0.92, total_loss:  22.49\n",
            "[[191 159 787 826   1]]\n",
            "[[ 45 144 278 363   2]]\n",
            "[[353 106 702 474   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "[[ 42   3 374 332   2]]\n",
            "epoch:12 step:   11/60, lr:0.000040, giou_loss:   0.90, conf_loss:  22.00, prob_loss:   1.21, total_loss:  24.11\n",
            "[[ 84  20 686 473   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[ 25   6 481 375   2]]\n",
            "[[ 55 120 458 528   2]]\n",
            "[[134  86 398 358   2]]\n",
            "epoch:12 step:   12/60, lr:0.000040, giou_loss:   0.71, conf_loss:  20.95, prob_loss:   0.95, total_loss:  22.61\n",
            "[[ 29  44 197 214   0]]\n",
            "[[363  86 642 367   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "[[ 93 206 781 726   1   4 236 433 756   1]]\n",
            "[[ 64  29 271 237   0]]\n",
            "epoch:12 step:   13/60, lr:0.000040, giou_loss:   1.17, conf_loss:  22.71, prob_loss:   0.77, total_loss:  24.65\n",
            "[[167  29 346 201   0]]\n",
            "[[ 86 125 449 614   1]]\n",
            "[[117 100 550 529   0]]\n",
            "[[407 277 522 376   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "epoch:12 step:   14/60, lr:0.000040, giou_loss:   1.70, conf_loss:  23.03, prob_loss:   3.45, total_loss:  28.18\n",
            "[[ 193  207 1700  983    1]]\n",
            "[[ 70  30 400 362   2]]\n",
            "[[  1  33 283 278   0]]\n",
            "[[ 32   2 419 195   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "epoch:12 step:   15/60, lr:0.000040, giou_loss:   0.86, conf_loss:  21.93, prob_loss:   1.97, total_loss:  24.75\n",
            "[[ 84  32 422 393   2  58  69 410 432   2]]\n",
            "[[   5    1 1578  624    1]]\n",
            "[[147 218 336 412   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "[[ 66  16 385 128   1]]\n",
            "epoch:12 step:   16/60, lr:0.000040, giou_loss:   0.97, conf_loss:  21.70, prob_loss:   0.44, total_loss:  23.11\n",
            "[[ 55 151 246 340   2 328 173 488 351   2]]\n",
            "[[161  37 626 493   2]]\n",
            "[[ 48  66 410 393   2]]\n",
            "[[ 488  249 1122 1249    1  546  621 1071 1679    1]]\n",
            "epoch:12 step:   17/60, lr:0.000040, giou_loss:   1.12, conf_loss:  21.73, prob_loss:   0.66, total_loss:  23.50\n",
            "[[ 141   91 1542 1482    2]]\n",
            "[[ 103  625 2559 4515    1]]\n",
            "[[ 11 163 190 333   2]]\n",
            "[[ 69  32 571 235   1]]\n",
            "epoch:12 step:   18/60, lr:0.000039, giou_loss:   0.75, conf_loss:  21.50, prob_loss:   0.64, total_loss:  22.89\n",
            "[[ 59   5 482 334   1]]\n",
            "[[ 10  30 196 212   0]]\n",
            "[[ 143   89 1249 1116    0]]\n",
            "[[ 24   5 335 276   1 100  65 361 353   1]]\n",
            "epoch:12 step:   19/60, lr:0.000039, giou_loss:   0.47, conf_loss:  21.11, prob_loss:   2.90, total_loss:  24.49\n",
            "[[219 196 587 585   2]]\n",
            "[[ 70  34 180 144   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[ 780  235 1352 1235    1]]\n",
            "[[ 46  98 505 348   1]]\n",
            "epoch:12 step:   20/60, lr:0.000039, giou_loss:   1.44, conf_loss:  21.82, prob_loss:   2.38, total_loss:  25.64\n",
            "[[   7   19 1821  942    1]]\n",
            "[[   1   52 1181  518    1]]\n",
            "[[113 258 583 700   0]]\n",
            "[[132  11 401 275   0]]\n",
            "epoch:12 step:   21/60, lr:0.000039, giou_loss:   0.67, conf_loss:  20.84, prob_loss:   2.14, total_loss:  23.66\n",
            "[[  27  247  291  499    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "[[   8  549  815 1488    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "[[ 111  499 1153  990    1   68  224 1011  751    1]]\n",
            "[[  1 165 427 379   1]]\n",
            "epoch:12 step:   22/60, lr:0.000039, giou_loss:   1.17, conf_loss:  23.07, prob_loss:   3.92, total_loss:  28.16\n",
            "[[ 38   1 345 299   2]]\n",
            "[[329  82 841 639   0]]\n",
            "[[103 104 224 247   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "[[ 163    1 1017  879    0]]\n",
            "epoch:12 step:   23/60, lr:0.000039, giou_loss:   0.95, conf_loss:  21.34, prob_loss:   1.05, total_loss:  23.33\n",
            "[[506 262 670 412   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "[[ 93  22 645 296   1]]\n",
            "[[  28  346 2760 2829    1]]\n",
            "[[179  38 414 282   0]]\n",
            "epoch:12 step:   24/60, lr:0.000039, giou_loss:   1.33, conf_loss:  22.35, prob_loss:   0.87, total_loss:  24.55\n",
            "[[ 72  11 505 336   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[ 65  23 491 354   1]]\n",
            "[[ 98  84 307 305   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "[[ 54  58 223 166   1]]\n",
            "epoch:12 step:   25/60, lr:0.000038, giou_loss:   0.60, conf_loss:  21.01, prob_loss:   1.44, total_loss:  23.04\n",
            "[[330 160 755 569   2]]\n",
            "[[ 81  19 973 506   1]]\n",
            "[[ 36 158 492 644   2]]\n",
            "[[ 52  66 488 303   1]]\n",
            "epoch:12 step:   26/60, lr:0.000038, giou_loss:   1.14, conf_loss:  21.68, prob_loss:   2.29, total_loss:  25.10\n",
            "[[391 103 614 317   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "[[  1  55 194 257   0]]\n",
            "[[ 191  187  808  807    2  990  370 1578  974    2]]\n",
            "[[ 12  14 210 205   2 214  75 383 248   2]]\n",
            "epoch:12 step:   27/60, lr:0.000038, giou_loss:   1.40, conf_loss:  20.90, prob_loss:   0.71, total_loss:  23.00\n",
            "[[1228  125 2693 1699    2]]\n",
            "[[ 80  29 332 288   2]]\n",
            "[[ 54 228 433 577   0]]\n",
            "[[115 183 411 455   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "epoch:12 step:   28/60, lr:0.000038, giou_loss:   1.34, conf_loss:  23.00, prob_loss:   1.98, total_loss:  26.32\n",
            "[[ 984  467 1407  874    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "[[304 105 773 439   0   3  40 475 439   0]]\n",
            "[[ 25  42 275 297   0]]\n",
            "[[ 188  114 1067 1000    2]]\n",
            "epoch:12 step:   29/60, lr:0.000038, giou_loss:   0.89, conf_loss:  21.99, prob_loss:   2.56, total_loss:  25.44\n",
            "[[ 27  48 237 310   1]]\n",
            "[[ 167  326  901 1047    2  172  181  860  843    2]]\n",
            "[[202  33 518 326   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "[[  8  61 347 263   1]]\n",
            "epoch:12 step:   30/60, lr:0.000038, giou_loss:   0.73, conf_loss:  21.10, prob_loss:   0.96, total_loss:  22.79\n",
            "[[ 87 265 694 868   0]]\n",
            "[[ 94 245 358 558   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "[[438  36 858 412   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "[[136 129 273 288   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "epoch:12 step:   31/60, lr:0.000038, giou_loss:   1.46, conf_loss:  21.89, prob_loss:   0.93, total_loss:  24.28\n",
            "[[ 72  94 176 201   2]]\n",
            "[[124 145 886 880   2]]\n",
            "[[119  53 603 343   1]]\n",
            "[[ 259  265  885 1403    1]]\n",
            "epoch:12 step:   32/60, lr:0.000037, giou_loss:   1.47, conf_loss:  22.08, prob_loss:   2.28, total_loss:  25.82\n",
            "[[ 43 271 527 747   0   1 132 368 605   0]]\n",
            "[[ 69  43 218 258   1]]\n",
            "[[  0  92 405 434   1]]\n",
            "[[  89  356  853  698    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "epoch:12 step:   33/60, lr:0.000037, giou_loss:   0.71, conf_loss:  21.06, prob_loss:   0.46, total_loss:  22.23\n",
            "[[ 66  11 520 467   2]]\n",
            "[[  4  37 340 400   2]]\n",
            "[[189  99 445 329   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "[[ 362  423  926 1003    0   78  176  590  665    0]]\n",
            "epoch:12 step:   34/60, lr:0.000037, giou_loss:   0.91, conf_loss:  21.06, prob_loss:   2.17, total_loss:  24.14\n",
            "[[174 144 356 355   0]]\n",
            "[[ 24  11 253 235   2]]\n",
            "[[204 154 624 596   0]]\n",
            "[[   4  364  757 1046    0]]\n",
            "epoch:12 step:   35/60, lr:0.000037, giou_loss:   0.56, conf_loss:  20.01, prob_loss:   1.78, total_loss:  22.35\n",
            "[[ 53 315 955 793   1]]\n",
            "[[400  64 950 616   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "[[ 29   1 834 214   1]]\n",
            "[[301 155 659 513   2  77 133 674 590   1]]\n",
            "epoch:12 step:   36/60, lr:0.000037, giou_loss:   1.21, conf_loss:  20.97, prob_loss:   0.72, total_loss:  22.91\n",
            "[[ 16   1 157 242   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[ 21  18 188 195   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "[[ 26  13 123 118   2]]\n",
            "[[120  45 481 336   2]]\n",
            "epoch:12 step:   37/60, lr:0.000037, giou_loss:   1.54, conf_loss:  22.96, prob_loss:   2.55, total_loss:  27.04\n",
            "[[221  83 463 338   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "[[119 470 497 881   2 147  48 869 476   1 169 180 839 798   1]]\n",
            "[[ 98  14 272 189   0  18  22 168 190   0]]\n",
            "[[ 92  61 498 480   0]]\n",
            "epoch:12 step:   38/60, lr:0.000037, giou_loss:   1.00, conf_loss:  21.41, prob_loss:   3.77, total_loss:  26.18\n",
            "[[217 420 393 590   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "[[ 173  313 1128  949    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "[[  1  23 241 227   0]]\n",
            "[[ 17  19 562 536   2]]\n",
            "epoch:12 step:   39/60, lr:0.000036, giou_loss:   1.27, conf_loss:  21.19, prob_loss:   1.39, total_loss:  23.85\n",
            "[[125  58 302 221   0]]\n",
            "[[357  50 935 562   2]]\n",
            "[[108  53 611 299   1]]\n",
            "[[317 156 541 387   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "epoch:12 step:   40/60, lr:0.000036, giou_loss:   0.94, conf_loss:  21.38, prob_loss:   1.36, total_loss:  23.68\n",
            "[[ 107   73 1464 1393    0]]\n",
            "[[490 253 826 623   2 430  58 759 380   2]]\n",
            "[[ 16  47 665 751   0]]\n",
            "[[109  48 633 406   1]]\n",
            "epoch:12 step:   41/60, lr:0.000036, giou_loss:   0.95, conf_loss:  20.65, prob_loss:   1.67, total_loss:  23.27\n",
            "[[  8  17 155 161   0]]\n",
            "[[ 30 116 200 404   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[297 113 416 228   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "[[ 24   6 170 149   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "epoch:12 step:   42/60, lr:0.000036, giou_loss:   1.61, conf_loss:  22.64, prob_loss:   3.02, total_loss:  27.27\n",
            "[[ 628  185 1586  925    2  822  643 1620 1066    2]]\n",
            "[[ 18  21 336 325   1]]\n",
            "[[  70  159 1041  943    1  445  447 1069 1046    0]]\n",
            "[[214 159 296 247   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "epoch:12 step:   43/60, lr:0.000036, giou_loss:   1.15, conf_loss:  22.85, prob_loss:   4.25, total_loss:  28.25\n",
            "[[ 38 106 203 301   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[101  27 595 274   1 272  80 498 306   1]]\n",
            "[[193  33 439 258   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "[[   0   99 1867 1919    2]]\n",
            "epoch:12 step:   44/60, lr:0.000036, giou_loss:   1.44, conf_loss:  23.27, prob_loss:   2.00, total_loss:  26.70\n",
            "[[178  94 475 381   0]]\n",
            "[[241 279 452 475   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[107  26 419 314   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[205 196 375 374   0 197 144 363 328   0]]\n",
            "epoch:12 step:   45/60, lr:0.000036, giou_loss:   1.55, conf_loss:  21.34, prob_loss:   0.97, total_loss:  23.87\n",
            "[[294 127 543 379   0 314 112 594 385   0]]\n",
            "[[  1  16 327 291   0]]\n",
            "[[ 29 216 202 388   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "[[ 10   7 164 152   2]]\n",
            "epoch:12 step:   46/60, lr:0.000035, giou_loss:   0.66, conf_loss:  20.41, prob_loss:   0.48, total_loss:  21.55\n",
            "[[  2   2 282 303   2]]\n",
            "[[ 74   8 160  96   2  83 107 161 198   2]]\n",
            "[[ 42  55 477 497   0]]\n",
            "[[ 97  60 744 424   1]]\n",
            "epoch:12 step:   47/60, lr:0.000035, giou_loss:   0.79, conf_loss:  20.80, prob_loss:   0.54, total_loss:  22.13\n",
            "[[ 31  39 209 217   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "[[ 34  35 290 291   0]]\n",
            "[[ 76 279 218 424   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "[[265 201 498 468   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "epoch:12 step:   48/60, lr:0.000035, giou_loss:   1.24, conf_loss:  22.04, prob_loss:   2.55, total_loss:  25.83\n",
            "[[344  76 640 394   0]]\n",
            "[[ 50  25 351 345   2]]\n",
            "[[172 106 791 720   0]]\n",
            "[[ 92  48 405 174   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "epoch:12 step:   49/60, lr:0.000035, giou_loss:   1.02, conf_loss:  21.21, prob_loss:   0.50, total_loss:  22.72\n",
            "[[149  77 392 300   2]]\n",
            "[[100  21 407 353   0]]\n",
            "[[165  70 584 468   2 664 128 919 369   2]]\n",
            "[[ 22  34 296 301   2]]\n",
            "epoch:12 step:   50/60, lr:0.000035, giou_loss:   0.82, conf_loss:  20.03, prob_loss:   1.54, total_loss:  22.39\n",
            "[[ 73   3 341 280   0]]\n",
            "[[ 73  86 285 291   2]]\n",
            "[[101  10 242 167   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[315  17 561 278   2]]\n",
            "epoch:12 step:   51/60, lr:0.000035, giou_loss:   0.89, conf_loss:  20.90, prob_loss:   0.45, total_loss:  22.25\n",
            "[[  3  17 407 161   1]]\n",
            "[[ 78  42 365 328   0]]\n",
            "[[228  85 397 240   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "[[376 568 770 949   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "epoch:12 step:   52/60, lr:0.000035, giou_loss:   0.64, conf_loss:  19.81, prob_loss:   0.30, total_loss:  20.76\n",
            "[[ 422   11 1529  766    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "[[ 26  39 324 327   0]]\n",
            "[[124  69 512 244   1]]\n",
            "[[184  71 535 444   0 363  83 770 514   0]]\n",
            "epoch:12 step:   53/60, lr:0.000035, giou_loss:   0.61, conf_loss:  20.07, prob_loss:   3.57, total_loss:  24.25\n",
            "[[ 10  30 369 363   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[ 416  163 1256  999    2]]\n",
            "[[ 244   31  977  806    2  580   21 1273  753    2]]\n",
            "[[ 18 258 646 432   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "epoch:12 step:   54/60, lr:0.000034, giou_loss:   0.86, conf_loss:  21.34, prob_loss:   1.87, total_loss:  24.07\n",
            "[[  31  112 1685 1009    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[206 145 629 567   0 401 115 725 441   0]]\n",
            "[[ 28  33 277 265   2]]\n",
            "[[  52   95 1031 1090    0]]\n",
            "epoch:12 step:   55/60, lr:0.000034, giou_loss:   0.78, conf_loss:  20.67, prob_loss:   0.50, total_loss:  21.95\n",
            "[[ 52 123 253 328   2]]\n",
            "[[ 111    4 1424 1071    1]]\n",
            "[[ 624  182 1146  516    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "[[  8  64 300 180   1]]\n",
            "epoch:12 step:   56/60, lr:0.000034, giou_loss:   1.14, conf_loss:  20.92, prob_loss:   2.80, total_loss:  24.86\n",
            "[[218 185 546 509   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "[[222   1 585 348   0]]\n",
            "[[110  50 334 292   2]]\n",
            "[[ 33  63 564 568   2]]\n",
            "epoch:12 step:   57/60, lr:0.000034, giou_loss:   0.92, conf_loss:  20.48, prob_loss:   0.63, total_loss:  22.03\n",
            "[[138  13 941 834   0   1   1 407 617   0 104   4 624 557   0]]\n",
            "[[582 163 956 541   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "[[ 64  15 726 718   2]]\n",
            "[[ 21 163 692 420   1]]\n",
            "epoch:12 step:   58/60, lr:0.000034, giou_loss:   0.77, conf_loss:  20.45, prob_loss:   0.76, total_loss:  21.98\n",
            "[[  4  56 329 385   0 304  62 575 353   0]]\n",
            "[[ 71  18 258 220   2]]\n",
            "[[ 292   43  715  469    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "[[121  96 588 371   1 190 100 623 330   1]]\n",
            "epoch:12 step:   59/60, lr:0.000034, giou_loss:   0.89, conf_loss:  20.69, prob_loss:   0.91, total_loss:  22.49\n",
            "[[ 29  31 431 120   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "[[  5  12 205 200   0]]\n",
            "[[ 39  82 165 222   0]]\n",
            "[[  6   8 299 124   1]]\n",
            "epoch:12 step:    0/60, lr:0.000034, giou_loss:   0.99, conf_loss:  20.29, prob_loss:   0.92, total_loss:  22.20\n",
            "[[ 17   3 345 305   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "[[223  69 443 270   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "[[139 125 441 439   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "[[ 76   3 273 773   1]]\n",
            "epoch:12 step:    1/60, lr:0.000033, giou_loss:   1.28, conf_loss:  21.91, prob_loss:   5.91, total_loss:  29.11\n",
            "[[111  51 372 277   2]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[155 105 453 436   0]]\n",
            "[[133 165 922 525   1]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[150 165 923 983   2]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[279  14 562 295   2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[145  36 421 286   2]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[166  92 638 559   2]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[203 115 584 452   1]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "\n",
            "\n",
            "giou_val_loss:    nan, conf_val_loss:  21.31, prob_val_loss:   1.46, total_val_loss:    nan\n",
            "\n",
            "\n",
            "[[ 733  155 1467  876    2  172  181  860  843    2]]\n",
            "[[ 18  45 424 464   0]]\n",
            "[[ 23 215 315 331   1]]\n",
            "[[216  80 358 225   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "epoch:13 step:    2/60, lr:0.000033, giou_loss:   1.09, conf_loss:  21.15, prob_loss:   1.66, total_loss:  23.90\n",
            "[[ 45  27 214 135   1]]\n",
            "[[  57  215 1711 1112    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[356 105 526 393   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[ 168    1 1126  741    2  822  643 1620 1066    2]]\n",
            "epoch:13 step:    3/60, lr:0.000033, giou_loss:   1.57, conf_loss:  22.92, prob_loss:   3.93, total_loss:  28.41\n",
            "[[118   5 414 277   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "[[109  37 502 439   2]]\n",
            "[[217 321 545 645   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "[[560  91 896 461   2 430  58 759 380   2]]\n",
            "epoch:13 step:    4/60, lr:0.000033, giou_loss:   1.21, conf_loss:  21.87, prob_loss:   1.60, total_loss:  24.68\n",
            "[[  4  71 366 398   2]]\n",
            "[[ 62  69 824 804   2]]\n",
            "[[238 227 405 404   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "[[441 231 664 445   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "epoch:13 step:    5/60, lr:0.000033, giou_loss:   1.28, conf_loss:  22.23, prob_loss:   1.89, total_loss:  25.40\n",
            "[[ 89  48 613 406   1]]\n",
            "[[ 54  18 657 629   2]]\n",
            "[[190 101 425 345   0]]\n",
            "[[  5 234 368 723   1]]\n",
            "epoch:13 step:    6/60, lr:0.000033, giou_loss:   0.86, conf_loss:  20.44, prob_loss:   0.84, total_loss:  22.14\n",
            "[[171 105 411 349   2 291  81 545 348   2]]\n",
            "[[ 76 301 546 743   0]]\n",
            "[[113  34 324 230   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[ 11   5 446 447   0]]\n",
            "epoch:13 step:    7/60, lr:0.000033, giou_loss:   1.01, conf_loss:  20.76, prob_loss:   1.08, total_loss:  22.85\n",
            "[[290 111 939 815   0]]\n",
            "[[  4  53 130 193   0]]\n",
            "[[ 41 126 392 499   0 363  83 770 514   0]]\n",
            "[[ 54 189 807 871   0]]\n",
            "epoch:13 step:    8/60, lr:0.000032, giou_loss:   0.87, conf_loss:  20.75, prob_loss:   3.78, total_loss:  25.39\n",
            "[[153  49 483 381   2]]\n",
            "[[ 35  45 203 215   0]]\n",
            "[[ 20  75 242 294   2 213  98 413 320   2]]\n",
            "[[ 68 161 187 276   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "epoch:13 step:    9/60, lr:0.000032, giou_loss:   1.04, conf_loss:  21.34, prob_loss:   2.67, total_loss:  25.04\n",
            "[[145  46 342 816   1]]\n",
            "[[   5   64  269  316    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "[[ 42  22 238 205   0]]\n",
            "[[  8  92 434 306   1]]\n",
            "epoch:13 step:   10/60, lr:0.000032, giou_loss:   1.17, conf_loss:  21.30, prob_loss:   0.96, total_loss:  23.44\n",
            "[[ 24   5 335 276   1 100  65 361 353   1]]\n",
            "[[ 15  12 394 361   0]]\n",
            "[[ 47  74 188 315   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[184 111 533 479   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "epoch:13 step:   11/60, lr:0.000032, giou_loss:   1.39, conf_loss:  21.72, prob_loss:   2.16, total_loss:  25.27\n",
            "[[ 17 328 250 595   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "[[305  26 724 424   2 664 128 919 369   2]]\n",
            "[[ 42 125 526 601   0   1 132 368 605   0]]\n",
            "[[ 28  33 277 265   2]]\n",
            "epoch:13 step:   12/60, lr:0.000032, giou_loss:   1.19, conf_loss:  20.50, prob_loss:   1.38, total_loss:  23.07\n",
            "[[ 23   8 272 260   0 314 112 594 385   0]]\n",
            "[[ 58  10 245 212   2]]\n",
            "[[ 38  43 339 363   2]]\n",
            "[[ 16  18 618 471   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "epoch:13 step:   13/60, lr:0.000032, giou_loss:   0.65, conf_loss:  19.66, prob_loss:   0.90, total_loss:  21.21\n",
            "[[ 13   4 263 259   0]]\n",
            "[[  9  71 566 305   1]]\n",
            "[[ 24  20 178 165   2]]\n",
            "[[ 11  17 158 161   0]]\n",
            "epoch:13 step:   14/60, lr:0.000032, giou_loss:   0.47, conf_loss:  20.14, prob_loss:   1.47, total_loss:  22.08\n",
            "[[  2  29 295 145   1]]\n",
            "[[  1  24 406 366   1]]\n",
            "[[   2  438  957 1074    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "[[544 116 969 525   2]]\n",
            "epoch:13 step:   15/60, lr:0.000032, giou_loss:   0.88, conf_loss:  20.22, prob_loss:   2.13, total_loss:  23.22\n",
            "[[ 17  15 340 349   0]]\n",
            "[[184  45 643 295   1]]\n",
            "[[ 41   5 196 152   0]]\n",
            "[[  8  11 208 199   0]]\n",
            "epoch:13 step:   16/60, lr:0.000031, giou_loss:   0.51, conf_loss:  19.89, prob_loss:   1.38, total_loss:  21.78\n",
            "[[155  30 452 317   0]]\n",
            "[[ 90 167 642 441   1]]\n",
            "[[ 17  20 203 202   0]]\n",
            "[[133  52 602 386   0   3  40 475 439   0]]\n",
            "epoch:13 step:   17/60, lr:0.000031, giou_loss:   1.02, conf_loss:  20.50, prob_loss:   0.78, total_loss:  22.30\n",
            "[[209  93 597 268   1]]\n",
            "[[226 113 395 268   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "[[101  31 763 734   2]]\n",
            "[[   7    7 1187  473    1]]\n",
            "epoch:13 step:   18/60, lr:0.000031, giou_loss:   0.89, conf_loss:  20.06, prob_loss:   0.61, total_loss:  21.55\n",
            "[[  8   3 439 444   2]]\n",
            "[[ 27  72 521 319   1 272  80 498 306   1]]\n",
            "[[ 124   58  888  400    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "[[ 65 260 967 738   1]]\n",
            "epoch:13 step:   19/60, lr:0.000031, giou_loss:   0.75, conf_loss:  20.33, prob_loss:   2.37, total_loss:  23.45\n",
            "[[ 69  43 218 258   1]]\n",
            "[[ 83  26 762 774   0]]\n",
            "[[261  82 589 384   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "[[   4   54  983 1049    0]]\n",
            "epoch:13 step:   20/60, lr:0.000031, giou_loss:   0.72, conf_loss:  19.73, prob_loss:   2.72, total_loss:  23.18\n",
            "[[  44  219  678 1219    1  546  621 1071 1679    1]]\n",
            "[[355 289 531 459   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "[[ 47  52 379 381   2]]\n",
            "[[ 64  16 150 104   2  83 107 161 198   2]]\n",
            "epoch:13 step:   21/60, lr:0.000031, giou_loss:   1.57, conf_loss:  21.27, prob_loss:   1.01, total_loss:  23.85\n",
            "[[375  31 743 420   2]]\n",
            "[[132  28 728 695   1]]\n",
            "[[345 137 583 415   2 347 127 603 398   2]]\n",
            "[[ 66   4 368 318   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "epoch:13 step:   22/60, lr:0.000031, giou_loss:   1.08, conf_loss:  20.26, prob_loss:   0.98, total_loss:  22.31\n",
            "[[269   5 576 303   2]]\n",
            "[[ 25  34 166 191   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[602 238 881 519   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "[[ 35  26 218 216   2]]\n",
            "epoch:13 step:   23/60, lr:0.000030, giou_loss:   1.20, conf_loss:  20.40, prob_loss:   1.01, total_loss:  22.61\n",
            "[[454 212 759 518   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[ 16  30 908 517   1]]\n",
            "[[  23  158 1380 1478    0]]\n",
            "[[318 146 680 511   0]]\n",
            "epoch:13 step:   24/60, lr:0.000030, giou_loss:   0.69, conf_loss:  19.48, prob_loss:   1.96, total_loss:  22.13\n",
            "[[ 48  97 532 387   1]]\n",
            "[[ 93  79 272 249   2]]\n",
            "[[ 148   82 1721  705    1]]\n",
            "[[374   1 830 487   2]]\n",
            "epoch:13 step:   25/60, lr:0.000030, giou_loss:   1.02, conf_loss:  19.32, prob_loss:   1.36, total_loss:  21.70\n",
            "[[101  32 278 195   0]]\n",
            "[[ 26  33 344 337   1]]\n",
            "[[ 38  56 376 417   2  58  69 410 432   2]]\n",
            "[[529 223 923 604   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "epoch:13 step:   26/60, lr:0.000030, giou_loss:   0.87, conf_loss:  19.86, prob_loss:   1.52, total_loss:  22.25\n",
            "[[ 288  347 1091 1168    0    1    1  407  617    0  104    4  624  557\n",
            "     0]]\n",
            "[[ 45  34 142 139   2]]\n",
            "[[357  11 716 344   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[287  12 555 289   0]]\n",
            "epoch:13 step:   27/60, lr:0.000030, giou_loss:   1.04, conf_loss:  20.69, prob_loss:   0.84, total_loss:  22.57\n",
            "[[ 66   1 151  95   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "[[  4 128 675 385   1]]\n",
            "[[122 139 448 414   0]]\n",
            "[[ 551   58 1284  833    2  580   21 1273  753    2]]\n",
            "epoch:13 step:   28/60, lr:0.000030, giou_loss:   0.83, conf_loss:  19.79, prob_loss:   0.54, total_loss:  21.17\n",
            "[[130 262 294 412   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "[[  4  80 174 258   0 197 144 363 328   0]]\n",
            "[[118  19 479 310   2]]\n",
            "[[ 729  357 1107  768    2  147   48  869  476    1  169  180  839  798\n",
            "     1]]\n",
            "epoch:13 step:   29/60, lr:0.000030, giou_loss:   1.55, conf_loss:  20.73, prob_loss:   1.14, total_loss:  23.42\n",
            "[[ 98  44 271 216   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "[[114  36 236 159   2 139   5 275 149   2]]\n",
            "[[ 59 130 268 351   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "[[232   8 839 611   0]]\n",
            "epoch:13 step:   30/60, lr:0.000030, giou_loss:   1.02, conf_loss:  20.16, prob_loss:   1.75, total_loss:  22.93\n",
            "[[ 820  137 1392 1137    1]]\n",
            "[[ 42  16 220 194   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "[[ 31   5 338 337   0]]\n",
            "[[160  37 496 400   2]]\n",
            "epoch:13 step:   31/60, lr:0.000029, giou_loss:   1.23, conf_loss:  20.25, prob_loss:   0.62, total_loss:  22.10\n",
            "[[ 83  15 352 279   0]]\n",
            "[[ 254   80 1225  864    1  445  447 1069 1046    0]]\n",
            "[[  7  15 626 629   0]]\n",
            "[[ 43 135 219 324   0]]\n",
            "epoch:13 step:   32/60, lr:0.000029, giou_loss:   0.62, conf_loss:  18.97, prob_loss:   0.90, total_loss:  20.49\n",
            "[[ 77  66 275 257   2 214  75 383 248   2]]\n",
            "[[  15  427 2471 4317    1]]\n",
            "[[141  83 644 329   1]]\n",
            "[[108 277 223 376   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "epoch:13 step:   33/60, lr:0.000029, giou_loss:   1.84, conf_loss:  21.64, prob_loss:   3.09, total_loss:  26.57\n",
            "[[ 339  266 1218 1152    2]]\n",
            "[[112  48 308 293   1]]\n",
            "[[  5   1 368 348   0]]\n",
            "[[ 18 246 451 675   0]]\n",
            "epoch:13 step:   34/60, lr:0.000029, giou_loss:   0.81, conf_loss:  19.34, prob_loss:   0.61, total_loss:  20.75\n",
            "[[ 31   3 418 196   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[ 64 101 466 190   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "[[  36   53 1078  544    1   68  224 1011  751    1]]\n",
            "[[ 35 101 217 312   0]]\n",
            "epoch:13 step:   35/60, lr:0.000029, giou_loss:   1.19, conf_loss:  20.52, prob_loss:   1.25, total_loss:  22.95\n",
            "[[ 92 124 605 636   2]]\n",
            "[[ 35  96 268 315   2]]\n",
            "[[ 324  174 1431  929    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "[[ 186  693  609 1100    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "epoch:13 step:   36/60, lr:0.000029, giou_loss:   0.80, conf_loss:  20.71, prob_loss:   1.30, total_loss:  22.81\n",
            "[[ 147  251  669  585    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "[[263   7 509 232   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "[[404 173 827 595   0 401 115 725 441   0]]\n",
            "[[ 75  43 394 155   1]]\n",
            "epoch:13 step:   37/60, lr:0.000029, giou_loss:   1.23, conf_loss:  21.40, prob_loss:   1.93, total_loss:  24.56\n",
            "[[112 267 395 537   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[ 43   4 441 419   0]]\n",
            "[[ 55   1 600 518   2]]\n",
            "[[  8 116 228 317   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "epoch:13 step:   38/60, lr:0.000029, giou_loss:   1.27, conf_loss:  21.84, prob_loss:   3.19, total_loss:  26.30\n",
            "[[  22   33 1889 1853    2]]\n",
            "[[231  57 547 350   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "[[ 49 117 889 953   2]]\n",
            "[[185  76 449 348   2]]\n",
            "epoch:13 step:   39/60, lr:0.000028, giou_loss:   0.77, conf_loss:  19.89, prob_loss:   1.88, total_loss:  22.54\n",
            "[[617 127 991 505   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "[[144  55 337 257   0]]\n",
            "[[327  28 464 187   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "[[ 29  34 455 365   1]]\n",
            "epoch:13 step:   40/60, lr:0.000028, giou_loss:   1.27, conf_loss:  19.34, prob_loss:   0.60, total_loss:  21.20\n",
            "[[223  55 481 303   2 357  35 656 334   2]]\n",
            "[[215 181 406 370   2 328 173 488 351   2]]\n",
            "[[ 74  58 497 387   1]]\n",
            "[[204  63 647 458   1]]\n",
            "epoch:13 step:   41/60, lr:0.000028, giou_loss:   0.81, conf_loss:  19.21, prob_loss:   0.43, total_loss:  20.45\n",
            "[[ 24  36 428 180   1]]\n",
            "[[ 54  15 341 301   0]]\n",
            "[[ 395  209 1021 1347    1]]\n",
            "[[ 17  22 263 283   2]]\n",
            "epoch:13 step:   42/60, lr:0.000028, giou_loss:   1.36, conf_loss:  20.13, prob_loss:   1.53, total_loss:  23.03\n",
            "[[213 134 295 222   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "[[161 168 519 526   2  77 133 674 590   1]]\n",
            "[[ 119  192 1520 1583    2]]\n",
            "[[ 29  60 465 297   1]]\n",
            "epoch:13 step:   43/60, lr:0.000028, giou_loss:   1.03, conf_loss:  20.79, prob_loss:   1.92, total_loss:  23.74\n",
            "[[ 51  25 303 284   2]]\n",
            "[[401 201 713 489   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[ 18 172 706 692   1   4 236 433 756   1]]\n",
            "[[ 76  46 654 558   2]]\n",
            "epoch:13 step:   44/60, lr:0.000028, giou_loss:   0.92, conf_loss:  20.68, prob_loss:   1.44, total_loss:  23.04\n",
            "[[ 82  27 395 153   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "[[131  12 320 206   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "[[154 171 685 676   2]]\n",
            "[[ 48  53 255 261   0]]\n",
            "epoch:13 step:   45/60, lr:0.000028, giou_loss:   0.91, conf_loss:  20.00, prob_loss:   0.57, total_loss:  21.48\n",
            "[[  4   3 286 248   0]]\n",
            "[[141   5 421 306   2]]\n",
            "[[228  58 338 168   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[ 87  24 507 400   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "epoch:13 step:   46/60, lr:0.000028, giou_loss:   0.90, conf_loss:  20.20, prob_loss:   0.90, total_loss:  22.00\n",
            "[[317 156 541 387   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "[[ 52  47 455 455   2]]\n",
            "[[ 23 341 333 657   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[125 133 772 497   1]]\n",
            "epoch:13 step:   47/60, lr:0.000027, giou_loss:   0.88, conf_loss:  20.23, prob_loss:   1.75, total_loss:  22.87\n",
            "[[  58   85 2790 2568    1]]\n",
            "[[ 26  78 172 221   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "[[1283  407 2748 1981    2]]\n",
            "[[  9  24 307 312   0]]\n",
            "epoch:13 step:   48/60, lr:0.000027, giou_loss:   0.57, conf_loss:  18.88, prob_loss:   0.30, total_loss:  19.75\n",
            "[[121 161 588 436   1 190 100 623 330   1]]\n",
            "[[ 44  62 287 285   2]]\n",
            "[[314  58 779 514   2]]\n",
            "[[344  76 640 394   0]]\n",
            "epoch:13 step:   49/60, lr:0.000027, giou_loss:   0.96, conf_loss:  20.16, prob_loss:   0.55, total_loss:  21.67\n",
            "[[ 80  69 644 649   0  78 176 590 665   0]]\n",
            "[[  1 104 225 346   2]]\n",
            "[[150  39 329 211   0]]\n",
            "[[ 54  66 508 522   2]]\n",
            "epoch:13 step:   50/60, lr:0.000027, giou_loss:   0.78, conf_loss:  18.73, prob_loss:   0.44, total_loss:  19.94\n",
            "[[135  95 399 408   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "[[ 70  71 282 276   2]]\n",
            "[[110  19 214 126   2]]\n",
            "[[140  72 368 305   2]]\n",
            "epoch:13 step:   51/60, lr:0.000027, giou_loss:   1.25, conf_loss:  19.91, prob_loss:   1.72, total_loss:  22.88\n",
            "[[ 61  25 541 229   1]]\n",
            "[[  6 241 634 415   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "[[ 265  522  882 1142    2  990  370 1578  974    2]]\n",
            "[[ 42   5 298 261   0]]\n",
            "epoch:13 step:   52/60, lr:0.000027, giou_loss:   0.72, conf_loss:  18.80, prob_loss:   0.25, total_loss:  19.78\n",
            "[[  27  575  834 1514    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "[[ 37   4 158 147   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "[[ 75  21 563 328   1]]\n",
            "[[432 209 944 766   0]]\n",
            "epoch:13 step:   53/60, lr:0.000027, giou_loss:   1.07, conf_loss:  19.91, prob_loss:   0.46, total_loss:  21.45\n",
            "[[ 287  233 1141 1111    0]]\n",
            "[[117  82 302 174   1 305  81 489 175   1]]\n",
            "[[178 116 379 321   2]]\n",
            "[[ 36   5 246 267   1]]\n",
            "epoch:13 step:   54/60, lr:0.000027, giou_loss:   1.42, conf_loss:  19.62, prob_loss:   1.17, total_loss:  22.21\n",
            "[[ 261    7 1574 1074    1]]\n",
            "[[ 26  28 300 295   2]]\n",
            "[[369  82 611 337   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "[[ 63  63 565 266   1]]\n",
            "epoch:13 step:   55/60, lr:0.000026, giou_loss:   0.65, conf_loss:  19.09, prob_loss:   0.28, total_loss:  20.01\n",
            "[[  8   7 237 231   2]]\n",
            "[[ 461    2 1161  709    0]]\n",
            "[[ 43  67 220 234   0]]\n",
            "[[ 97  13 530 338   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "epoch:13 step:   56/60, lr:0.000026, giou_loss:   0.57, conf_loss:  19.41, prob_loss:   0.65, total_loss:  20.62\n",
            "[[ 28  50 268 254   0]]\n",
            "[[ 255  496 1060  709    1]]\n",
            "[[ 145   17 1652  793    1]]\n",
            "[[  4  20 343 222   1]]\n",
            "epoch:13 step:   57/60, lr:0.000026, giou_loss:   0.94, conf_loss:  19.07, prob_loss:   1.45, total_loss:  21.46\n",
            "[[ 23   9 197 184   0  18  22 168 190   0]]\n",
            "[[271 332 436 527   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[322  40 872 592   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "[[ 47  39 295 295   2]]\n",
            "epoch:13 step:   58/60, lr:0.000026, giou_loss:   1.04, conf_loss:  19.67, prob_loss:   1.05, total_loss:  21.75\n",
            "[[ 36  79 492 448   2]]\n",
            "[[ 45  30 308 318   0  79  18 338 270   0]]\n",
            "[[ 13  27 338 356   0 304  62 575 353   0]]\n",
            "[[ 158  116 1264 1143    0]]\n",
            "epoch:13 step:   59/60, lr:0.000026, giou_loss:   0.51, conf_loss:  18.42, prob_loss:   3.45, total_loss:  22.38\n",
            "[[190  12 422 182   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "[[  53  144  476  570    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "[[   5   35 1819  958    1]]\n",
            "[[ 70 146 490 588   0]]\n",
            "epoch:13 step:    0/60, lr:0.000026, giou_loss:   1.00, conf_loss:  19.83, prob_loss:   0.58, total_loss:  21.41\n",
            "[[211   2 715 529   0]]\n",
            "[[279  16 860 421   1  24 174 599 657   1]]\n",
            "[[155  99 411 329   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "[[  8 154 427 348   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "epoch:13 step:    1/60, lr:0.000026, giou_loss:   1.13, conf_loss:  20.83, prob_loss:   0.94, total_loss:  22.90\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[133 165 922 525   1]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[155 105 453 436   0]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[111  51 372 277   2]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[145  36 421 286   2]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[150 165 923 983   2]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[203 115 584 452   1]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[143  40 415 346   0]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[ 10   8 344 336   0]]\n",
            "\n",
            "\n",
            "giou_val_loss:    nan, conf_val_loss:  19.95, prob_val_loss:   1.64, total_val_loss:    nan\n",
            "\n",
            "\n",
            "[[ 637   28 1015  439    2  147   48  869  476    1  169  180  839  798\n",
            "     1]]\n",
            "[[  17  145 2749 2628    1]]\n",
            "[[176 142 346 320   0 197 144 363 328   0]]\n",
            "[[220 105 765 622   2]]\n",
            "epoch:14 step:    2/60, lr:0.000026, giou_loss:   1.14, conf_loss:  19.35, prob_loss:   0.68, total_loss:  21.17\n",
            "[[ 44  27 447 435   2]]\n",
            "[[ 70 166 261 355   2 328 173 488 351   2]]\n",
            "[[234 132 416 343   0]]\n",
            "[[ 157  178 1011 1056    0]]\n",
            "epoch:14 step:    3/60, lr:0.000025, giou_loss:   0.70, conf_loss:  18.65, prob_loss:   1.41, total_loss:  20.75\n",
            "[[ 79   4 510 445   2]]\n",
            "[[ 29   2 392 349   0]]\n",
            "[[ 102   99 1969 1919    2]]\n",
            "[[ 48  33 246 224   2 214  75 383 248   2]]\n",
            "epoch:14 step:    4/60, lr:0.000025, giou_loss:   0.45, conf_loss:  18.81, prob_loss:   1.73, total_loss:  20.99\n",
            "[[219 209 397 387   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "[[ 168    1 1126  741    2  822  643 1620 1066    2]]\n",
            "[[  38  189 1017 1184    0]]\n",
            "[[129  72 271 217   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "epoch:14 step:    5/60, lr:0.000025, giou_loss:   1.07, conf_loss:  19.14, prob_loss:   2.06, total_loss:  22.27\n",
            "[[ 90  92 709 706   0]]\n",
            "[[ 300  271 1271 1055    1  445  447 1069 1046    0]]\n",
            "[[ 93  79 272 249   2]]\n",
            "[[143  77 253 187   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "epoch:14 step:    6/60, lr:0.000025, giou_loss:   0.65, conf_loss:  18.96, prob_loss:   0.75, total_loss:  20.36\n",
            "[[  9  14 244 258   0]]\n",
            "[[119  72 613 319   1 272  80 498 306   1]]\n",
            "[[121 161 588 436   1 190 100 623 330   1]]\n",
            "[[  62  394 1527 1968    2]]\n",
            "epoch:14 step:    7/60, lr:0.000025, giou_loss:   0.84, conf_loss:  19.18, prob_loss:   1.73, total_loss:  21.75\n",
            "[[ 399  133  822  559    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "[[ 71 134 555 610   0   1 132 368 605   0]]\n",
            "[[246 169 595 537   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "[[607 235 943 605   2 430  58 759 380   2]]\n",
            "epoch:14 step:    8/60, lr:0.000025, giou_loss:   1.32, conf_loss:  21.36, prob_loss:   2.26, total_loss:  24.95\n",
            "[[ 29   9 462 438   0]]\n",
            "[[144  55 337 257   0]]\n",
            "[[141   5 421 306   2]]\n",
            "[[149  18 485 381   2]]\n",
            "epoch:14 step:    9/60, lr:0.000025, giou_loss:   0.53, conf_loss:  18.93, prob_loss:   0.93, total_loss:  20.40\n",
            "[[ 176  201 1683  977    1]]\n",
            "[[  5 110 464 360   1]]\n",
            "[[  4  26 322 330   1]]\n",
            "[[  1  65 427 279   1]]\n",
            "epoch:14 step:   10/60, lr:0.000025, giou_loss:   0.90, conf_loss:  18.86, prob_loss:   2.15, total_loss:  21.91\n",
            "[[ 114  320  867 1002    0]]\n",
            "[[ 37  52 205 222   0]]\n",
            "[[398 250 662 563   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "[[ 30 221 206 410   0]]\n",
            "epoch:14 step:   11/60, lr:0.000024, giou_loss:   0.82, conf_loss:  19.59, prob_loss:   0.77, total_loss:  21.19\n",
            "[[ 33  12 283 267   0]]\n",
            "[[ 97  13 276 185   0]]\n",
            "[[ 18  13 689 270   1]]\n",
            "[[ 120  123 1227  878    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "epoch:14 step:   12/60, lr:0.000024, giou_loss:   0.91, conf_loss:  19.54, prob_loss:   1.83, total_loss:  22.28\n",
            "[[ 98  36 294 281   1]]\n",
            "[[  8   1 237 225   2]]\n",
            "[[102  53 239 212   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "[[  14  130  747  905    2  580   21 1273  753    2]]\n",
            "epoch:14 step:   13/60, lr:0.000024, giou_loss:   0.52, conf_loss:  19.67, prob_loss:   2.85, total_loss:  23.04\n",
            "[[202  37 470 314   0]]\n",
            "[[ 43  38 922 924   2]]\n",
            "[[319  51 582 339   0  79  18 338 270   0]]\n",
            "[[253 212 621 601   2]]\n",
            "epoch:14 step:   14/60, lr:0.000024, giou_loss:   0.80, conf_loss:  18.33, prob_loss:   0.83, total_loss:  19.96\n",
            "[[ 21   1 826 214   1]]\n",
            "[[ 59  22 562 268   1]]\n",
            "[[ 24  11 349 340   0 304  62 575 353   0]]\n",
            "[[ 46  55 343 342   0]]\n",
            "epoch:14 step:   15/60, lr:0.000024, giou_loss:   0.94, conf_loss:  19.34, prob_loss:   3.60, total_loss:  23.87\n",
            "[[  2 103 736 824   2 172 181 860 843   2]]\n",
            "[[ 43   2 275 172   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "[[276  10 559 280   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[186   5 886 712   0]]\n",
            "epoch:14 step:   16/60, lr:0.000024, giou_loss:   1.29, conf_loss:  20.37, prob_loss:   1.34, total_loss:  23.01\n",
            "[[ 52  21 321 285   0]]\n",
            "[[ 26  69 604 581   2]]\n",
            "[[536 301 961 710   2]]\n",
            "[[329 265 936 868   0]]\n",
            "epoch:14 step:   17/60, lr:0.000024, giou_loss:   0.61, conf_loss:  18.65, prob_loss:   0.57, total_loss:  19.83\n",
            "[[ 44  43 357 169   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "[[ 118   73 1475 1393    0]]\n",
            "[[ 13 101 305 217   1]]\n",
            "[[  0  92 405 434   1]]\n",
            "epoch:14 step:   18/60, lr:0.000024, giou_loss:   0.70, conf_loss:  18.88, prob_loss:   0.24, total_loss:  19.81\n",
            "[[ 596  800 1230 1800    1  546  621 1071 1679    1]]\n",
            "[[131  25 459 327   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "[[ 31  17 280 249   2]]\n",
            "[[ 52 156 699 520   1]]\n",
            "epoch:14 step:   19/60, lr:0.000023, giou_loss:   0.85, conf_loss:  19.31, prob_loss:   1.61, total_loss:  21.77\n",
            "[[  4  28 214 290   1]]\n",
            "[[190  64 305 163   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "[[ 12  11 310 299   0]]\n",
            "[[  2  15 490 322   1]]\n",
            "epoch:14 step:   20/60, lr:0.000023, giou_loss:   0.71, conf_loss:  19.79, prob_loss:   0.78, total_loss:  21.28\n",
            "[[ 66 175 376 491   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[ 309  110 1264  746    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "[[277  33 418 274   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[ 35 124 548 636   2]]\n",
            "epoch:14 step:   21/60, lr:0.000023, giou_loss:   1.01, conf_loss:  19.22, prob_loss:   3.27, total_loss:  23.49\n",
            "[[265  78 498 345   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "[[100  20 554 476   2]]\n",
            "[[ 72  40 498 371   1]]\n",
            "[[458 209 763 515   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "epoch:14 step:   22/60, lr:0.000023, giou_loss:   0.83, conf_loss:  19.74, prob_loss:   1.00, total_loss:  21.57\n",
            "[[ 163  294 1003 1130    2]]\n",
            "[[245  58 848 669   2]]\n",
            "[[ 34  29 393 362   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[ 245   83 1007  818    2]]\n",
            "epoch:14 step:   23/60, lr:0.000023, giou_loss:   0.79, conf_loss:  18.82, prob_loss:   0.74, total_loss:  20.35\n",
            "[[  14   74  278  326    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "[[  7   5 346 207   1]]\n",
            "[[210  80 407 850   1]]\n",
            "[[221  83 463 338   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "epoch:14 step:   24/60, lr:0.000023, giou_loss:   1.26, conf_loss:  20.51, prob_loss:   1.07, total_loss:  22.83\n",
            "[[155  26 493 387   2  58  69 410 432   2]]\n",
            "[[ 32  24 634 477   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[ 31  15 157 155   0]]\n",
            "[[ 78  57 394 350   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "epoch:14 step:   25/60, lr:0.000023, giou_loss:   0.90, conf_loss:  18.47, prob_loss:   0.34, total_loss:  19.71\n",
            "[[106   7 228 130   2 139   5 275 149   2]]\n",
            "[[   1   83 1181  549    1]]\n",
            "[[126  16 422 288   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "[[ 38  41 481 436   1]]\n",
            "epoch:14 step:   26/60, lr:0.000023, giou_loss:   1.01, conf_loss:  20.24, prob_loss:   1.50, total_loss:  22.75\n",
            "[[  76   60 1182 1087    0]]\n",
            "[[ 21 130 230 351   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "[[ 356  161  973  781    2  990  370 1578  974    2]]\n",
            "[[  16  209 1417 1600    2]]\n",
            "epoch:14 step:   27/60, lr:0.000022, giou_loss:   0.62, conf_loss:  18.78, prob_loss:   0.80, total_loss:  20.20\n",
            "[[ 57 126 230 298   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "[[188  14 611 343   1]]\n",
            "[[ 639  130 1318  878    0]]\n",
            "[[ 47  14 275 247   2]]\n",
            "epoch:14 step:   28/60, lr:0.000022, giou_loss:   0.82, conf_loss:  18.19, prob_loss:   0.45, total_loss:  19.45\n",
            "[[119  97 449 429   2]]\n",
            "[[ 243  155 1046  976    0    1    1  407  617    0  104    4  624  557\n",
            "     0]]\n",
            "[[ 207  161  729  495    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "[[  92    3 1906  926    1]]\n",
            "epoch:14 step:   29/60, lr:0.000022, giou_loss:   1.05, conf_loss:  19.09, prob_loss:   1.09, total_loss:  21.23\n",
            "[[  5  11 159 156   2]]\n",
            "[[244 202 663 600   2 664 128 919 369   2]]\n",
            "[[  5  60 245 264   0]]\n",
            "[[1501  169 2073 1169    1]]\n",
            "epoch:14 step:   30/60, lr:0.000022, giou_loss:   0.95, conf_loss:  19.81, prob_loss:   1.65, total_loss:  22.41\n",
            "[[ 68  20 275 228   0]]\n",
            "[[142 159 562 601   0]]\n",
            "[[  4  21 437 346   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[ 12 129 232 330   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "epoch:14 step:   31/60, lr:0.000022, giou_loss:   1.04, conf_loss:  20.63, prob_loss:   3.32, total_loss:  24.98\n",
            "[[172 403 500 727   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "[[  4  45 473 379   0   3  40 475 439   0]]\n",
            "[[165  86 377 291   2]]\n",
            "[[ 13   2 537 360   1]]\n",
            "epoch:14 step:   32/60, lr:0.000022, giou_loss:   1.00, conf_loss:  18.58, prob_loss:   0.39, total_loss:  19.97\n",
            "[[ 79  31 248 139   1]]\n",
            "[[ 17  16 217 204   0]]\n",
            "[[101   4 288 206   2]]\n",
            "[[ 25  40 917 527   1]]\n",
            "epoch:14 step:   33/60, lr:0.000022, giou_loss:   0.62, conf_loss:  18.41, prob_loss:   0.27, total_loss:  19.30\n",
            "[[ 91   7 355 279   2]]\n",
            "[[ 38  48 569 553   2]]\n",
            "[[ 603  126 1026  533    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "[[ 88 122 390 436   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "epoch:14 step:   34/60, lr:0.000022, giou_loss:   1.25, conf_loss:  20.07, prob_loss:   2.13, total_loss:  23.45\n",
            "[[598 168 877 449   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "[[109  44 213 151   2]]\n",
            "[[205  57 416 253   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[1235  316 1861 1454    1]]\n",
            "epoch:14 step:   35/60, lr:0.000022, giou_loss:   2.28, conf_loss:  21.54, prob_loss:   2.60, total_loss:  26.41\n",
            "[[161 208 524 697   1]]\n",
            "[[ 84  71 446 398   2]]\n",
            "[[108  28 419 299   1 100  65 361 353   1]]\n",
            "[[  4  43 286 288   0]]\n",
            "epoch:14 step:   36/60, lr:0.000021, giou_loss:   0.70, conf_loss:  18.45, prob_loss:   0.37, total_loss:  19.52\n",
            "[[147  45 288 202   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[ 63  18 370 316   2]]\n",
            "[[  4   2 327 336   0]]\n",
            "[[ 52 106 371 218   1]]\n",
            "epoch:14 step:   37/60, lr:0.000021, giou_loss:   0.89, conf_loss:  18.70, prob_loss:   0.35, total_loss:  19.94\n",
            "[[ 45  76 529 366   1]]\n",
            "[[  16  228  823 1167    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "[[207  66 663 435   2]]\n",
            "[[103  29 325 248   2 213  98 413 320   2]]\n",
            "epoch:14 step:   38/60, lr:0.000021, giou_loss:   0.65, conf_loss:  18.85, prob_loss:   3.46, total_loss:  22.96\n",
            "[[ 14  59 401 252   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[ 67  23 663 690   1]]\n",
            "[[ 60  30 234 205   0  18  22 168 190   0]]\n",
            "[[397 213 961 793   0  78 176 590 665   0]]\n",
            "epoch:14 step:   39/60, lr:0.000021, giou_loss:   0.84, conf_loss:  18.58, prob_loss:   1.36, total_loss:  20.78\n",
            "[[129  17 778 721   0]]\n",
            "[[  1   9 257 239   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "[[164  11 410 272   2]]\n",
            "[[ 64 101 466 190   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "epoch:14 step:   40/60, lr:0.000021, giou_loss:   1.03, conf_loss:  19.27, prob_loss:   1.33, total_loss:  21.63\n",
            "[[ 35 399 937 877   1]]\n",
            "[[ 47  79 296 331   0 314 112 594 385   0]]\n",
            "[[ 37  25 338 345   2]]\n",
            "[[239  52 597 410   2  77 133 674 590   1]]\n",
            "epoch:14 step:   41/60, lr:0.000021, giou_loss:   0.77, conf_loss:  18.01, prob_loss:   0.44, total_loss:  19.21\n",
            "[[243   2 824 407   1  24 174 599 657   1]]\n",
            "[[ 61 129 250 323   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "[[ 50 216 346 534   0]]\n",
            "[[404 173 827 595   0 401 115 725 441   0]]\n",
            "epoch:14 step:   42/60, lr:0.000021, giou_loss:   1.06, conf_loss:  19.50, prob_loss:   0.71, total_loss:  21.27\n",
            "[[ 79 233 441 598   0]]\n",
            "[[ 80 137 244 287   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "[[ 47  29 243 212   0]]\n",
            "[[ 49  53 226 216   0]]\n",
            "epoch:14 step:   43/60, lr:0.000021, giou_loss:   1.07, conf_loss:  19.03, prob_loss:   3.42, total_loss:  23.52\n",
            "[[110   2 461 375   0 363  83 770 514   0]]\n",
            "[[ 35 106 723 626   1   4 236 433 756   1]]\n",
            "[[139  48 225 136   2  83 107 161 198   2]]\n",
            "[[  9  45 415 464   0]]\n",
            "epoch:14 step:   44/60, lr:0.000020, giou_loss:   0.67, conf_loss:  19.03, prob_loss:   0.96, total_loss:  20.66\n",
            "[[ 70  23 219 238   1]]\n",
            "[[ 70  75 303 294   2]]\n",
            "[[  56   23 1710  920    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[ 45  57 214 212   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "epoch:14 step:   45/60, lr:0.000020, giou_loss:   0.90, conf_loss:  18.66, prob_loss:   0.36, total_loss:  19.92\n",
            "[[ 12  71 189 238   0]]\n",
            "[[ 30   4 304 271   2]]\n",
            "[[  1  67 202 272   2]]\n",
            "[[  4   3 423 197   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "epoch:14 step:   46/60, lr:0.000020, giou_loss:   0.75, conf_loss:  18.88, prob_loss:   0.53, total_loss:  20.15\n",
            "[[213  33 459 258   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "[[  43    4 2499 3894    1]]\n",
            "[[ 42  86 282 330   2 291  81 545 348   2]]\n",
            "[[ 10   3 258 259   2]]\n",
            "epoch:14 step:   47/60, lr:0.000020, giou_loss:   0.95, conf_loss:  19.68, prob_loss:   1.29, total_loss:  21.91\n",
            "[[  1   5 505 532   0]]\n",
            "[[ 95   6 565 448   0]]\n",
            "[[ 56  25 312 281   0]]\n",
            "[[ 58   7 451 409   2]]\n",
            "epoch:14 step:   48/60, lr:0.000020, giou_loss:   0.46, conf_loss:  18.66, prob_loss:   0.50, total_loss:  19.62\n",
            "[[ 346   47 1008  750    2]]\n",
            "[[ 37  84 594 318   1]]\n",
            "[[ 13  74 515 277   1]]\n",
            "[[ 140  196 1182  687    1   68  224 1011  751    1]]\n",
            "epoch:14 step:   49/60, lr:0.000020, giou_loss:   0.97, conf_loss:  18.06, prob_loss:   0.25, total_loss:  19.27\n",
            "[[ 75  70 172 175   2]]\n",
            "[[666 371 978 659   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[196 317 575 666   0]]\n",
            "[[ 26  82 414 257   1]]\n",
            "epoch:14 step:   50/60, lr:0.000020, giou_loss:   1.57, conf_loss:  18.96, prob_loss:   0.82, total_loss:  21.35\n",
            "[[  2   6 157 153   0]]\n",
            "[[ 65  18 693 192   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "[[130  25 295 220   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[  76   22  840  364    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "epoch:14 step:   51/60, lr:0.000020, giou_loss:   0.99, conf_loss:  19.45, prob_loss:   1.12, total_loss:  21.57\n",
            "[[  6   3 293 289   0]]\n",
            "[[144 136 470 411   0]]\n",
            "[[435 173 947 730   0]]\n",
            "[[127  40 488 331   2]]\n",
            "epoch:14 step:   52/60, lr:0.000020, giou_loss:   0.56, conf_loss:  17.73, prob_loss:   1.21, total_loss:  19.50\n",
            "[[104  61 656 335   1]]\n",
            "[[405 131 779 509   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "[[  6   7 153 151   0]]\n",
            "[[214 159 296 247   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "epoch:14 step:   53/60, lr:0.000019, giou_loss:   0.85, conf_loss:  19.43, prob_loss:   1.76, total_loss:  22.05\n",
            "[[ 31  68 217 250   0]]\n",
            "[[132   7 390 255   2 357  35 656 334   2]]\n",
            "[[ 38   9 184 152   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "[[  7   4 192  96   1 305  81 489 175   1]]\n",
            "epoch:14 step:   54/60, lr:0.000019, giou_loss:   0.86, conf_loss:  18.25, prob_loss:   0.90, total_loss:  20.01\n",
            "[[163 568 557 949   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "[[ 174   10 1487 1077    1]]\n",
            "[[ 41  65 506 521   2]]\n",
            "[[ 10  26 414 170   1]]\n",
            "epoch:14 step:   55/60, lr:0.000019, giou_loss:   0.87, conf_loss:  17.96, prob_loss:   0.60, total_loss:  19.43\n",
            "[[ 323    1 1896  624    1]]\n",
            "[[ 54  72 604 624   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "[[311 143 549 421   2 347 127 603 398   2]]\n",
            "[[ 98  12 341 235   2]]\n",
            "epoch:14 step:   56/60, lr:0.000019, giou_loss:   0.73, conf_loss:  17.87, prob_loss:   0.51, total_loss:  19.11\n",
            "[[114 105 284 393   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[  1  31 294 147   1]]\n",
            "[[  2  30 437 472   0]]\n",
            "[[162 194 582 570   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "epoch:14 step:   57/60, lr:0.000019, giou_loss:   1.21, conf_loss:  18.80, prob_loss:   1.98, total_loss:  21.99\n",
            "[[141  71 577 308   1]]\n",
            "[[289  57 687 472   0]]\n",
            "[[412  92 636 323   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "[[355 289 531 459   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "epoch:14 step:   58/60, lr:0.000019, giou_loss:   1.44, conf_loss:  20.03, prob_loss:   2.18, total_loss:  23.65\n",
            "[[ 47   1 132  95   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "[[ 36 158 492 644   2]]\n",
            "[[ 22  29 143 172   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "[[ 40 199 207 376   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "epoch:14 step:   59/60, lr:0.000019, giou_loss:   1.24, conf_loss:  20.02, prob_loss:   1.58, total_loss:  22.84\n",
            "[[ 66  44 373 376   0]]\n",
            "[[ 62  77 245 267   2]]\n",
            "[[315 231 538 445   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "[[275 242 499 484   2]]\n",
            "epoch:14 step:    0/60, lr:0.000019, giou_loss:   0.92, conf_loss:  17.93, prob_loss:   0.62, total_loss:  19.47\n",
            "[[ 37  52 369 381   2]]\n",
            "[[197  72 316 187   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "[[101  22 581 226   1]]\n",
            "[[ 76   7 328 266   2]]\n",
            "epoch:14 step:    1/60, lr:0.000019, giou_loss:   1.05, conf_loss:  18.90, prob_loss:   1.62, total_loss:  21.57\n",
            "[[ 43 134 358 455   0]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[150 165 923 983   2]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[227  62 723 500   0]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[145  36 421 286   2]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[203 115 584 452   1]]\n",
            "[[174  20 364 199   2]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[155 105 453 436   0]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[111  51 372 277   2]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[133 165 922 525   1]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "\n",
            "\n",
            "giou_val_loss:    nan, conf_val_loss:  19.96, prob_val_loss:   1.68, total_val_loss:    nan\n",
            "\n",
            "\n",
            "[[ 69  43 218 258   1]]\n",
            "[[ 28   5 630 458   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[ 95 132 489 513   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "[[142   5 611 339   0   3  40 475 439   0]]\n",
            "epoch:15 step:    2/60, lr:0.000018, giou_loss:   0.91, conf_loss:  18.00, prob_loss:   0.38, total_loss:  19.29\n",
            "[[  1  88 420 282   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "[[ 48  53 255 261   0]]\n",
            "[[147  45 288 202   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[345 137 583 415   2 347 127 603 398   2]]\n",
            "epoch:15 step:    3/60, lr:0.000018, giou_loss:   0.75, conf_loss:  18.85, prob_loss:   0.38, total_loss:  19.98\n",
            "[[475  54 777 368   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "[[ 25 117 107 205   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "[[  6  20 175 175   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "[[ 57  46 178 189   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "epoch:15 step:    4/60, lr:0.000018, giou_loss:   1.54, conf_loss:  20.07, prob_loss:   3.21, total_loss:  24.81\n",
            "[[ 44   7 293 239   2]]\n",
            "[[ 84  71 446 398   2]]\n",
            "[[ 30   7 167 166   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "[[238 227 405 404   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "epoch:15 step:    5/60, lr:0.000018, giou_loss:   1.15, conf_loss:  20.13, prob_loss:   1.74, total_loss:  23.03\n",
            "[[232  96 465 315   2]]\n",
            "[[145  14 242 119   2]]\n",
            "[[ 41  57 287 318   2]]\n",
            "[[ 34  20 290 250   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "epoch:15 step:    6/60, lr:0.000018, giou_loss:   0.70, conf_loss:  18.48, prob_loss:   3.01, total_loss:  22.20\n",
            "[[137   9 444 307   2]]\n",
            "[[ 38  50 892 928   0]]\n",
            "[[ 88  18 323 262   0]]\n",
            "[[ 41  96 379 457   2  58  69 410 432   2]]\n",
            "epoch:15 step:    7/60, lr:0.000018, giou_loss:   0.57, conf_loss:  17.97, prob_loss:   0.61, total_loss:  19.15\n",
            "[[107 124 277 302   0 197 144 363 328   0]]\n",
            "[[452 314 616 464   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "[[  56   99 1413 1419    0]]\n",
            "[[311  29 918 632   0]]\n",
            "epoch:15 step:    8/60, lr:0.000018, giou_loss:   1.22, conf_loss:  19.33, prob_loss:   2.44, total_loss:  22.99\n",
            "[[117  82 302 174   1 305  81 489 175   1]]\n",
            "[[190  22 401 218   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[ 772  150 1389  770    2  990  370 1578  974    2]]\n",
            "[[ 361   53  933 1053    1]]\n",
            "epoch:15 step:    9/60, lr:0.000018, giou_loss:   1.85, conf_loss:  19.60, prob_loss:   3.06, total_loss:  24.51\n",
            "[[557 199 936 548   0]]\n",
            "[[120  24 383 312   0  79  18 338 270   0]]\n",
            "[[ 37   1 122  95   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "[[   3  198  974  982    1  445  447 1069 1046    0]]\n",
            "epoch:15 step:   10/60, lr:0.000018, giou_loss:   1.29, conf_loss:  18.53, prob_loss:   2.80, total_loss:  22.63\n",
            "[[132 229 355 443   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "[[373 216 751 627   2 147  48 869 476   1 169 180 839 798   1]]\n",
            "[[ 28  18 207 190   0]]\n",
            "[[ 17  31 159 176   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "epoch:15 step:   11/60, lr:0.000017, giou_loss:   1.34, conf_loss:  19.78, prob_loss:   2.57, total_loss:  23.68\n",
            "[[353 177 776 599   0 401 115 725 441   0]]\n",
            "[[ 64  42 377 168   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "[[465 391 890 800   2]]\n",
            "[[343 148 607 461   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "epoch:15 step:   12/60, lr:0.000017, giou_loss:   1.29, conf_loss:  19.14, prob_loss:   0.88, total_loss:  21.31\n",
            "[[ 33 116 935 594   1]]\n",
            "[[ 26  69 604 581   2]]\n",
            "[[125  57 349 299   2]]\n",
            "[[ 14  36 264 291   0]]\n",
            "epoch:15 step:   13/60, lr:0.000017, giou_loss:   0.43, conf_loss:  17.64, prob_loss:   0.20, total_loss:  18.26\n",
            "[[124  62 393 326   0]]\n",
            "[[ 31   6 466 448   0]]\n",
            "[[ 25   6 461 243   1]]\n",
            "[[103 210 655 484   1]]\n",
            "epoch:15 step:   14/60, lr:0.000017, giou_loss:   0.72, conf_loss:  17.78, prob_loss:   0.79, total_loss:  19.29\n",
            "[[ 74  10 497 339   1]]\n",
            "[[ 596  800 1230 1800    1  546  621 1071 1679    1]]\n",
            "[[274   0 637 347   0]]\n",
            "[[ 30  33 226 216   0]]\n",
            "epoch:15 step:   15/60, lr:0.000017, giou_loss:   0.81, conf_loss:  18.79, prob_loss:   2.67, total_loss:  22.27\n",
            "[[111  30 354 253   2]]\n",
            "[[ 89 104 602 616   2]]\n",
            "[[  40  199 1505 1773    2]]\n",
            "[[150  35 476 310   0]]\n",
            "epoch:15 step:   16/60, lr:0.000017, giou_loss:   0.45, conf_loss:  17.24, prob_loss:   0.43, total_loss:  18.12\n",
            "[[ 267   51 1070  872    0    1    1  407  617    0  104    4  624  557\n",
            "     0]]\n",
            "[[ 46  66 223 233   0]]\n",
            "[[ 237  691  840 1302    2]]\n",
            "[[ 86   6 714 180   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "epoch:15 step:   17/60, lr:0.000017, giou_loss:   0.65, conf_loss:  17.85, prob_loss:   0.23, total_loss:  18.73\n",
            "[[ 54  38 733 786   0]]\n",
            "[[ 91   1 421 333   2]]\n",
            "[[ 93 153 461 542   2]]\n",
            "[[ 85  26 609 384   1]]\n",
            "epoch:15 step:   18/60, lr:0.000017, giou_loss:   0.68, conf_loss:  17.42, prob_loss:   0.69, total_loss:  18.79\n",
            "[[ 30  55 116 143   2  83 107 161 198   2]]\n",
            "[[ 27  94 515 401   1]]\n",
            "[[ 304   97 1877  720    1]]\n",
            "[[ 24   7 347 341   0]]\n",
            "epoch:15 step:   19/60, lr:0.000017, giou_loss:   0.83, conf_loss:  19.09, prob_loss:   0.51, total_loss:  20.43\n",
            "[[ 17   8 304 294   0]]\n",
            "[[  5  39 253 295   2]]\n",
            "[[ 75  43 394 155   1]]\n",
            "[[  1 171 532 676   2]]\n",
            "epoch:15 step:   20/60, lr:0.000017, giou_loss:   0.40, conf_loss:  17.69, prob_loss:   0.95, total_loss:  19.04\n",
            "[[ 76  36 249 208   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "[[204  78 308 185   2]]\n",
            "[[264 115 622 473   2  77 133 674 590   1]]\n",
            "[[  5 129 475 571   0]]\n",
            "epoch:15 step:   21/60, lr:0.000016, giou_loss:   0.58, conf_loss:  17.89, prob_loss:   0.71, total_loss:  19.18\n",
            "[[230  25 826 692   1]]\n",
            "[[127  77 520 479   2]]\n",
            "[[ 43  21 350 353   0]]\n",
            "[[391 241 624 508   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "epoch:15 step:   22/60, lr:0.000016, giou_loss:   1.24, conf_loss:  18.76, prob_loss:   1.11, total_loss:  21.11\n",
            "[[109  78 659 630   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "[[ 82  98 541 348   1]]\n",
            "[[ 15   7 313 295   0]]\n",
            "[[134 208 896 943   2]]\n",
            "epoch:15 step:   23/60, lr:0.000016, giou_loss:   0.78, conf_loss:  17.34, prob_loss:   0.26, total_loss:  18.38\n",
            "[[102  18 463 309   2]]\n",
            "[[138  51 350 256   2]]\n",
            "[[ 65  14 220 161   0]]\n",
            "[[102  37 605 283   1]]\n",
            "epoch:15 step:   24/60, lr:0.000016, giou_loss:   0.67, conf_loss:  17.63, prob_loss:   0.69, total_loss:  18.98\n",
            "[[ 13 211 433 653   0]]\n",
            "[[ 43 128 714 385   1]]\n",
            "[[133 438 416 708   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[  4   1 197 203   0]]\n",
            "epoch:15 step:   25/60, lr:0.000016, giou_loss:   1.09, conf_loss:  19.33, prob_loss:   1.24, total_loss:  21.65\n",
            "[[ 320  132 1199 1018    2]]\n",
            "[[ 15  74 184 182   1]]\n",
            "[[199  67 309 177   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[263  43 564 363   2]]\n",
            "epoch:15 step:   26/60, lr:0.000016, giou_loss:   0.97, conf_loss:  18.35, prob_loss:   0.84, total_loss:  20.17\n",
            "[[ 61  87 389 389   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "[[264  25 506 280   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "[[  12   21  776  363    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "[[ 14  27 440 358   1]]\n",
            "epoch:15 step:   27/60, lr:0.000016, giou_loss:   0.88, conf_loss:  18.90, prob_loss:   2.97, total_loss:  22.75\n",
            "[[ 15   9 213 200   2 214  75 383 248   2]]\n",
            "[[ 51  25 303 284   2]]\n",
            "[[251  16 475 247   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "[[ 14  29 188 204   0  18  22 168 190   0]]\n",
            "epoch:15 step:   28/60, lr:0.000016, giou_loss:   0.63, conf_loss:  18.33, prob_loss:   0.76, total_loss:  19.71\n",
            "[[  7  80 229 299   2 213  98 413 320   2]]\n",
            "[[ 96  43 242 186   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "[[244  81 422 259   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "[[ 470  161  992  495    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "epoch:15 step:   29/60, lr:0.000016, giou_loss:   1.11, conf_loss:  18.46, prob_loss:   1.25, total_loss:  20.82\n",
            "[[ 13   7 277 279   2]]\n",
            "[[179 107 743 687   0  78 176 590 665   0]]\n",
            "[[ 79   2 201 125   2 139   5 275 149   2]]\n",
            "[[ 31 334 836 547   1]]\n",
            "epoch:15 step:   30/60, lr:0.000015, giou_loss:   0.80, conf_loss:  17.83, prob_loss:   0.48, total_loss:  19.11\n",
            "[[  2  42 295 158   1]]\n",
            "[[ 38 174 595 408   1]]\n",
            "[[ 78  79 257 249   2]]\n",
            "[[ 969  133 1392  540    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "epoch:15 step:   31/60, lr:0.000015, giou_loss:   1.43, conf_loss:  19.09, prob_loss:   0.68, total_loss:  21.20\n",
            "[[173  16 571 431   0]]\n",
            "[[  18   17 1525  793    1]]\n",
            "[[ 78  16 414 379   2]]\n",
            "[[  10  428 2466 4318    1]]\n",
            "epoch:15 step:   32/60, lr:0.000015, giou_loss:   0.64, conf_loss:  17.95, prob_loss:   0.37, total_loss:  18.97\n",
            "[[355 289 531 459   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "[[  68  158 1175  913    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "[[ 37  20 205 190   0]]\n",
            "[[ 68  79 300 249   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "epoch:15 step:   33/60, lr:0.000015, giou_loss:   1.27, conf_loss:  18.80, prob_loss:   2.20, total_loss:  22.28\n",
            "[[231  57 547 350   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "[[  6  18 206 206   0]]\n",
            "[[171 236 859 756   1   4 236 433 756   1]]\n",
            "[[ 97 120 551 576   2]]\n",
            "epoch:15 step:   34/60, lr:0.000015, giou_loss:   0.75, conf_loss:  17.81, prob_loss:   0.38, total_loss:  18.95\n",
            "[[355  23 474 138   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "[[   3    1 1817  924    1]]\n",
            "[[336  72 604 349   0]]\n",
            "[[ 22  38 148 178   0]]\n",
            "epoch:15 step:   35/60, lr:0.000015, giou_loss:   1.25, conf_loss:  18.92, prob_loss:   4.60, total_loss:  24.76\n",
            "[[175  54 454 335   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "[[ 166   70 1208  561    1   68  224 1011  751    1]]\n",
            "[[  7   1 236 225   2]]\n",
            "[[  7  12 346 214   1]]\n",
            "epoch:15 step:   36/60, lr:0.000015, giou_loss:   1.38, conf_loss:  19.35, prob_loss:   0.85, total_loss:  21.58\n",
            "[[125 213 421 485   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "[[ 600  381 1226 1519    1]]\n",
            "[[ 364  234 1204 1070    2]]\n",
            "[[  4 177 296 293   1]]\n",
            "epoch:15 step:   37/60, lr:0.000015, giou_loss:   1.73, conf_loss:  19.41, prob_loss:   2.46, total_loss:  23.59\n",
            "[[182 158 649 433   1 190 100 623 330   1]]\n",
            "[[  83   75 1950 1895    2]]\n",
            "[[165   2 362 772   1]]\n",
            "[[317 188 737 564   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "epoch:15 step:   38/60, lr:0.000015, giou_loss:   0.94, conf_loss:  17.67, prob_loss:   0.39, total_loss:  19.00\n",
            "[[ 73  24 274 229   2]]\n",
            "[[221  58 410 252   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "[[119  72 613 319   1 272  80 498 306   1]]\n",
            "[[ 260  120 1239 1115    0]]\n",
            "epoch:15 step:   39/60, lr:0.000015, giou_loss:   0.81, conf_loss:  17.78, prob_loss:   0.42, total_loss:  19.02\n",
            "[[ 11  43 592 448   1  24 174 599 657   1]]\n",
            "[[123  35 857 756   2 172 181 860 843   2]]\n",
            "[[  1  54 406 396   1]]\n",
            "[[ 48  21 667 635   0]]\n",
            "epoch:15 step:   40/60, lr:0.000014, giou_loss:   0.54, conf_loss:  18.02, prob_loss:   0.24, total_loss:  18.80\n",
            "[[103  79 559 565   2]]\n",
            "[[318 146 680 511   0]]\n",
            "[[ 24  20 178 165   2]]\n",
            "[[218 163 546 487   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "epoch:15 step:   41/60, lr:0.000014, giou_loss:   0.71, conf_loss:  18.19, prob_loss:   1.21, total_loss:  20.12\n",
            "[[  2  34 242 238   0]]\n",
            "[[ 99  83 458 416   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[ 63  48 273 310   1]]\n",
            "[[ 14  43 242 276   2]]\n",
            "epoch:15 step:   42/60, lr:0.000014, giou_loss:   0.67, conf_loss:  18.02, prob_loss:   0.32, total_loss:  19.01\n",
            "[[381 227 693 515   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[170 468 715 985   2]]\n",
            "[[ 61  18 317 274   0]]\n",
            "[[ 20  71 260 315   2 291  81 545 348   2]]\n",
            "epoch:15 step:   43/60, lr:0.000014, giou_loss:   0.97, conf_loss:  18.12, prob_loss:   0.95, total_loss:  20.04\n",
            "[[318  80 488 368   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[ 12 236 659 600   1]]\n",
            "[[ 29  85 361 414   2]]\n",
            "[[228 272 410 483   0]]\n",
            "epoch:15 step:   44/60, lr:0.000014, giou_loss:   0.95, conf_loss:  18.04, prob_loss:   1.39, total_loss:  20.38\n",
            "[[241  30 356 129   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "[[208  52 870 755   2]]\n",
            "[[ 114   11  537  437    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "[[ 15  62 340 391   0 304  62 575 353   0]]\n",
            "epoch:15 step:   45/60, lr:0.000014, giou_loss:   0.99, conf_loss:  18.56, prob_loss:   1.14, total_loss:  20.68\n",
            "[[150   6 654 533   0]]\n",
            "[[ 94 181 285 370   2 328 173 488 351   2]]\n",
            "[[462 130 603 371   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[ 19  76 277 324   2 357  35 656 334   2]]\n",
            "epoch:15 step:   46/60, lr:0.000014, giou_loss:   1.05, conf_loss:  18.87, prob_loss:   1.42, total_loss:  21.34\n",
            "[[183  60 648 516   2]]\n",
            "[[  4   9 180 198   0]]\n",
            "[[  8 154 344 524   2 430  58 759 380   2]]\n",
            "[[ 98  60 347 312   0 314 112 594 385   0]]\n",
            "epoch:15 step:   47/60, lr:0.000014, giou_loss:   1.03, conf_loss:  17.90, prob_loss:   2.58, total_loss:  21.52\n",
            "[[ 65  91 957 578   1]]\n",
            "[[ 302  140 1703 1531    2]]\n",
            "[[ 363   48 1318  684    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "[[ 13  71 199 253   0]]\n",
            "epoch:15 step:   48/60, lr:0.000014, giou_loss:   0.65, conf_loss:  17.07, prob_loss:   0.33, total_loss:  18.05\n",
            "[[ 16 279 665 983   0]]\n",
            "[[  1   4 283 249   0]]\n",
            "[[205 116 568 605   1]]\n",
            "[[ 322   88 1428 1115    0]]\n",
            "epoch:15 step:   49/60, lr:0.000014, giou_loss:   0.89, conf_loss:  17.79, prob_loss:   3.12, total_loss:  21.81\n",
            "[[349 227 861 784   0]]\n",
            "[[343  89 799 458   2]]\n",
            "[[111  12 298 214   2]]\n",
            "[[ 85 137 488 545   2]]\n",
            "epoch:15 step:   50/60, lr:0.000014, giou_loss:   0.54, conf_loss:  17.23, prob_loss:   0.55, total_loss:  18.32\n",
            "[[  5  52 411 471   0]]\n",
            "[[  6   3 324 307   1]]\n",
            "[[ 15  21 312 308   0]]\n",
            "[[ 602    8 1915 1075    1]]\n",
            "epoch:15 step:   51/60, lr:0.000013, giou_loss:   0.48, conf_loss:  17.79, prob_loss:   0.54, total_loss:  18.82\n",
            "[[376 560 541 755   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[ 17   2 404 195   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[196 299 570 677   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "[[ 52  45 532 249   1]]\n",
            "epoch:15 step:   52/60, lr:0.000013, giou_loss:   1.59, conf_loss:  19.08, prob_loss:   1.22, total_loss:  21.89\n",
            "[[   4  364  757 1046    0]]\n",
            "[[ 18  22 292 289   2]]\n",
            "[[165  26 584 424   2 664 128 919 369   2]]\n",
            "[[ 380  272 1187 1211    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "epoch:15 step:   53/60, lr:0.000013, giou_loss:   0.66, conf_loss:  17.49, prob_loss:   0.43, total_loss:  18.58\n",
            "[[  64   41  797  816    2  580   21 1273  753    2]]\n",
            "[[100 107 584 397   1]]\n",
            "[[266  50 615 418   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "[[289  47 498 268   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "epoch:15 step:   54/60, lr:0.000013, giou_loss:   0.78, conf_loss:  18.60, prob_loss:   3.22, total_loss:  22.59\n",
            "[[ 11 318 321 634   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[ 63  61 565 264   1]]\n",
            "[[ 490  277 1448 1017    2  822  643 1620 1066    2]]\n",
            "[[ 53  47 299 272   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "epoch:15 step:   55/60, lr:0.000013, giou_loss:   0.59, conf_loss:  17.60, prob_loss:   0.30, total_loss:  18.50\n",
            "[[  57  215 1711 1112    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[ 22  26 205 216   2]]\n",
            "[[  60   85 2792 2568    1]]\n",
            "[[246 120 442 365   1]]\n",
            "epoch:15 step:   56/60, lr:0.000013, giou_loss:   0.67, conf_loss:  17.87, prob_loss:   0.87, total_loss:  19.40\n",
            "[[ 10   8 414 152   1]]\n",
            "[[ 87  40 383 358   0]]\n",
            "[[  1   1 148 145   0]]\n",
            "[[  27  247  291  499    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "epoch:15 step:   57/60, lr:0.000013, giou_loss:   1.00, conf_loss:  18.97, prob_loss:   1.93, total_loss:  21.90\n",
            "[[149  50 592 445   1]]\n",
            "[[  2  45 435 370   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[401 252 706 558   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[ 19   1 719 708   0]]\n",
            "epoch:15 step:   58/60, lr:0.000013, giou_loss:   0.61, conf_loss:  17.86, prob_loss:   1.52, total_loss:  20.00\n",
            "[[180 173 531 546   0 363  83 770 514   0]]\n",
            "[[ 33 213 459 427   1]]\n",
            "[[ 67 107 347 408   2]]\n",
            "[[ 69 334 553 810   0   1 132 368 605   0]]\n",
            "epoch:15 step:   59/60, lr:0.000013, giou_loss:   0.69, conf_loss:  17.70, prob_loss:   0.67, total_loss:  19.06\n",
            "[[ 56 110 444 285   1]]\n",
            "[[ 35 135 255 336   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "[[149 233 551 322   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "[[  19  135 1199  601    1]]\n",
            "epoch:15 step:    0/60, lr:0.000013, giou_loss:   1.41, conf_loss:  18.96, prob_loss:   1.38, total_loss:  21.75\n",
            "[[179   5 490 276   1 100  65 361 353   1]]\n",
            "[[ 29  19 206 182   0]]\n",
            "[[ 38 173 471 602   0]]\n",
            "[[ 41  19 472 460   2]]\n",
            "epoch:15 step:    1/60, lr:0.000012, giou_loss:   0.46, conf_loss:  17.14, prob_loss:   0.59, total_loss:  18.20\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[155 105 453 436   0]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "[[145  36 421 286   2]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[111  51 372 277   2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[174  20 364 199   2]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[227  62 723 500   0]]\n",
            "[[150 165 923 983   2]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[133 165 922 525   1]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[203 115 584 452   1]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[ 61 130 415 473   2]]\n",
            "\n",
            "\n",
            "giou_val_loss:    nan, conf_val_loss:  20.97, prob_val_loss:   1.67, total_val_loss:    nan\n",
            "\n",
            "\n",
            "[[183  51 429 276   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "[[300  93 543 316   2]]\n",
            "[[ 27  18 655 192   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "[[ 325  560  748  967    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "epoch:16 step:    2/60, lr:0.000012, giou_loss:   1.41, conf_loss:  19.16, prob_loss:   0.86, total_loss:  21.43\n",
            "[[  2  13 295 129   1]]\n",
            "[[ 17   2 324 334   0]]\n",
            "[[ 39  55 445 474   0]]\n",
            "[[ 626  146 1229  757    2]]\n",
            "epoch:16 step:    3/60, lr:0.000012, giou_loss:   0.81, conf_loss:  17.62, prob_loss:   0.26, total_loss:  18.68\n",
            "[[329 265 936 868   0]]\n",
            "[[292  95 654 460   0]]\n",
            "[[140  72 368 305   2]]\n",
            "[[300  60 467 237   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "epoch:16 step:    4/60, lr:0.000012, giou_loss:   0.93, conf_loss:  18.03, prob_loss:   1.08, total_loss:  20.04\n",
            "[[ 11  43 481 485   0]]\n",
            "[[ 33  36 219 218   0]]\n",
            "[[ 47  23 410 512   1]]\n",
            "[[ 56  73 314 321   2 357  35 656 334   2]]\n",
            "epoch:16 step:    5/60, lr:0.000012, giou_loss:   0.69, conf_loss:  17.83, prob_loss:   0.43, total_loss:  18.95\n",
            "[[ 17   1 453 238   1]]\n",
            "[[123  93 260 252   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "[[312   1 431 116   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "[[139 125 441 439   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "epoch:16 step:    6/60, lr:0.000012, giou_loss:   1.90, conf_loss:  19.90, prob_loss:   2.19, total_loss:  23.99\n",
            "[[ 648  132  912  384    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "[[ 33   1 340 299   2]]\n",
            "[[124  39 452 341   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "[[123  58 546 387   1]]\n",
            "epoch:16 step:    7/60, lr:0.000012, giou_loss:   1.11, conf_loss:  18.60, prob_loss:   2.33, total_loss:  22.04\n",
            "[[ 57  38 183 178   0]]\n",
            "[[157 129 590 558   0]]\n",
            "[[  25  352 1679 1249    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[  51   96  573  430    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "epoch:16 step:    8/60, lr:0.000012, giou_loss:   0.72, conf_loss:  17.75, prob_loss:   0.74, total_loss:  19.20\n",
            "[[  61   28 1568  804    1]]\n",
            "[[ 104  102 1505 1493    2]]\n",
            "[[ 20  50 216 295   1]]\n",
            "[[120  63 421 383   2]]\n",
            "epoch:16 step:    9/60, lr:0.000012, giou_loss:   0.83, conf_loss:  18.00, prob_loss:   1.67, total_loss:  20.50\n",
            "[[174  62 607 387   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[ 74  90 342 367   0]]\n",
            "[[115  18 441 293   0]]\n",
            "[[ 289  152  923 1152    1  546  621 1071 1679    1]]\n",
            "epoch:16 step:   10/60, lr:0.000012, giou_loss:   1.03, conf_loss:  18.54, prob_loss:   0.44, total_loss:  20.01\n",
            "[[108  43 249 200   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[150   6 654 533   0]]\n",
            "[[  4  87 244 291   0]]\n",
            "[[ 168    1 1126  741    2  822  643 1620 1066    2]]\n",
            "epoch:16 step:   11/60, lr:0.000012, giou_loss:   0.78, conf_loss:  17.38, prob_loss:   0.43, total_loss:  18.59\n",
            "[[ 22  66 358 436   2 430  58 759 380   2]]\n",
            "[[ 19   3 198 175   0]]\n",
            "[[ 10   1 259 233   2]]\n",
            "[[602 238 881 519   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "epoch:16 step:   12/60, lr:0.000011, giou_loss:   1.16, conf_loss:  18.89, prob_loss:   1.09, total_loss:  21.13\n",
            "[[ 77 127 496 525   2 664 128 919 369   2]]\n",
            "[[ 24  70 337 196   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "[[ 99  10 519 386   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "[[ 19  61 421 150   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "epoch:16 step:   13/60, lr:0.000011, giou_loss:   1.31, conf_loss:  17.62, prob_loss:   3.98, total_loss:  22.91\n",
            "[[144 118 313 273   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "[[  2  18 149 162   0]]\n",
            "[[397 213 961 793   0  78 176 590 665   0]]\n",
            "[[ 17   4 328 275   1 100  65 361 353   1]]\n",
            "epoch:16 step:   14/60, lr:0.000011, giou_loss:   0.58, conf_loss:  17.00, prob_loss:   1.67, total_loss:  19.26\n",
            "[[  4  17 233 241   2]]\n",
            "[[ 25  40 917 527   1]]\n",
            "[[ 103  625 2559 4515    1]]\n",
            "[[  64   86 1106  577    1   68  224 1011  751    1]]\n",
            "epoch:16 step:   15/60, lr:0.000011, giou_loss:   0.88, conf_loss:  17.77, prob_loss:   1.24, total_loss:  19.88\n",
            "[[ 80 261 385 567   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[ 96  96 217 239   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "[[ 45  24 471 355   1]]\n",
            "[[ 12   5 331 117   1]]\n",
            "epoch:16 step:   16/60, lr:0.000011, giou_loss:   0.85, conf_loss:  17.93, prob_loss:   0.35, total_loss:  19.12\n",
            "[[ 76  59 450 437   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "[[ 60  76 224 226   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "[[ 43  37 226 227   2]]\n",
            "[[ 22  65 371 433   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "epoch:16 step:   17/60, lr:0.000011, giou_loss:   0.94, conf_loss:  19.06, prob_loss:   2.32, total_loss:  22.31\n",
            "[[  16   34 1196  500    1]]\n",
            "[[178  61 866 581   1   4 236 433 756   1]]\n",
            "[[   3   56  736  831    2  580   21 1273  753    2]]\n",
            "[[ 81 211 501 653   0]]\n",
            "epoch:16 step:   18/60, lr:0.000011, giou_loss:   0.68, conf_loss:  17.44, prob_loss:   0.26, total_loss:  18.38\n",
            "[[  4  16 228 247   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "[[ 16   8 171 155   0]]\n",
            "[[ 51   3 414 350   0]]\n",
            "[[257 207 435 385   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "epoch:16 step:   19/60, lr:0.000011, giou_loss:   0.78, conf_loss:  17.36, prob_loss:   1.16, total_loss:  19.30\n",
            "[[121  15 645 373   1]]\n",
            "[[ 75  21 563 328   1]]\n",
            "[[123  25 293 203   0 197 144 363 328   0]]\n",
            "[[264  24 687 446   0 401 115 725 441   0]]\n",
            "epoch:16 step:   20/60, lr:0.000011, giou_loss:   0.72, conf_loss:  17.69, prob_loss:   1.16, total_loss:  19.58\n",
            "[[319  93 489 381   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[ 163  294 1003 1130    2]]\n",
            "[[ 53  62 194 303   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[434  36 899 492   2]]\n",
            "epoch:16 step:   21/60, lr:0.000011, giou_loss:   1.33, conf_loss:  19.36, prob_loss:   5.18, total_loss:  25.87\n",
            "[[145 302 378 569   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "[[284 161 581 448   0]]\n",
            "[[  25   71 1382 1391    0]]\n",
            "[[  90   13 1403 1080    1]]\n",
            "epoch:16 step:   22/60, lr:0.000011, giou_loss:   0.84, conf_loss:  17.62, prob_loss:   2.03, total_loss:  20.50\n",
            "[[ 20  29 102 117   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "[[160  66 663 312   1]]\n",
            "[[ 43   3 430 196   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[186  49 738 323   1]]\n",
            "epoch:16 step:   23/60, lr:0.000011, giou_loss:   0.90, conf_loss:  17.66, prob_loss:   0.47, total_loss:  19.03\n",
            "[[  1  82 420 276   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "[[  5  10 323 314   1]]\n",
            "[[ 63  63 565 266   1]]\n",
            "[[ 19 128 445 342   1]]\n",
            "epoch:16 step:   24/60, lr:0.000010, giou_loss:   1.08, conf_loss:  17.71, prob_loss:   2.87, total_loss:  21.66\n",
            "[[ 93 142 461 531   2]]\n",
            "[[ 18  72 267 324   0 314 112 594 385   0]]\n",
            "[[261 113 403 258   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "[[   5    5 1819  928    1]]\n",
            "epoch:16 step:   25/60, lr:0.000010, giou_loss:   0.82, conf_loss:  18.22, prob_loss:   2.72, total_loss:  21.76\n",
            "[[ 37  80 210 252   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "[[  4  37 268 309   2]]\n",
            "[[ 15   4 263 260   2]]\n",
            "[[ 398  147 1160  882    2]]\n",
            "epoch:16 step:   26/60, lr:0.000010, giou_loss:   0.53, conf_loss:  17.50, prob_loss:   1.20, total_loss:  19.23\n",
            "[[ 43 135 219 324   0]]\n",
            "[[341 156 692 529   0 363  83 770 514   0]]\n",
            "[[337  83 579 338   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "[[158  54 488 386   2]]\n",
            "epoch:16 step:   27/60, lr:0.000010, giou_loss:   0.63, conf_loss:  17.78, prob_loss:   0.61, total_loss:  19.01\n",
            "[[  3  61 342 263   1]]\n",
            "[[241  60 666 469   2]]\n",
            "[[  7 249 290 519   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[  48  165 2780 2648    1]]\n",
            "epoch:16 step:   28/60, lr:0.000010, giou_loss:   0.65, conf_loss:  17.56, prob_loss:   0.25, total_loss:  18.47\n",
            "[[159  45 520 336   2]]\n",
            "[[ 220  362 1027 1301    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "[[  4  71 216 276   2]]\n",
            "[[ 18  83 689 340   1]]\n",
            "epoch:16 step:   29/60, lr:0.000010, giou_loss:   0.78, conf_loss:  17.57, prob_loss:   1.54, total_loss:  19.89\n",
            "[[ 379  162 1026  526    1]]\n",
            "[[ 87 137 325 415   2 347 127 603 398   2]]\n",
            "[[ 176  217 1749  840    1]]\n",
            "[[ 57  69 911 947   0]]\n",
            "epoch:16 step:   30/60, lr:0.000010, giou_loss:   0.86, conf_loss:  17.80, prob_loss:   2.47, total_loss:  21.13\n",
            "[[ 444   32 1022  544    2]]\n",
            "[[ 110  730  913 1551    0    1    1  407  617    0  104    4  624  557\n",
            "     0]]\n",
            "[[ 11  43 592 448   1  24 174 599 657   1]]\n",
            "[[ 37   3 244 211   0]]\n",
            "epoch:16 step:   31/60, lr:0.000010, giou_loss:   0.57, conf_loss:  17.53, prob_loss:   0.34, total_loss:  18.44\n",
            "[[ 69   8 215 151   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "[[ 79  98 325 359   2]]\n",
            "[[ 75  18 262 220   2]]\n",
            "[[ 71 157 262 346   2 328 173 488 351   2]]\n",
            "epoch:16 step:   32/60, lr:0.000010, giou_loss:   0.72, conf_loss:  16.84, prob_loss:   3.77, total_loss:  21.32\n",
            "[[ 10  17 284 284   2]]\n",
            "[[315 231 538 445   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "[[ 22  20 176 165   2]]\n",
            "[[ 20  43 455 485   0]]\n",
            "epoch:16 step:   33/60, lr:0.000010, giou_loss:   0.96, conf_loss:  18.16, prob_loss:   3.58, total_loss:  22.69\n",
            "[[ 804  233 1376 1233    1]]\n",
            "[[ 36  71 213 234   0]]\n",
            "[[ 33  63 629 730   1]]\n",
            "[[109  64 447 425   2  58  69 410 432   2]]\n",
            "epoch:16 step:   34/60, lr:0.000010, giou_loss:   1.30, conf_loss:  17.85, prob_loss:   1.09, total_loss:  20.24\n",
            "[[ 85 137 488 545   2]]\n",
            "[[262  92 459 862   1]]\n",
            "[[ 93  79 272 249   2]]\n",
            "[[ 16  23 303 309   0]]\n",
            "epoch:16 step:   35/60, lr:0.000010, giou_loss:   0.84, conf_loss:  16.93, prob_loss:   0.56, total_loss:  18.34\n",
            "[[174  23 406 193   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "[[ 90  92 468 503   2 147  48 869 476   1 169 180 839 798   1]]\n",
            "[[ 65  70 519 526   2]]\n",
            "[[291  23 531 267   2 291  81 545 348   2]]\n",
            "epoch:16 step:   36/60, lr:0.000009, giou_loss:   0.59, conf_loss:  17.29, prob_loss:   0.69, total_loss:  18.57\n",
            "[[152  24 550 439   0]]\n",
            "[[  1  36 406 378   1]]\n",
            "[[108  46 501 448   2]]\n",
            "[[ 25  45 199 220   0  18  22 168 190   0]]\n",
            "epoch:16 step:   37/60, lr:0.000009, giou_loss:   0.48, conf_loss:  17.17, prob_loss:   0.54, total_loss:  18.19\n",
            "[[ 94  39 204 149   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[ 19  18 187 188   0]]\n",
            "[[  1  33 363 360   2]]\n",
            "[[ 415  282 1032  902    2  990  370 1578  974    2]]\n",
            "epoch:16 step:   38/60, lr:0.000009, giou_loss:   0.80, conf_loss:  17.34, prob_loss:   0.47, total_loss:  18.60\n",
            "[[243  68 637 449   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "[[ 102   99 1969 1919    2]]\n",
            "[[199  82 384 174   1 305  81 489 175   1]]\n",
            "[[  6  33 437 474   2]]\n",
            "epoch:16 step:   39/60, lr:0.000009, giou_loss:   1.36, conf_loss:  17.95, prob_loss:   1.10, total_loss:  20.42\n",
            "[[119  34 477 392   2  77 133 674 590   1]]\n",
            "[[235  59 897 762   2]]\n",
            "[[210   9 910 716   0]]\n",
            "[[ 94   3 588 250   1 272  80 498 306   1]]\n",
            "epoch:16 step:   40/60, lr:0.000009, giou_loss:   0.82, conf_loss:  17.63, prob_loss:   0.63, total_loss:  19.07\n",
            "[[ 17  57 421 201   1]]\n",
            "[[  3  14 339 377   2]]\n",
            "[[ 28  56 224 239   0]]\n",
            "[[356  72 906 624   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "epoch:16 step:   41/60, lr:0.000009, giou_loss:   0.61, conf_loss:  17.15, prob_loss:   0.49, total_loss:  18.24\n",
            "[[  1  55 194 257   0]]\n",
            "[[162 113 384 332   2 213  98 413 320   2]]\n",
            "[[  8  26 300 142   1]]\n",
            "[[ 27 159 832 372   1]]\n",
            "epoch:16 step:   42/60, lr:0.000009, giou_loss:   0.73, conf_loss:  17.32, prob_loss:   0.36, total_loss:  18.41\n",
            "[[ 24 100 304 401   2]]\n",
            "[[251 119 563 407   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[ 28  14 125 119   2]]\n",
            "[[231  57 547 350   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "epoch:16 step:   43/60, lr:0.000009, giou_loss:   1.14, conf_loss:  18.39, prob_loss:   3.13, total_loss:  22.67\n",
            "[[220 381 431 577   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[ 15   2 265 257   0]]\n",
            "[[ 92 241 268 411   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "[[ 74  37 159 131   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "epoch:16 step:   44/60, lr:0.000009, giou_loss:   0.95, conf_loss:  18.63, prob_loss:   1.90, total_loss:  21.48\n",
            "[[  5  12 205 200   0]]\n",
            "[[172  54 405 273   2]]\n",
            "[[  3  23 259 279   0]]\n",
            "[[ 70  25 290 226   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "epoch:16 step:   45/60, lr:0.000009, giou_loss:   0.66, conf_loss:  18.28, prob_loss:   2.54, total_loss:  21.48\n",
            "[[ 97  62 366 326   0]]\n",
            "[[ 489  357 1223 1078    2  172  181  860  843    2]]\n",
            "[[407 277 522 376   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "[[ 35   5 204 113   1]]\n",
            "epoch:16 step:   46/60, lr:0.000009, giou_loss:   1.40, conf_loss:  18.51, prob_loss:   0.85, total_loss:  20.75\n",
            "[[155  34 364 255   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "[[ 51  25 303 284   2]]\n",
            "[[ 29   2 327 290   0]]\n",
            "[[  2   5 212 267   1]]\n",
            "epoch:16 step:   47/60, lr:0.000009, giou_loss:   0.52, conf_loss:  17.15, prob_loss:   0.48, total_loss:  18.16\n",
            "[[ 59 257 438 606   0]]\n",
            "[[267 253 456 447   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "[[135  26 317 237   0]]\n",
            "[[   0  437  955 1073    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "epoch:16 step:   48/60, lr:0.000009, giou_loss:   1.16, conf_loss:  18.09, prob_loss:   5.63, total_loss:  24.88\n",
            "[[107 120 193 208   2  83 107 161 198   2]]\n",
            "[[ 14 124 470 493   2]]\n",
            "[[  3  30 285 275   0]]\n",
            "[[ 591   48 1103  605    0]]\n",
            "epoch:16 step:   49/60, lr:0.000008, giou_loss:   0.80, conf_loss:  18.01, prob_loss:   0.55, total_loss:  19.36\n",
            "[[ 45  66 504 316   1]]\n",
            "[[143  80 341 271   2 214  75 383 248   2]]\n",
            "[[148 127 628 331   1]]\n",
            "[[  0  76 296 394   0]]\n",
            "epoch:16 step:   50/60, lr:0.000008, giou_loss:   0.79, conf_loss:  18.57, prob_loss:   0.43, total_loss:  19.79\n",
            "[[  1  14 324 348   0]]\n",
            "[[313 153 478 348   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[ 57 308 541 784   0   1 132 368 605   0]]\n",
            "[[ 13  97 323 413   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "epoch:16 step:   51/60, lr:0.000008, giou_loss:   0.87, conf_loss:  18.22, prob_loss:   0.89, total_loss:  19.98\n",
            "[[114  23 733 637   0]]\n",
            "[[ 82  83 346 396   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "[[ 30  67 207 234   0]]\n",
            "[[ 135  252 1106 1036    1  445  447 1069 1046    0]]\n",
            "epoch:16 step:   52/60, lr:0.000008, giou_loss:   0.66, conf_loss:  17.57, prob_loss:   0.39, total_loss:  18.62\n",
            "[[ 69  16 365 288   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "[[ 326  160 1432 1187    0]]\n",
            "[[ 10   7 132 130   2 139   5 275 149   2]]\n",
            "[[244 187 700 673   2]]\n",
            "epoch:16 step:   53/60, lr:0.000008, giou_loss:   0.66, conf_loss:  18.01, prob_loss:   2.22, total_loss:  20.89\n",
            "[[120  44 604 334   1]]\n",
            "[[ 93  14 242 229   1]]\n",
            "[[ 793  161 1900  916    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "[[159   6 484 335   0 304  62 575 353   0]]\n",
            "epoch:16 step:   54/60, lr:0.000008, giou_loss:   1.10, conf_loss:  17.86, prob_loss:   1.08, total_loss:  20.04\n",
            "[[ 27  25 929 503   1]]\n",
            "[[ 320   17  743  443    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "[[ 392    7 1041  711    0]]\n",
            "[[165  81 389 323   2]]\n",
            "epoch:16 step:   55/60, lr:0.000008, giou_loss:   0.79, conf_loss:  16.96, prob_loss:   0.24, total_loss:  18.00\n",
            "[[  1 210 558 444   1]]\n",
            "[[ 21  30 534 542   2]]\n",
            "[[   4  364  757 1046    0]]\n",
            "[[ 93   9 481 184   1]]\n",
            "epoch:16 step:   56/60, lr:0.000008, giou_loss:   0.64, conf_loss:  16.82, prob_loss:   0.27, total_loss:  17.72\n",
            "[[ 183  230  947  572    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "[[ 50  67 306 297   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "[[232  43 495 331   0  79  18 338 270   0]]\n",
            "[[231  36 335 143   2]]\n",
            "epoch:16 step:   57/60, lr:0.000008, giou_loss:   1.12, conf_loss:  17.37, prob_loss:   0.76, total_loss:  19.24\n",
            "[[408   2 953 519   2]]\n",
            "[[ 39  51 371 380   2]]\n",
            "[[141  63 500 396   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[ 73  30 604 535   2]]\n",
            "epoch:16 step:   58/60, lr:0.000008, giou_loss:   0.80, conf_loss:  17.26, prob_loss:   1.09, total_loss:  19.14\n",
            "[[254 163 582 487   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "[[ 222   79 1201 1074    0]]\n",
            "[[179  38 414 282   0]]\n",
            "[[  68  305  947 1191    2]]\n",
            "epoch:16 step:   59/60, lr:0.000008, giou_loss:   0.46, conf_loss:  17.42, prob_loss:   0.50, total_loss:  18.38\n",
            "[[ 52  18 654 471   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[240  53 709 387   0   3  40 475 439   0]]\n",
            "[[ 553  120 1232  868    0]]\n",
            "[[ 103  294 1568 1868    2]]\n",
            "epoch:16 step:    0/60, lr:0.000008, giou_loss:   0.67, conf_loss:  16.84, prob_loss:   0.34, total_loss:  17.86\n",
            "[[ 600  381 1226 1519    1]]\n",
            "[[109 125 310 330   2]]\n",
            "[[150 121 617 396   1 190 100 623 330   1]]\n",
            "[[204  63 647 458   1]]\n",
            "epoch:16 step:    1/60, lr:0.000008, giou_loss:   1.18, conf_loss:  18.05, prob_loss:   1.40, total_loss:  20.63\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[143  40 415 346   0]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[145  36 421 286   2]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[203 115 584 452   1]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[150 165 923 983   2]]\n",
            "[[111  51 372 277   2]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[133 165 922 525   1]]\n",
            "[[155 105 453 436   0]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "\n",
            "\n",
            "giou_val_loss:    nan, conf_val_loss:  17.99, prob_val_loss:   1.80, total_val_loss:    nan\n",
            "\n",
            "\n",
            "[[ 15  87 255 291   0]]\n",
            "[[ 85 137 488 545   2]]\n",
            "[[298 102 647 470   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "[[162  24 665 270   1]]\n",
            "epoch:17 step:    2/60, lr:0.000007, giou_loss:   0.56, conf_loss:  17.44, prob_loss:   0.64, total_loss:  18.64\n",
            "[[ 53  64 179 204   0]]\n",
            "[[283  67 621 428   2  58  69 410 432   2]]\n",
            "[[  8  57 451 452   1]]\n",
            "[[ 39 129 591 403   1]]\n",
            "epoch:17 step:    3/60, lr:0.000007, giou_loss:   0.53, conf_loss:  16.95, prob_loss:   0.27, total_loss:  17.75\n",
            "[[ 30 117 489 367   1]]\n",
            "[[ 33  15 935 493   1]]\n",
            "[[ 752   72 1859  827    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "[[ 47  12 467 388   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "epoch:17 step:    4/60, lr:0.000007, giou_loss:   0.99, conf_loss:  17.14, prob_loss:   0.64, total_loss:  18.78\n",
            "[[ 28  27 364 390   2]]\n",
            "[[317  79 481 229   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "[[378 252 548 540   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[205  86 375 264   0 197 144 363 328   0]]\n",
            "epoch:17 step:    5/60, lr:0.000007, giou_loss:   1.54, conf_loss:  19.69, prob_loss:   2.62, total_loss:  23.85\n",
            "[[100  23 533 348   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[ 202  195  774 1195    1]]\n",
            "[[ 88  16 271 206   2]]\n",
            "[[ 68 161 187 276   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "epoch:17 step:    6/60, lr:0.000007, giou_loss:   1.01, conf_loss:  18.41, prob_loss:   2.13, total_loss:  21.56\n",
            "[[143  95 613 537   0]]\n",
            "[[421  16 733 304   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[ 55  68 442 261   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[ 71  16 584 528   2]]\n",
            "epoch:17 step:    7/60, lr:0.000007, giou_loss:   0.60, conf_loss:  17.53, prob_loss:   0.65, total_loss:  18.78\n",
            "[[  8  61 347 263   1]]\n",
            "[[  34   64 1848  987    1]]\n",
            "[[ 99  21 286 223   2]]\n",
            "[[330  77 598 354   0]]\n",
            "epoch:17 step:    8/60, lr:0.000007, giou_loss:   0.77, conf_loss:  17.24, prob_loss:   0.80, total_loss:  18.82\n",
            "[[332 291 700 680   2]]\n",
            "[[ 15  21 264 253   2]]\n",
            "[[ 83  40 975 527   1]]\n",
            "[[ 69  43 218 258   1]]\n",
            "epoch:17 step:    9/60, lr:0.000007, giou_loss:   0.61, conf_loss:  16.65, prob_loss:   1.52, total_loss:  18.79\n",
            "[[254 163 582 487   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "[[ 115   18 1094 1013    0]]\n",
            "[[206  42 784 554   2]]\n",
            "[[  8  17 155 161   0]]\n",
            "epoch:17 step:   10/60, lr:0.000007, giou_loss:   0.41, conf_loss:  17.15, prob_loss:   1.25, total_loss:  18.81\n",
            "[[ 366  248 1831 1822    2]]\n",
            "[[ 13   5 242 229   2]]\n",
            "[[ 50  11 506 497   2]]\n",
            "[[ 81 164 475 545   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "epoch:17 step:   11/60, lr:0.000007, giou_loss:   0.40, conf_loss:  16.39, prob_loss:   1.97, total_loss:  18.76\n",
            "[[ 47  78 327 379   2]]\n",
            "[[619 188 915 460   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "[[103  57 699 724   1]]\n",
            "[[255   1 562 299   2]]\n",
            "epoch:17 step:   12/60, lr:0.000007, giou_loss:   1.07, conf_loss:  18.10, prob_loss:   1.22, total_loss:  20.39\n",
            "[[  62  208 2794 2691    1]]\n",
            "[[ 434    1 1239  214    1]]\n",
            "[[ 19  97 140 240   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "[[171 236 859 756   1   4 236 433 756   1]]\n",
            "epoch:17 step:   13/60, lr:0.000007, giou_loss:   0.82, conf_loss:  17.56, prob_loss:   0.35, total_loss:  18.74\n",
            "[[ 28 101 274 362   2]]\n",
            "[[ 37  45 369 374   2]]\n",
            "[[  4  23 435 464   2]]\n",
            "[[ 68  25 501 454   0]]\n",
            "epoch:17 step:   14/60, lr:0.000007, giou_loss:   0.45, conf_loss:  16.64, prob_loss:   0.59, total_loss:  17.68\n",
            "[[ 38 164 153 263   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "[[ 674   27 1408  748    2  172  181  860  843    2]]\n",
            "[[ 73 145 252 315   2]]\n",
            "[[135   7 367 177   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "epoch:17 step:   15/60, lr:0.000007, giou_loss:   0.85, conf_loss:  17.90, prob_loss:   0.68, total_loss:  19.44\n",
            "[[ 13   8 329 301   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "[[ 48  53 255 261   0]]\n",
            "[[ 14  88 200 270   0]]\n",
            "[[ 360   48  624  300    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "epoch:17 step:   16/60, lr:0.000007, giou_loss:   0.47, conf_loss:  17.52, prob_loss:   0.56, total_loss:  18.56\n",
            "[[ 60  41 379 153   1]]\n",
            "[[ 19  59 473 515   2]]\n",
            "[[ 38  31 235 801   1]]\n",
            "[[132 247 779 611   1]]\n",
            "epoch:17 step:   17/60, lr:0.000006, giou_loss:   0.92, conf_loss:  17.49, prob_loss:   0.42, total_loss:  18.84\n",
            "[[315 231 538 445   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "[[204 154 624 596   0]]\n",
            "[[159  85 296 244   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "[[ 83  11 577 258   1 272  80 498 306   1]]\n",
            "epoch:17 step:   18/60, lr:0.000006, giou_loss:   1.68, conf_loss:  18.15, prob_loss:   0.85, total_loss:  20.68\n",
            "[[ 13  87 186 259   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "[[189  99 445 329   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "[[ 54 156 534 360   1]]\n",
            "[[184  88 640 457   2]]\n",
            "epoch:17 step:   19/60, lr:0.000006, giou_loss:   1.05, conf_loss:  17.82, prob_loss:   0.87, total_loss:  19.74\n",
            "[[   2  139  856 1017    0]]\n",
            "[[116   6 539 335   1]]\n",
            "[[ 14   1 288 268   2]]\n",
            "[[ 50  26 227 189   0]]\n",
            "epoch:17 step:   20/60, lr:0.000006, giou_loss:   0.58, conf_loss:  16.39, prob_loss:   0.58, total_loss:  17.55\n",
            "[[  68   97 1425 1417    0]]\n",
            "[[ 596  101 1199  712    2]]\n",
            "[[148 218 797 922   0]]\n",
            "[[ 45  27 288 250   2]]\n",
            "epoch:17 step:   21/60, lr:0.000006, giou_loss:   0.60, conf_loss:  16.42, prob_loss:   0.75, total_loss:  17.77\n",
            "[[  5   5 102 110   2]]\n",
            "[[   0  354  955  990    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "[[ 454    2 1767 1069    1]]\n",
            "[[132  52 365 319   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "epoch:17 step:   22/60, lr:0.000006, giou_loss:   0.76, conf_loss:  17.72, prob_loss:   0.55, total_loss:  19.03\n",
            "[[ 791  317 1170  666    0]]\n",
            "[[  2 167 559 401   1]]\n",
            "[[  10  625 2466 4515    1]]\n",
            "[[ 214  137  840 1275    1]]\n",
            "epoch:17 step:   23/60, lr:0.000006, giou_loss:   1.56, conf_loss:  19.42, prob_loss:   2.97, total_loss:  23.95\n",
            "[[191  43 853 746   2]]\n",
            "[[202  23 371 178   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "[[ 24   4 528 531   0]]\n",
            "[[  6   9 367 300   2]]\n",
            "epoch:17 step:   24/60, lr:0.000006, giou_loss:   0.58, conf_loss:  16.19, prob_loss:   0.49, total_loss:  17.26\n",
            "[[ 125   15 1779  912    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[ 70  25 290 226   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "[[  2  18 143 175   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[131  10 619 317   1]]\n",
            "epoch:17 step:   25/60, lr:0.000006, giou_loss:   0.72, conf_loss:  18.78, prob_loss:   1.82, total_loss:  21.32\n",
            "[[ 21 251 505 727   0   1 132 368 605   0]]\n",
            "[[ 21  23 313 139   1]]\n",
            "[[ 232  261 1111 1147    2]]\n",
            "[[ 28  23 280 282   2]]\n",
            "epoch:17 step:   26/60, lr:0.000006, giou_loss:   0.55, conf_loss:  16.63, prob_loss:   0.67, total_loss:  17.86\n",
            "[[ 11  60 417 479   0]]\n",
            "[[ 15 113 686 370   1]]\n",
            "[[ 43 206 607 786   0  78 176 590 665   0]]\n",
            "[[161  56 580 454   2 664 128 919 369   2]]\n",
            "epoch:17 step:   27/60, lr:0.000006, giou_loss:   0.63, conf_loss:  16.99, prob_loss:   0.49, total_loss:  18.11\n",
            "[[  4  29 183 201   0]]\n",
            "[[ 113    2  877  344    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "[[358  46 554 291   1]]\n",
            "[[436  72 738 386   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "epoch:17 step:   28/60, lr:0.000006, giou_loss:   1.15, conf_loss:  18.12, prob_loss:   2.69, total_loss:  21.96\n",
            "[[  7 105 476 439   0   3  40 475 439   0]]\n",
            "[[221  83 463 338   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "[[ 17  20 158 261   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[  4   9 202 200   2 214  75 383 248   2]]\n",
            "epoch:17 step:   29/60, lr:0.000006, giou_loss:   0.93, conf_loss:  18.34, prob_loss:   0.86, total_loss:  20.13\n",
            "[[  3  31 285 276   0]]\n",
            "[[ 19  30 638 644   0]]\n",
            "[[345 137 583 415   2 347 127 603 398   2]]\n",
            "[[ 63  44 319 300   0]]\n",
            "epoch:17 step:   30/60, lr:0.000006, giou_loss:   0.70, conf_loss:  17.61, prob_loss:   1.12, total_loss:  19.44\n",
            "[[228   8 654 339   1]]\n",
            "[[564 104 926 469   0]]\n",
            "[[ 34  95 359 424   0 304  62 575 353   0]]\n",
            "[[280  97 638 455   2  77 133 674 590   1]]\n",
            "epoch:17 step:   31/60, lr:0.000006, giou_loss:   0.83, conf_loss:  17.41, prob_loss:   0.92, total_loss:  19.16\n",
            "[[297 233 809 790   0]]\n",
            "[[184 110 582 525   0]]\n",
            "[[  5  30 424 224   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "[[142  54 382 298   2 291  81 545 348   2]]\n",
            "epoch:17 step:   32/60, lr:0.000006, giou_loss:   0.76, conf_loss:  16.78, prob_loss:   0.36, total_loss:  17.89\n",
            "[[139  47 315 217   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "[[  28  171  761  946    2  580   21 1273  753    2]]\n",
            "[[ 57 262 685 436   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "[[178   6 360 217   0]]\n",
            "epoch:17 step:   33/60, lr:0.000005, giou_loss:   1.12, conf_loss:  18.25, prob_loss:   1.95, total_loss:  21.33\n",
            "[[125  75 461 445   2 430  58 759 380   2]]\n",
            "[[197  72 339 217   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "[[211   4 676 460   2]]\n",
            "[[171  36 380 257   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "epoch:17 step:   34/60, lr:0.000005, giou_loss:   1.06, conf_loss:  17.87, prob_loss:   1.78, total_loss:  20.70\n",
            "[[ 256   91 1657 1482    2]]\n",
            "[[ 79  40 515 277   1]]\n",
            "[[124  62 393 326   0]]\n",
            "[[ 74  83 243 191   1]]\n",
            "epoch:17 step:   35/60, lr:0.000005, giou_loss:   0.73, conf_loss:  16.81, prob_loss:   0.30, total_loss:  17.84\n",
            "[[ 20   1 270 256   0]]\n",
            "[[147  43 293 186   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "[[284  21 533 273   0 314 112 594 385   0]]\n",
            "[[ 563  267 1197 1267    1  546  621 1071 1679    1]]\n",
            "epoch:17 step:   36/60, lr:0.000005, giou_loss:   0.87, conf_loss:  16.99, prob_loss:   0.29, total_loss:  18.15\n",
            "[[152  18 453 338   2]]\n",
            "[[282 136 471 330   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "[[368  73 626 321   2 357  35 656 334   2]]\n",
            "[[ 11   2 275 274   2]]\n",
            "epoch:17 step:   37/60, lr:0.000005, giou_loss:   0.69, conf_loss:  17.43, prob_loss:   0.87, total_loss:  18.99\n",
            "[[ 160   58 1667  834    1]]\n",
            "[[  43  244 1616  867    1]]\n",
            "[[  9   7 164 154   0]]\n",
            "[[  5  11 182 178   0]]\n",
            "epoch:17 step:   38/60, lr:0.000005, giou_loss:   0.46, conf_loss:  16.99, prob_loss:   0.76, total_loss:  18.21\n",
            "[[ 83   7 205 130   2 139   5 275 149   2]]\n",
            "[[158  54 488 386   2]]\n",
            "[[ 23  77 336 203   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "[[136  44 358 263   2 213  98 413 320   2]]\n",
            "epoch:17 step:   39/60, lr:0.000005, giou_loss:   1.05, conf_loss:  17.47, prob_loss:   1.93, total_loss:  20.45\n",
            "[[ 561    1 1261  708    0]]\n",
            "[[  4 159  86 247   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "[[156  74 419 362   0  79  18 338 270   0]]\n",
            "[[151  46 379 279   2]]\n",
            "epoch:17 step:   40/60, lr:0.000005, giou_loss:   0.78, conf_loss:  18.09, prob_loss:   0.85, total_loss:  19.73\n",
            "[[199  82 384 174   1 305  81 489 175   1]]\n",
            "[[188   1 795 604   0]]\n",
            "[[ 241  388 1044 1209    0    1    1  407  617    0  104    4  624  557\n",
            "     0]]\n",
            "[[ 22  14 309 300   0]]\n",
            "epoch:17 step:   41/60, lr:0.000005, giou_loss:   1.27, conf_loss:  18.62, prob_loss:   3.34, total_loss:  23.23\n",
            "[[242 302 667 711   2]]\n",
            "[[105 297 415 613   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[259 147 426 324   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "[[ 73  86 285 291   2]]\n",
            "epoch:17 step:   42/60, lr:0.000005, giou_loss:   0.91, conf_loss:  18.34, prob_loss:   1.09, total_loss:  20.34\n",
            "[[ 63  48 273 310   1]]\n",
            "[[ 54  28 733 776   0]]\n",
            "[[133 438 416 708   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[ 168    1 1126  741    2  822  643 1620 1066    2]]\n",
            "epoch:17 step:   43/60, lr:0.000005, giou_loss:   0.90, conf_loss:  18.48, prob_loss:   0.74, total_loss:  20.13\n",
            "[[119  37 478 370   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[ 568  282 1185  902    2  990  370 1578  974    2]]\n",
            "[[ 53  28 584 533   2]]\n",
            "[[ 30   4 278 260   2]]\n",
            "epoch:17 step:   44/60, lr:0.000005, giou_loss:   0.44, conf_loss:  16.91, prob_loss:   0.49, total_loss:  17.83\n",
            "[[160  75 548 250   1]]\n",
            "[[ 83  22 485 111   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "[[ 17  26 213 209   0]]\n",
            "[[144 136 470 411   0]]\n",
            "epoch:17 step:   45/60, lr:0.000005, giou_loss:   0.96, conf_loss:  17.05, prob_loss:   0.92, total_loss:  18.93\n",
            "[[ 36  38 271 282   0]]\n",
            "[[ 43  28 153 138   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[ 99  84 406 416   0]]\n",
            "[[ 366  575 1173 1514    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "epoch:17 step:   46/60, lr:0.000005, giou_loss:   0.95, conf_loss:  18.07, prob_loss:   2.22, total_loss:  21.24\n",
            "[[  1  60 294 176   1]]\n",
            "[[ 56  72 449 474   2]]\n",
            "[[457 474 736 755   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "[[  6  53 180 228   0  18  22 168 190   0]]\n",
            "epoch:17 step:   47/60, lr:0.000005, giou_loss:   1.06, conf_loss:  17.65, prob_loss:   1.42, total_loss:  20.13\n",
            "[[  14  133 1194  599    1]]\n",
            "[[  8  32 553 549   2]]\n",
            "[[ 22  48 107 142   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "[[  27   31  450  457    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "epoch:17 step:   48/60, lr:0.000005, giou_loss:   0.78, conf_loss:  16.57, prob_loss:   0.25, total_loss:  17.60\n",
            "[[  60   48 1927 1868    2]]\n",
            "[[412 176 790 587   2 147  48 869 476   1 169 180 839 798   1]]\n",
            "[[  4  17 366 344   2]]\n",
            "[[ 20   5 343 339   0]]\n",
            "epoch:17 step:   49/60, lr:0.000005, giou_loss:   0.44, conf_loss:  16.77, prob_loss:   0.53, total_loss:  17.74\n",
            "[[ 22  20 176 165   2]]\n",
            "[[ 39  13 204 208   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[  3  12 203 200   0]]\n",
            "[[297  84 878 489   1  24 174 599 657   1]]\n",
            "epoch:17 step:   50/60, lr:0.000004, giou_loss:   0.71, conf_loss:  17.76, prob_loss:   0.99, total_loss:  19.46\n",
            "[[   3  335  974 1119    1  445  447 1069 1046    0]]\n",
            "[[  5  16 323 320   1]]\n",
            "[[  74  221 1116  712    1   68  224 1011  751    1]]\n",
            "[[101  38 205 145   2]]\n",
            "epoch:17 step:   51/60, lr:0.000004, giou_loss:   0.71, conf_loss:  17.11, prob_loss:   2.45, total_loss:  20.28\n",
            "[[ 31  55 117 143   2  83 107 161 198   2]]\n",
            "[[151  53 653 256   1]]\n",
            "[[154   7 704 559   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "[[ 456  144 1562 1171    0]]\n",
            "epoch:17 step:   52/60, lr:0.000004, giou_loss:   0.63, conf_loss:  16.84, prob_loss:   0.41, total_loss:  17.87\n",
            "[[ 16   1 379 348   0]]\n",
            "[[  1  99 427 313   1]]\n",
            "[[202 109 435 328   2]]\n",
            "[[122  75 450 377   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "epoch:17 step:   53/60, lr:0.000004, giou_loss:   0.64, conf_loss:  16.52, prob_loss:   1.78, total_loss:  18.94\n",
            "[[  1  13 603 466   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[  0  32 405 374   1]]\n",
            "[[144  55 337 257   0]]\n",
            "[[ 11 293 202 482   2 328 173 488 351   2]]\n",
            "epoch:17 step:   54/60, lr:0.000004, giou_loss:   0.65, conf_loss:  16.88, prob_loss:   0.29, total_loss:  17.82\n",
            "[[ 434   65  857  472    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "[[280  95 576 413   0]]\n",
            "[[ 70 216 433 705   1]]\n",
            "[[424  56 798 434   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "epoch:17 step:   55/60, lr:0.000004, giou_loss:   0.97, conf_loss:  18.41, prob_loss:   1.10, total_loss:  20.47\n",
            "[[ 322  372 1162 1208    2]]\n",
            "[[172  33 418 258   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "[[ 77  27 374 314   0]]\n",
            "[[ 506  129 1259  811    0]]\n",
            "epoch:17 step:   56/60, lr:0.000004, giou_loss:   0.81, conf_loss:  16.91, prob_loss:   0.92, total_loss:  18.64\n",
            "[[  2  21 178 210   0]]\n",
            "[[122  49 346 291   2]]\n",
            "[[143 306 354 502   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[ 21  27 319 315   0]]\n",
            "epoch:17 step:   57/60, lr:0.000004, giou_loss:   0.78, conf_loss:  17.41, prob_loss:   0.69, total_loss:  18.88\n",
            "[[302  46 566 359   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "[[233 249 538 555   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[  7  19 411 163   1]]\n",
            "[[  1  29 202 234   2]]\n",
            "epoch:17 step:   58/60, lr:0.000004, giou_loss:   0.96, conf_loss:  16.96, prob_loss:   0.62, total_loss:  18.54\n",
            "[[ 292  159 1054  894    2]]\n",
            "[[179   5 490 276   1 100  65 361 353   1]]\n",
            "[[152 142 575 564   0 401 115 725 441   0]]\n",
            "[[109  48 633 406   1]]\n",
            "epoch:17 step:   59/60, lr:0.000004, giou_loss:   0.53, conf_loss:  16.60, prob_loss:   0.57, total_loss:  17.70\n",
            "[[ 50  45 534 335   1]]\n",
            "[[ 23  55 458 497   0]]\n",
            "[[156   7 334 185   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "[[  2  48 226 279   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "epoch:17 step:    0/60, lr:0.000004, giou_loss:   0.74, conf_loss:  16.77, prob_loss:   0.20, total_loss:  17.71\n",
            "[[ 89  71 440 444   0 363  83 770 514   0]]\n",
            "[[ 24  11 192 181   0]]\n",
            "[[110  67 577 342   1 190 100 623 330   1]]\n",
            "[[  65  153  587  487    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "epoch:17 step:    1/60, lr:0.000004, giou_loss:   0.76, conf_loss:  17.38, prob_loss:   0.75, total_loss:  18.89\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[155 105 453 436   0]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[145  36 421 286   2]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[166  92 638 559   2]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[111  51 372 277   2]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[150 165 923 983   2]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[279  14 562 295   2]]\n",
            "[[133 165 922 525   1]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[203 115 584 452   1]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[174  20 364 199   2]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "\n",
            "\n",
            "giou_val_loss:    nan, conf_val_loss:  18.58, prob_val_loss:   1.78, total_val_loss:    nan\n",
            "\n",
            "\n",
            "[[110 104 789 852   0]]\n",
            "[[ 91  62 237 205   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "[[ 63  38 332 302   0]]\n",
            "[[ 24  34 631 637   0]]\n",
            "epoch:18 step:    2/60, lr:0.000004, giou_loss:   0.75, conf_loss:  16.64, prob_loss:   1.53, total_loss:  18.92\n",
            "[[ 64  19 174 129   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[ 47  52 379 381   2]]\n",
            "[[433 161 552 276   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "[[ 38 185 709 442   1]]\n",
            "epoch:18 step:    3/60, lr:0.000004, giou_loss:   1.36, conf_loss:  18.59, prob_loss:   2.20, total_loss:  22.15\n",
            "[[163  90 330 267   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "[[  66  298 1573 1074    1]]\n",
            "[[ 98   1 485 194   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[ 272   68  695  475    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "epoch:18 step:    4/60, lr:0.000004, giou_loss:   0.91, conf_loss:  18.33, prob_loss:   1.47, total_loss:  20.71\n",
            "[[  6 196 298 312   1]]\n",
            "[[   1  135 1181  601    1]]\n",
            "[[ 71 116 262 305   2 328 173 488 351   2]]\n",
            "[[  4   3 286 248   0]]\n",
            "epoch:18 step:    5/60, lr:0.000004, giou_loss:   0.75, conf_loss:  17.19, prob_loss:   0.26, total_loss:  18.21\n",
            "[[ 90  92 709 706   0]]\n",
            "[[  3 126 599 793   1]]\n",
            "[[ 28  13 276 269   2]]\n",
            "[[196 344 621 753   2]]\n",
            "epoch:18 step:    6/60, lr:0.000004, giou_loss:   0.50, conf_loss:  17.08, prob_loss:   0.23, total_loss:  17.81\n",
            "[[ 84 110 648 690   0  78 176 590 665   0]]\n",
            "[[ 73  86 285 291   2]]\n",
            "[[ 180  250 1034 1128    0]]\n",
            "[[253  71 718 527   2]]\n",
            "epoch:18 step:    7/60, lr:0.000004, giou_loss:   0.65, conf_loss:  16.70, prob_loss:   0.49, total_loss:  17.84\n",
            "[[ 92  48 405 174   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "[[113   1 918 214   1]]\n",
            "[[496 316 716 517   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "[[157  91 346 285   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "epoch:18 step:    8/60, lr:0.000004, giou_loss:   1.02, conf_loss:  18.58, prob_loss:   0.84, total_loss:  20.43\n",
            "[[ 50 129 553 375   1]]\n",
            "[[ 46  55 343 342   0]]\n",
            "[[ 60  62 562 265   1]]\n",
            "[[  17    1 1590  624    1]]\n",
            "epoch:18 step:    9/60, lr:0.000004, giou_loss:   0.50, conf_loss:  16.79, prob_loss:   0.28, total_loss:  17.57\n",
            "[[295 134 460 329   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[ 45  36 222 199   0]]\n",
            "[[ 27 122 113 210   2  83 107 161 198   2]]\n",
            "[[ 83 170 316 389   2]]\n",
            "epoch:18 step:   10/60, lr:0.000004, giou_loss:   1.16, conf_loss:  17.93, prob_loss:   1.02, total_loss:  20.11\n",
            "[[325  13 663 374   2  58  69 410 432   2]]\n",
            "[[  8 145 172 295   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "[[ 85 137 488 545   2]]\n",
            "[[207  42 404 812   1]]\n",
            "epoch:18 step:   11/60, lr:0.000003, giou_loss:   1.13, conf_loss:  17.89, prob_loss:   1.05, total_loss:  20.07\n",
            "[[  1  21 148 165   0]]\n",
            "[[436 316 741 622   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[166  22 602 259   1]]\n",
            "[[ 34  95 359 424   0 304  62 575 353   0]]\n",
            "epoch:18 step:   12/60, lr:0.000003, giou_loss:   0.65, conf_loss:  16.66, prob_loss:   3.29, total_loss:  20.59\n",
            "[[  1  12 299 300   0]]\n",
            "[[ 44   2 185 159   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[222  61 400 239   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "[[ 73   2 577 529   0]]\n",
            "epoch:18 step:   13/60, lr:0.000003, giou_loss:   0.68, conf_loss:  17.02, prob_loss:   0.36, total_loss:  18.06\n",
            "[[  6  60 299 176   1]]\n",
            "[[286  80 550 393   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "[[ 16  29 436 471   0]]\n",
            "[[  93   34 1199 1061    0]]\n",
            "epoch:18 step:   14/60, lr:0.000003, giou_loss:   0.46, conf_loss:  16.52, prob_loss:   0.22, total_loss:  17.19\n",
            "[[ 51  83 943 570   1]]\n",
            "[[ 38  95 294 325   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "[[ 23  81 536 593   2]]\n",
            "[[  8 386 910 864   1]]\n",
            "epoch:18 step:   15/60, lr:0.000003, giou_loss:   0.67, conf_loss:  16.91, prob_loss:   0.47, total_loss:  18.05\n",
            "[[ 59  25 378 137   1]]\n",
            "[[114 124 581 399   1 190 100 623 330   1]]\n",
            "[[500  89 920 465   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "[[  9 112 242 379   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "epoch:18 step:   16/60, lr:0.000003, giou_loss:   0.79, conf_loss:  16.91, prob_loss:   0.30, total_loss:  18.00\n",
            "[[  65  191 1107  682    1   68  224 1011  751    1]]\n",
            "[[ 162  229  779  849    2  990  370 1578  974    2]]\n",
            "[[ 341  109 1220  995    2]]\n",
            "[[145 131 601 617   2]]\n",
            "epoch:18 step:   17/60, lr:0.000003, giou_loss:   0.57, conf_loss:  16.10, prob_loss:   1.78, total_loss:  18.45\n",
            "[[117  82 302 174   1 305  81 489 175   1]]\n",
            "[[  1  23  83 111   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "[[ 12   6 405 408   2]]\n",
            "[[142  37 470 339   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "epoch:18 step:   18/60, lr:0.000003, giou_loss:   1.50, conf_loss:  18.31, prob_loss:   1.64, total_loss:  21.46\n",
            "[[  4  17 473 351   0   3  40 475 439   0]]\n",
            "[[ 12  14 614 467   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[ 156  296 1557 1687    2]]\n",
            "[[ 11  70 251 274   0]]\n",
            "epoch:18 step:   19/60, lr:0.000003, giou_loss:   0.68, conf_loss:  16.66, prob_loss:   0.39, total_loss:  17.74\n",
            "[[202  37 470 314   0]]\n",
            "[[401 331 729 655   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "[[  1  23 194 225   0]]\n",
            "[[ 69 334 553 810   0   1 132 368 605   0]]\n",
            "epoch:18 step:   20/60, lr:0.000003, giou_loss:   0.82, conf_loss:  17.84, prob_loss:   1.17, total_loss:  19.83\n",
            "[[ 286   64 1019  839    2  580   21 1273  753    2]]\n",
            "[[279   7 605 282   0]]\n",
            "[[529 218 923 599   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "[[  7   4 461 460   2]]\n",
            "epoch:18 step:   21/60, lr:0.000003, giou_loss:   0.72, conf_loss:  16.32, prob_loss:   1.25, total_loss:  18.29\n",
            "[[ 26  56 654 230   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "[[ 596  800 1230 1800    1  546  621 1071 1679    1]]\n",
            "[[ 47  25 299 284   2]]\n",
            "[[ 70  31 406 394   2]]\n",
            "epoch:18 step:   22/60, lr:0.000003, giou_loss:   0.76, conf_loss:  17.85, prob_loss:   1.60, total_loss:  20.21\n",
            "[[ 20  86 439 280   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "[[ 289  215 1943 1112    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[  3  24 252 256   2]]\n",
            "[[324 291 692 680   2]]\n",
            "epoch:18 step:   23/60, lr:0.000003, giou_loss:   0.53, conf_loss:  16.60, prob_loss:   1.38, total_loss:  18.50\n",
            "[[120 174 344 405   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "[[ 188   98 1295  853    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "[[ 95  44 192 149   2]]\n",
            "[[ 50  98 248 289   2 214  75 383 248   2]]\n",
            "epoch:18 step:   24/60, lr:0.000003, giou_loss:   1.27, conf_loss:  18.69, prob_loss:   1.98, total_loss:  21.94\n",
            "[[228  90 365 249   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "[[255   1 562 299   2]]\n",
            "[[126  46 322 291   1]]\n",
            "[[261 278 864 889   2]]\n",
            "epoch:18 step:   25/60, lr:0.000003, giou_loss:   1.22, conf_loss:  18.00, prob_loss:   1.14, total_loss:  20.35\n",
            "[[ 43   2 230 204   2]]\n",
            "[[ 43 135 219 324   0]]\n",
            "[[180  12 415 256   0]]\n",
            "[[123  58 546 387   1]]\n",
            "epoch:18 step:   26/60, lr:0.000003, giou_loss:   0.57, conf_loss:  16.17, prob_loss:   0.43, total_loss:  17.17\n",
            "[[110   9 252 154   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "[[252 166 490 444   2 347 127 603 398   2]]\n",
            "[[  8  61 347 263   1]]\n",
            "[[  8  84 250 339   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "epoch:18 step:   27/60, lr:0.000003, giou_loss:   0.68, conf_loss:  17.54, prob_loss:   0.95, total_loss:  19.17\n",
            "[[ 23  43 429 462   0]]\n",
            "[[   8  371  987 1366    0]]\n",
            "[[ 57 144 393 514   2 430  58 759 380   2]]\n",
            "[[262  52 508 313   2]]\n",
            "epoch:18 step:   28/60, lr:0.000003, giou_loss:   0.77, conf_loss:  17.51, prob_loss:   0.69, total_loss:  18.97\n",
            "[[  95  148  617  482    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "[[267   3 569 317   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "[[ 27 170 830 991   0   1   1 407 617   0 104   4 624 557   0]]\n",
            "[[168   3 379 199   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "epoch:18 step:   29/60, lr:0.000003, giou_loss:   0.95, conf_loss:  17.99, prob_loss:   0.91, total_loss:  19.86\n",
            "[[ 77  42 503 373   1]]\n",
            "[[ 20  28 276 284   0]]\n",
            "[[ 21  29 142 172   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "[[103 210 655 484   1]]\n",
            "epoch:18 step:   30/60, lr:0.000003, giou_loss:   0.58, conf_loss:  16.60, prob_loss:   0.31, total_loss:  17.49\n",
            "[[ 306    1 1264  741    2  822  643 1620 1066    2]]\n",
            "[[112  68 370 316   2 357  35 656 334   2]]\n",
            "[[ 25  42 275 297   0]]\n",
            "[[  25   98 1382 1418    0]]\n",
            "epoch:18 step:   31/60, lr:0.000003, giou_loss:   0.73, conf_loss:  16.56, prob_loss:   0.58, total_loss:  17.88\n",
            "[[135 218 537 307   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "[[ 489  372  753  624    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "[[ 35  35 161 175   0]]\n",
            "[[ 186    7 1026  843    2]]\n",
            "epoch:18 step:   32/60, lr:0.000003, giou_loss:   1.04, conf_loss:  18.16, prob_loss:   1.14, total_loss:  20.34\n",
            "[[160  38 593 363   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[  58   85 2790 2568    1]]\n",
            "[[342  83 701 416   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[189  60 390 265   2]]\n",
            "epoch:18 step:   33/60, lr:0.000003, giou_loss:   0.88, conf_loss:  17.22, prob_loss:   2.40, total_loss:  20.50\n",
            "[[110  46 214 153   2]]\n",
            "[[149   6 673 364   1]]\n",
            "[[123 223 419 541   0]]\n",
            "[[159  45 520 336   2]]\n",
            "epoch:18 step:   34/60, lr:0.000003, giou_loss:   1.11, conf_loss:  16.58, prob_loss:   0.45, total_loss:  18.14\n",
            "[[ 376   90 1138  825    2]]\n",
            "[[ 26  25 300 292   2]]\n",
            "[[ 145  857  952 1796    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "[[  5  40 179 215   0  18  22 168 190   0]]\n",
            "epoch:18 step:   35/60, lr:0.000003, giou_loss:   0.57, conf_loss:  16.80, prob_loss:   2.02, total_loss:  19.40\n",
            "[[404  35 860 404   2]]\n",
            "[[ 889  147 1461 1147    1]]\n",
            "[[129 132 517 307   1]]\n",
            "[[ 42  31 218 201   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "epoch:18 step:   36/60, lr:0.000002, giou_loss:   1.56, conf_loss:  18.63, prob_loss:   1.30, total_loss:  21.50\n",
            "[[198 103 510 391   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "[[   0  456  955 1092    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "[[ 36  45 204 215   0]]\n",
            "[[250  21 762 578   0]]\n",
            "epoch:18 step:   37/60, lr:0.000002, giou_loss:   0.92, conf_loss:  16.84, prob_loss:   2.25, total_loss:  20.01\n",
            "[[ 453  103 1187  824    2  172  181  860  843    2]]\n",
            "[[405 106 754 474   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "[[ 27  28 213 210   0]]\n",
            "[[121 124 768 488   1]]\n",
            "epoch:18 step:   38/60, lr:0.000002, giou_loss:   0.69, conf_loss:  17.89, prob_loss:   0.81, total_loss:  19.39\n",
            "[[312  48 663 421   0 363  83 770 514   0]]\n",
            "[[184 110 582 525   0]]\n",
            "[[315 231 538 445   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "[[131  32 353 251   2 213  98 413 320   2]]\n",
            "epoch:18 step:   39/60, lr:0.000002, giou_loss:   0.95, conf_loss:  16.93, prob_loss:   1.06, total_loss:  18.94\n",
            "[[ 10  49 320 365   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[ 51   3 414 350   0]]\n",
            "[[  1 171 532 676   2]]\n",
            "[[ 98  53 517 451   2 664 128 919 369   2]]\n",
            "epoch:18 step:   40/60, lr:0.000002, giou_loss:   0.56, conf_loss:  16.47, prob_loss:   0.45, total_loss:  17.47\n",
            "[[179   5 490 276   1 100  65 361 353   1]]\n",
            "[[ 13  21 190 188   0]]\n",
            "[[ 62  17 272 279   1]]\n",
            "[[ 60  22 309 274   0 314 112 594 385   0]]\n",
            "epoch:18 step:   41/60, lr:0.000002, giou_loss:   0.35, conf_loss:  17.00, prob_loss:   1.42, total_loss:  18.77\n",
            "[[ 80  29 263 219   2]]\n",
            "[[  1  29 432 470   2]]\n",
            "[[ 635  122 1013  533    2  147   48  869  476    1  169  180  839  798\n",
            "     1]]\n",
            "[[308  43 889 448   1  24 174 599 657   1]]\n",
            "epoch:18 step:   42/60, lr:0.000002, giou_loss:   0.75, conf_loss:  17.23, prob_loss:   0.55, total_loss:  18.54\n",
            "[[ 59   5 721 708   2]]\n",
            "[[ 163    5  586  431    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "[[257  42 558 362   2]]\n",
            "[[150  97 430 398   2]]\n",
            "epoch:18 step:   43/60, lr:0.000002, giou_loss:   0.70, conf_loss:  16.84, prob_loss:   2.68, total_loss:  20.21\n",
            "[[ 63  78 204 319   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[153 423 698 940   2]]\n",
            "[[ 24  28 288 300   2]]\n",
            "[[ 129  868  755 2006    1]]\n",
            "epoch:18 step:   44/60, lr:0.000002, giou_loss:   1.53, conf_loss:  18.83, prob_loss:   4.07, total_loss:  24.42\n",
            "[[ 16 127 379 616   1]]\n",
            "[[  56   24 1923 1844    2]]\n",
            "[[  58   35 1523 1609    2]]\n",
            "[[  7   6 162 153   0]]\n",
            "epoch:18 step:   45/60, lr:0.000002, giou_loss:   0.59, conf_loss:  16.80, prob_loss:   0.28, total_loss:  17.67\n",
            "[[318  86 558 330   2 291  81 545 348   2]]\n",
            "[[191 118 624 547   0]]\n",
            "[[ 17  82 241 324   2]]\n",
            "[[253  87 426 259   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "epoch:18 step:   46/60, lr:0.000002, giou_loss:   0.72, conf_loss:  16.84, prob_loss:   0.42, total_loss:  17.98\n",
            "[[242   1 942 708   0]]\n",
            "[[ 58 137 267 358   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "[[ 15  11 338 345   0]]\n",
            "[[ 78  79 257 249   2]]\n",
            "epoch:18 step:   47/60, lr:0.000002, giou_loss:   0.82, conf_loss:  17.47, prob_loss:   1.74, total_loss:  20.02\n",
            "[[  2  78 407 420   1]]\n",
            "[[101  50 544 445   1]]\n",
            "[[106   7 228 130   2 139   5 275 149   2]]\n",
            "[[ 59  72 287 305   2]]\n",
            "epoch:18 step:   48/60, lr:0.000002, giou_loss:   0.54, conf_loss:  17.17, prob_loss:   0.32, total_loss:  18.03\n",
            "[[  2  15 202 203   0]]\n",
            "[[127  69 611 359   1]]\n",
            "[[ 23  89 517 336   1 272  80 498 306   1]]\n",
            "[[ 45  21 352 353   0]]\n",
            "epoch:18 step:   49/60, lr:0.000002, giou_loss:   0.58, conf_loss:  16.76, prob_loss:   1.40, total_loss:  18.75\n",
            "[[246 102 416 390   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[ 125  315  889  657    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "[[ 527  240 1176  944    0]]\n",
            "[[ 87  73 449 438   0]]\n",
            "epoch:18 step:   50/60, lr:0.000002, giou_loss:   1.07, conf_loss:  17.74, prob_loss:   1.09, total_loss:  19.90\n",
            "[[ 58 105 420 432   2]]\n",
            "[[   5    8 1819  931    1]]\n",
            "[[241 236 929 756   1   4 236 433 756   1]]\n",
            "[[ 429   28 1182  710    0]]\n",
            "epoch:18 step:   51/60, lr:0.000002, giou_loss:   0.76, conf_loss:  16.46, prob_loss:   0.25, total_loss:  17.46\n",
            "[[145 131 315 309   0 197 144 363 328   0]]\n",
            "[[ 41  63 591 615   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "[[ 132  388 1103 1172    1  445  447 1069 1046    0]]\n",
            "[[ 70  39 558 346   1]]\n",
            "epoch:18 step:   52/60, lr:0.000002, giou_loss:   0.93, conf_loss:  17.41, prob_loss:   1.06, total_loss:  19.40\n",
            "[[  17   99 2473 3989    1]]\n",
            "[[ 21 163 447 377   1]]\n",
            "[[ 13  14 242 238   2]]\n",
            "[[187 213 369 424   0]]\n",
            "epoch:18 step:   53/60, lr:0.000002, giou_loss:   0.56, conf_loss:  16.59, prob_loss:   0.45, total_loss:  17.60\n",
            "[[ 18  21 336 325   1]]\n",
            "[[  1  12 197 195   0]]\n",
            "[[ 73  14 280 222   0]]\n",
            "[[231  57 547 350   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "epoch:18 step:   54/60, lr:0.000002, giou_loss:   0.49, conf_loss:  16.32, prob_loss:   2.79, total_loss:  19.60\n",
            "[[212  57 381 212   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "[[104 110 462 468   2  77 133 674 590   1]]\n",
            "[[ 43  60 417 438   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "[[132  98 591 348   1]]\n",
            "epoch:18 step:   55/60, lr:0.000002, giou_loss:   0.91, conf_loss:  17.05, prob_loss:   0.37, total_loss:  18.33\n",
            "[[602 238 881 519   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "[[ 57  41 236 213   0]]\n",
            "[[112 117 395 387   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[ 461   13 1774 1080    1]]\n",
            "epoch:18 step:   56/60, lr:0.000002, giou_loss:   1.57, conf_loss:  19.00, prob_loss:   0.92, total_loss:  21.50\n",
            "[[ 22  20 176 165   2]]\n",
            "[[ 14  12 301 298   0]]\n",
            "[[ 40  25 189 240   1]]\n",
            "[[ 15  12 247 182   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "epoch:18 step:   57/60, lr:0.000002, giou_loss:   0.59, conf_loss:  17.27, prob_loss:   0.25, total_loss:  18.12\n",
            "[[566  14 862 286   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "[[106  58 221 157   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "[[132  24 301 132   1]]\n",
            "[[ 791  317 1170  666    0]]\n",
            "epoch:18 step:   58/60, lr:0.000002, giou_loss:   2.30, conf_loss:  20.36, prob_loss:   3.58, total_loss:  26.24\n",
            "[[404 173 827 595   0 401 115 725 441   0]]\n",
            "[[ 33  20 118 114   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "[[395 128 641 353   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "[[  7   4 337 336   2]]\n",
            "epoch:18 step:   59/60, lr:0.000002, giou_loss:   0.96, conf_loss:  17.92, prob_loss:   1.15, total_loss:  20.03\n",
            "[[268  75 511 298   2]]\n",
            "[[  1   4 405 148   1]]\n",
            "[[143 253 613 695   0]]\n",
            "[[ 596   69 1174  581    2]]\n",
            "epoch:18 step:    0/60, lr:0.000002, giou_loss:   0.62, conf_loss:  16.29, prob_loss:   0.20, total_loss:  17.11\n",
            "[[ 13 173 570 407   1]]\n",
            "[[135  47 398 335   0  79  18 338 270   0]]\n",
            "[[ 57  50 492 492   0]]\n",
            "[[ 59  87 539 291   1]]\n",
            "epoch:18 step:    1/60, lr:0.000002, giou_loss:   0.62, conf_loss:  16.48, prob_loss:   0.47, total_loss:  17.57\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[174  20 364 199   2]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[133 165 922 525   1]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[279  14 562 295   2]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[150 165 923 983   2]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[111  51 372 277   2]]\n",
            "[[203 115 584 452   1]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[155 105 453 436   0]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[143  40 415 346   0]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[145  36 421 286   2]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "\n",
            "\n",
            "giou_val_loss:    nan, conf_val_loss:  18.63, prob_val_loss:   1.59, total_val_loss:    nan\n",
            "\n",
            "\n",
            "[[119  72 613 319   1 272  80 498 306   1]]\n",
            "[[  66   61  830  403    1  363  211 1013  754    1   37  187  793  485\n",
            "     1]]\n",
            "[[ 19   1 269 256   0]]\n",
            "[[245 169 478 436   0 339 345 578 569   0  10 239 214 471   0 193 191 423\n",
            "  406   0 442 176 635 327   0 301  22 533 216   0 535  55 700 220   0]]\n",
            "epoch:19 step:    2/60, lr:0.000002, giou_loss:   0.80, conf_loss:  17.46, prob_loss:   0.67, total_loss:  18.93\n",
            "[[ 87   1 589 204   1]]\n",
            "[[ 241   28 1044  849    0    1    1  407  617    0  104    4  624  557\n",
            "     0]]\n",
            "[[294 127 543 379   0 314 112 594 385   0]]\n",
            "[[ 44 145 341 432   0]]\n",
            "epoch:19 step:    3/60, lr:0.000002, giou_loss:   0.59, conf_loss:  16.65, prob_loss:   1.05, total_loss:  18.29\n",
            "[[ 47  69 666 683   0]]\n",
            "[[ 469  104 1116  468    1]]\n",
            "[[ 19  11 438 205   1 108  91 430 228   1 114  36 389 171   1]]\n",
            "[[155  99 411 329   2  24   1 239 187   2 401  47 600 297   2 205  25 417\n",
            "  209   2 302 272 566 398   2]]\n",
            "epoch:19 step:    4/60, lr:0.000002, giou_loss:   0.92, conf_loss:  17.44, prob_loss:   0.85, total_loss:  19.20\n",
            "[[  6 106 270 419   2 129  98 449 353   0 204  34 753 336   1]]\n",
            "[[540 257 934 638   2  73 164 474 550   0 177 604 858 966   1]]\n",
            "[[136  57 246 167   0 118  40 219 135   0 223  53 327 164   0]]\n",
            "[[  2  16 407 358   1]]\n",
            "epoch:19 step:    5/60, lr:0.000002, giou_loss:   1.23, conf_loss:  17.85, prob_loss:   0.90, total_loss:  19.98\n",
            "[[ 202  369  936 1090    2  172  181  860  843    2]]\n",
            "[[ 504  208 1266  943    2]]\n",
            "[[ 34  28 440 447   0]]\n",
            "[[ 25  36 204 208   0]]\n",
            "epoch:19 step:    6/60, lr:0.000002, giou_loss:   0.47, conf_loss:  16.13, prob_loss:   0.23, total_loss:  16.83\n",
            "[[ 45  28 480 470   0]]\n",
            "[[  46   72 1152 1099    0]]\n",
            "[[162 153 327 348   2 127 148 320 371   0  73 366 618 619   1]]\n",
            "[[159 358 335 528   0 282 296 449 448   0  57 284 211 424   0 481 239 615\n",
            "  387   0]]\n",
            "epoch:19 step:    7/60, lr:0.000002, giou_loss:   0.94, conf_loss:  18.81, prob_loss:   3.15, total_loss:  22.90\n",
            "[[ 40  14 223 204   2]]\n",
            "[[ 80 261 385 567   0 105 426 393 691   0 194 290 477 545   0]]\n",
            "[[ 82  43 231 258   1]]\n",
            "[[253   4 709 373   2]]\n",
            "epoch:19 step:    8/60, lr:0.000002, giou_loss:   0.69, conf_loss:  16.46, prob_loss:   0.29, total_loss:  17.44\n",
            "[[ 123  172 1696  795    1]]\n",
            "[[ 46  24 948 502   1]]\n",
            "[[120  45 481 336   2]]\n",
            "[[243  95 568 424   0 304  62 575 353   0]]\n",
            "epoch:19 step:    9/60, lr:0.000002, giou_loss:   0.82, conf_loss:  16.98, prob_loss:   0.73, total_loss:  18.53\n",
            "[[ 38   1 345 299   2]]\n",
            "[[ 10  20 230 221   0  35 217 253 453   0 183 177 382 411   0 605 298 787\n",
            "  513   0 498 370 675 567   0 333 239 574 463   0 191 350 373 543   0 443\n",
            "  425 655 598   0]]\n",
            "[[ 58 137 267 358   2 260  99 465 351   2 136   1 355 171   2]]\n",
            "[[267 243 646 592   0]]\n",
            "epoch:19 step:   10/60, lr:0.000002, giou_loss:   1.00, conf_loss:  18.24, prob_loss:   1.67, total_loss:  20.91\n",
            "[[ 44  26 395 399   0 363  83 770 514   0]]\n",
            "[[ 22   6 168 149   2  27 151 333 233   1 188   4 325 149   0]]\n",
            "[[318 271 667 639   2 389 337 818 688   2 621 277 965 620   2 365  61 805\n",
            "  419   2]]\n",
            "[[703 418 982 699   2 559  81 869 339   2 583 328 916 565   2]]\n",
            "epoch:19 step:   11/60, lr:0.000002, giou_loss:   0.99, conf_loss:  19.34, prob_loss:   1.27, total_loss:  21.60\n",
            "[[ 602  395 1025  802    2 1074  230 1458  657    2 1018  616 1498 1048\n",
            "     2]]\n",
            "[[ 21  46 218 816   1]]\n",
            "[[ 44 179 211 356   0 229 219 406 376   0 169 128 348 297   0]]\n",
            "[[ 74  83 243 191   1]]\n",
            "epoch:19 step:   12/60, lr:0.000001, giou_loss:   1.51, conf_loss:  19.74, prob_loss:   1.09, total_loss:  22.35\n",
            "[[ 67  22 330 310   0  79  18 338 270   0]]\n",
            "[[ 26  11 275 243   2]]\n",
            "[[ 12  20 674 723   2]]\n",
            "[[108 176 354 401   0   1  30 188 280   0 116   5 337 220   0]]\n",
            "epoch:19 step:   13/60, lr:0.000001, giou_loss:   0.56, conf_loss:  16.57, prob_loss:   1.01, total_loss:  18.13\n",
            "[[ 21  13 256 257   0]]\n",
            "[[ 12  60 189 227   0]]\n",
            "[[ 11  21 535 379   1]]\n",
            "[[548  69 884 439   2 430  58 759 380   2]]\n",
            "epoch:19 step:   14/60, lr:0.000001, giou_loss:   0.61, conf_loss:  16.87, prob_loss:   0.42, total_loss:  17.90\n",
            "[[140  72 368 305   2]]\n",
            "[[  1  39 294 155   1]]\n",
            "[[ 90   7 516 338   1]]\n",
            "[[ 23  18 191 188   0]]\n",
            "epoch:19 step:   15/60, lr:0.000001, giou_loss:   0.48, conf_loss:  16.19, prob_loss:   0.21, total_loss:  16.89\n",
            "[[216  21 652 258   1]]\n",
            "[[107 231 470 720   1]]\n",
            "[[230  69 650 511   0]]\n",
            "[[ 28  34 329 354   2]]\n",
            "epoch:19 step:   16/60, lr:0.000001, giou_loss:   0.61, conf_loss:  16.78, prob_loss:   0.30, total_loss:  17.69\n",
            "[[438  36 858 412   2  39 289 986 591   1 542  77 911 389   0]]\n",
            "[[  6  96 239 315   2]]\n",
            "[[ 13 111 641 285   1 120 106 707 416   1 279  14 720 308   1]]\n",
            "[[102  87 223 230   2  97   1 203 115   2   1  62 131 187   2   1   2 111\n",
            "   70   2  30 201 152 250   2]]\n",
            "epoch:19 step:   17/60, lr:0.000001, giou_loss:   0.84, conf_loss:  17.15, prob_loss:   0.62, total_loss:  18.61\n",
            "[[204  63 647 458   1]]\n",
            "[[146   7 410 279   2]]\n",
            "[[  7   7 247 251   2 291  81 545 348   2]]\n",
            "[[153 144 335 355   0]]\n",
            "epoch:19 step:   18/60, lr:0.000001, giou_loss:   0.57, conf_loss:  17.07, prob_loss:   1.50, total_loss:  19.14\n",
            "[[506 262 670 412   2 156  44 347 240   0 337  39 463 426   1 379 157 643\n",
            "  485   1 390  38 544 415   1]]\n",
            "[[  4 159  86 247   2  71 156 144 231   2  44  42 274 138   1 191 177 285\n",
            "  268   0]]\n",
            "[[ 14  13 288 280   2]]\n",
            "[[242  13 568 288   0]]\n",
            "epoch:19 step:   19/60, lr:0.000001, giou_loss:   1.13, conf_loss:  18.89, prob_loss:   1.71, total_loss:  21.73\n",
            "[[108 141 421 267   1 172  67 395 282   1  57  76 349 225   1]]\n",
            "[[203 280 571 669   2]]\n",
            "[[ 17   3 473 489   2]]\n",
            "[[  51  208  315  460    2  733  268 1004  535    2  543  213  796  440\n",
            "     2  283  247  517  455    2]]\n",
            "epoch:19 step:   20/60, lr:0.000001, giou_loss:   0.92, conf_loss:  16.97, prob_loss:   0.52, total_loss:  18.41\n",
            "[[  4  57 343 259   1]]\n",
            "[[112  39 581 373   0   3  40 475 439   0]]\n",
            "[[ 310  125 1817  901    1]]\n",
            "[[   8   58  741  833    2  580   21 1273  753    2]]\n",
            "epoch:19 step:   21/60, lr:0.000001, giou_loss:   0.77, conf_loss:  16.83, prob_loss:   1.01, total_loss:  18.61\n",
            "[[218  60 576 418   2  77 133 674 590   1]]\n",
            "[[ 330    2 1018  522    1    4  236  433  756    1]]\n",
            "[[ 15  10 226 206   2  77 296 286 514   2 254 376 466 575   2 189 461 391\n",
            "  631   2]]\n",
            "[[ 41 133 415 511   2 209 134 866 348   1 263 267 849 551   1]]\n",
            "epoch:19 step:   22/60, lr:0.000001, giou_loss:   0.85, conf_loss:  17.20, prob_loss:   0.74, total_loss:  18.80\n",
            "[[ 15   1 820 214   1]]\n",
            "[[  2   5 284 250   0]]\n",
            "[[ 499  445 1606 1200    1  199  587 1470 1268    1  797    1 1772  611\n",
            "     1 1052    1 1916  550    1]]\n",
            "[[ 62  72 251 266   0  99 131 264 316   0 235 142 404 324   0]]\n",
            "epoch:19 step:   23/60, lr:0.000001, giou_loss:   0.77, conf_loss:  16.93, prob_loss:   0.62, total_loss:  18.32\n",
            "[[254 163 582 487   0 217 448 535 713   0 603 470 800 716   0 468 179 727\n",
            "  467   0   1  63 308 414   0]]\n",
            "[[ 155   76 1126  860    1  445  447 1069 1046    0]]\n",
            "[[ 16  87 256 291   0]]\n",
            "[[ 76  11 672 678   1]]\n",
            "epoch:19 step:   24/60, lr:0.000001, giou_loss:   0.56, conf_loss:  17.83, prob_loss:   1.03, total_loss:  19.42\n",
            "[[144  55 337 257   0]]\n",
            "[[  5  18 335 350   2]]\n",
            "[[342  83 701 416   1 186  39 624 382   1 205   8 670 226   1]]\n",
            "[[  5  39 253 295   2]]\n",
            "epoch:19 step:   25/60, lr:0.000001, giou_loss:   0.46, conf_loss:  16.69, prob_loss:   0.52, total_loss:  17.67\n",
            "[[410 114 580 402   1 328 128 440 444   1 360  88 511 345   1 232 141 343\n",
            "  458   1]]\n",
            "[[174 101 576 190   1  71 109 467 261   1  80 145 477 354   1 203 165 554\n",
            "  436   1]]\n",
            "[[ 17  39 574 273   1]]\n",
            "[[286  77 598 365   2 105 333 930 529   1 115 452 958 660   1]]\n",
            "epoch:19 step:   26/60, lr:0.000001, giou_loss:   1.61, conf_loss:  19.36, prob_loss:   3.28, total_loss:  24.24\n",
            "[[ 470  161  992  495    1  628    1 1169  298    1    1    3  412  344\n",
            "     1    1  249  260  669    1  229  449  710  674    1  822  480 1198\n",
            "   674    1]]\n",
            "[[ 70  49 249 219   2]]\n",
            "[[1633  394 3098 1968    2]]\n",
            "[[ 16  11 245 235   2]]\n",
            "epoch:19 step:   27/60, lr:0.000001, giou_loss:   0.95, conf_loss:  17.72, prob_loss:   1.34, total_loss:  20.02\n",
            "[[126  13 299 185   2  25  93 310 376   1 103   5 267 166   0]]\n",
            "[[ 60  14 146 102   2  83 107 161 198   2]]\n",
            "[[ 52   8 259 216   0]]\n",
            "[[ 20  43 230 305   1]]\n",
            "epoch:19 step:   28/60, lr:0.000001, giou_loss:   0.70, conf_loss:  17.09, prob_loss:   0.53, total_loss:  18.32\n",
            "[[   1   39 1868 1859    2]]\n",
            "[[119 121 399 422   2]]\n",
            "[[187  75 356 230   2 164  49 324 212   2 226   2 368 143   2]]\n",
            "[[ 11  43 592 448   1  24 174 599 657   1]]\n",
            "epoch:19 step:   29/60, lr:0.000001, giou_loss:   0.51, conf_loss:  17.99, prob_loss:   0.70, total_loss:  19.21\n",
            "[[ 41  18 237 263   1]]\n",
            "[[ 463   31 1418  667    1  819  660 1600 1168    1  101  171 1281 1044\n",
            "     1]]\n",
            "[[ 14   7 337 341   0]]\n",
            "[[ 34  59 230 242   0]]\n",
            "epoch:19 step:   30/60, lr:0.000001, giou_loss:   0.41, conf_loss:  16.46, prob_loss:   2.99, total_loss:  19.87\n",
            "[[  2   1 318 294   1 171  49 543 224   1 158  45 524 304   1 190  51 533\n",
            "  255   1]]\n",
            "[[318 146 680 511   0]]\n",
            "[[ 13 340 305 456   1]]\n",
            "[[414 138 529 237   2 518 255 598 330   2 492 198 590 275   2 122 165 379\n",
            "  265   1 160 207 394 302   1 296 164 453 309   1 324 214 460 343   1 301\n",
            "   89 410 192   0 411 122 525 227   0]]\n",
            "epoch:19 step:   31/60, lr:0.000001, giou_loss:   1.51, conf_loss:  18.40, prob_loss:   3.47, total_loss:  23.38\n",
            "[[  21   62 1378 1382    0]]\n",
            "[[ 31 153 327 471   0]]\n",
            "[[ 45   5 352 337   0]]\n",
            "[[ 30  44 317 330   0]]\n",
            "epoch:19 step:   32/60, lr:0.000001, giou_loss:   0.63, conf_loss:  17.42, prob_loss:   1.62, total_loss:  19.67\n",
            "[[ 28  31 182 176   2]]\n",
            "[[ 12   2 264 261   2]]\n",
            "[[355  73 820 529   2]]\n",
            "[[ 223   67 1202 1062    0]]\n",
            "epoch:19 step:   33/60, lr:0.000001, giou_loss:   0.45, conf_loss:  16.45, prob_loss:   0.94, total_loss:  17.84\n",
            "[[  41  220  895 1098    0]]\n",
            "[[132  18 591 268   1]]\n",
            "[[ 46  19 304 267   2 357  35 656 334   2]]\n",
            "[[ 44   1 268 232   1  48 182 421 368   1  81 107 456 255   1]]\n",
            "epoch:19 step:   34/60, lr:0.000001, giou_loss:   0.70, conf_loss:  16.90, prob_loss:   0.52, total_loss:  18.13\n",
            "[[126  16 422 288   1 615  46 883 389   1 511  64 800 408   1 511 144 771\n",
            "  538   1 387 131 557 557   1 200 130 491 465   1 164  45 537 326   1]]\n",
            "[[ 12  45 153 202   0  46  40 148 149   0  86 108 197 217   0]]\n",
            "[[498 231 781 501   0 492 141 740 394   0 176 199 490 466   0 367  17 619\n",
            "  240   0 642  35 907 269   0]]\n",
            "[[269 257 460 446   2 328 173 488 351   2]]\n",
            "epoch:19 step:   35/60, lr:0.000001, giou_loss:   1.16, conf_loss:  19.68, prob_loss:   1.73, total_loss:  22.57\n",
            "[[ 141  314 1183  805    1   68  224 1011  751    1]]\n",
            "[[ 43  68 229 250   0]]\n",
            "[[  58   85 2790 2568    1]]\n",
            "[[509 125 811 439   0 559 211 838 464   0 252 272 529 514   0 699 185 939\n",
            "  453   0]]\n",
            "epoch:19 step:   36/60, lr:0.000001, giou_loss:   0.93, conf_loss:  17.07, prob_loss:   1.44, total_loss:  19.45\n",
            "[[ 30  14 215 106   1 305  81 489 175   1]]\n",
            "[[ 21  14 319 302   0]]\n",
            "[[178   2 581 410   2]]\n",
            "[[ 167   25 1568 1416    2]]\n",
            "epoch:19 step:   37/60, lr:0.000001, giou_loss:   0.99, conf_loss:  17.47, prob_loss:   0.77, total_loss:  19.23\n",
            "[[ 72 102 176 209   2]]\n",
            "[[115 290 667 564   1]]\n",
            "[[168  52 671 298   1]]\n",
            "[[  12  108 1192  574    1]]\n",
            "epoch:19 step:   38/60, lr:0.000001, giou_loss:   0.74, conf_loss:  16.42, prob_loss:   0.24, total_loss:  17.40\n",
            "[[219  95 644 504   2]]\n",
            "[[ 12  20 330 324   1]]\n",
            "[[ 46  19 479 448   0]]\n",
            "[[ 27  26 228 231   2]]\n",
            "epoch:19 step:   39/60, lr:0.000001, giou_loss:   0.52, conf_loss:  16.35, prob_loss:   0.21, total_loss:  17.08\n",
            "[[ 494    1 1452  741    2  822  643 1620 1066    2]]\n",
            "[[ 148   88 1802  985    1  478  264 1991 1332    1    1  189 1594  768\n",
            "     1]]\n",
            "[[ 57   2 561 529   0]]\n",
            "[[ 201   95  773 1095    1]]\n",
            "epoch:19 step:   40/60, lr:0.000001, giou_loss:   0.86, conf_loss:  17.14, prob_loss:   0.34, total_loss:  18.34\n",
            "[[119  49 216 154   2]]\n",
            "[[ 553  120 1232  868    0]]\n",
            "[[ 88 123 225 282   2  79  48 215 187   2 422 118 554 263   2 447  53 596\n",
            "  183   2 315  75 458 219   2 240 127 391 269   2 176  32 309 158   2]]\n",
            "[[ 27 115 511 405   1]]\n",
            "epoch:19 step:   41/60, lr:0.000001, giou_loss:   1.11, conf_loss:  18.05, prob_loss:   0.61, total_loss:  19.77\n",
            "[[124  62 393 326   0]]\n",
            "[[ 97  91 976 977   2]]\n",
            "[[ 22   9 360 370   2  58  69 410 432   2]]\n",
            "[[ 30 233 208 411   2  29  75 197 218   0  88 292 400 416   1]]\n",
            "epoch:19 step:   42/60, lr:0.000001, giou_loss:   0.67, conf_loss:  16.40, prob_loss:   0.47, total_loss:  17.54\n",
            "[[180 161 647 436   1 190 100 623 330   1]]\n",
            "[[  13    1 1326 1068    1]]\n",
            "[[  6 174 570 754   0  78 176 590 665   0]]\n",
            "[[159  66 336 229   0]]\n",
            "epoch:19 step:   43/60, lr:0.000001, giou_loss:   0.70, conf_loss:  17.03, prob_loss:   0.37, total_loss:  18.10\n",
            "[[270   3 657 196   1  37  75 186 237   1 124 122 666 311   1]]\n",
            "[[ 19   7 532 519   2]]\n",
            "[[ 600  381 1226 1519    1]]\n",
            "[[ 45   1 130  95   1  62  30 195 110   1   1  27  47  99   1  17   7  77\n",
            "   91   1]]\n",
            "epoch:19 step:   44/60, lr:0.000001, giou_loss:   1.36, conf_loss:  18.10, prob_loss:   1.74, total_loss:  21.20\n",
            "[[ 87 137 325 415   2 347 127 603 398   2]]\n",
            "[[ 24   5 335 276   1 100  65 361 353   1]]\n",
            "[[ 74 180 558 656   0   1 132 368 605   0]]\n",
            "[[  1  28 333 357   2]]\n",
            "epoch:19 step:   45/60, lr:0.000001, giou_loss:   0.52, conf_loss:  17.14, prob_loss:   0.77, total_loss:  18.43\n",
            "[[  4   9 204 197   0]]\n",
            "[[ 40   8 463 337   1]]\n",
            "[[  3 147 606 758   2]]\n",
            "[[ 75 244 453 655   2 147  48 869 476   1 169 180 839 798   1]]\n",
            "epoch:19 step:   46/60, lr:0.000001, giou_loss:   0.53, conf_loss:  16.92, prob_loss:   0.50, total_loss:  17.95\n",
            "[[  9   8 221 213   2]]\n",
            "[[ 10  73 436 287   1]]\n",
            "[[ 24 126 412 301   1]]\n",
            "[[  56  316  896 1152    2]]\n",
            "epoch:19 step:   47/60, lr:0.000001, giou_loss:   0.77, conf_loss:  17.42, prob_loss:   0.36, total_loss:  18.56\n",
            "[[330  56 573 279   2]]\n",
            "[[356  72 906 624   0 446  78 887 546   0 683  88 960 487   0]]\n",
            "[[ 75  23 468 425   2]]\n",
            "[[ 84  43 403 155   1]]\n",
            "epoch:19 step:   48/60, lr:0.000001, giou_loss:   0.77, conf_loss:  16.72, prob_loss:   0.56, total_loss:  18.05\n",
            "[[148 209 372 451   2]]\n",
            "[[  9  35 413 179   1]]\n",
            "[[ 57  77 183 217   0]]\n",
            "[[124   1 487 348   0]]\n",
            "epoch:19 step:   49/60, lr:0.000001, giou_loss:   0.50, conf_loss:  16.29, prob_loss:   0.26, total_loss:  17.04\n",
            "[[137  98 278 339   1 298  93 382 354   1 219 110 292 349   1 121 117 216\n",
            "  354   1]]\n",
            "[[  5  12 152 156   0]]\n",
            "[[150  37 418 314   0]]\n",
            "[[ 90  82 578 389   1]]\n",
            "epoch:19 step:   50/60, lr:0.000001, giou_loss:   1.17, conf_loss:  17.75, prob_loss:   2.67, total_loss:  21.59\n",
            "[[ 465  282  888  708    2  173  127  653  611    0  283  215 1172  764\n",
            "     1]]\n",
            "[[ 299  316 1052  998    0]]\n",
            "[[  7  36 438 477   2]]\n",
            "[[  27  459  834 1398    1   46  524 1011 1202    1    1  399  899  889\n",
            "     1  164  212  945  791    1]]\n",
            "epoch:19 step:   51/60, lr:0.000001, giou_loss:   0.77, conf_loss:  17.04, prob_loss:   0.54, total_loss:  18.34\n",
            "[[ 38  55 400 382   2]]\n",
            "[[337  83 579 338   2 440 148 666 404   0  54 228 627 528   1]]\n",
            "[[ 87  27 689 480   1  94   3 665 300   1 117   7 628 216   1 520 147 719\n",
            "  480   1]]\n",
            "[[   3    2 1817  925    1]]\n",
            "epoch:19 step:   52/60, lr:0.000001, giou_loss:   0.80, conf_loss:  17.25, prob_loss:   0.53, total_loss:  18.58\n",
            "[[  7   3 263 259   0]]\n",
            "[[327  17 746 415   2 664 128 919 369   2]]\n",
            "[[ 36  81 928 568   1]]\n",
            "[[100  21 222 144   2 139   5 275 149   2]]\n",
            "epoch:19 step:   53/60, lr:0.000001, giou_loss:   0.41, conf_loss:  16.02, prob_loss:   0.25, total_loss:  16.67\n",
            "[[104 220 274 398   0 197 144 363 328   0]]\n",
            "[[233   9 561 311   1  54 114 452 371   1 126  31 438 226   1]]\n",
            "[[  79  429  713 1429    1  546  621 1071 1679    1]]\n",
            "[[   6  343 2462 4233    1]]\n",
            "epoch:19 step:   54/60, lr:0.000001, giou_loss:   1.24, conf_loss:  18.42, prob_loss:   3.25, total_loss:  22.90\n",
            "[[  5  15 536 520   2]]\n",
            "[[ 22   8 177 155   0]]\n",
            "[[ 65  52 311 313   2]]\n",
            "[[ 69  63 301 233   1 170  43 355 216   1 140  56 290 246   1  86  75 172\n",
            "  290   1]]\n",
            "epoch:19 step:   55/60, lr:0.000001, giou_loss:   0.41, conf_loss:  16.76, prob_loss:   0.27, total_loss:  17.44\n",
            "[[ 65 129 545 333   1]]\n",
            "[[449 114 759 430   2  10 204 616 421   1  16 262 666 517   1  64  39 326\n",
            "  332   0]]\n",
            "[[ 76  20 499 442   0 401 115 725 441   0]]\n",
            "[[304  99 527 313   2 463 223 690 434   0 154 258 510 455   1]]\n",
            "epoch:19 step:   56/60, lr:0.000001, giou_loss:   1.38, conf_loss:  17.21, prob_loss:   0.43, total_loss:  19.02\n",
            "[[ 19 141 195 330   0]]\n",
            "[[ 57  12 244 214   2]]\n",
            "[[148  10 618 452   0]]\n",
            "[[ 44 118 163 233   0 208 181 339 306   0 264  64 370 159   0 331   1 466\n",
            "  105   0 420  46 541 162   0 324 104 438 211   0]]\n",
            "epoch:19 step:   57/60, lr:0.000001, giou_loss:   0.68, conf_loss:  16.29, prob_loss:   0.82, total_loss:  17.78\n",
            "[[323  59 901 571   2]]\n",
            "[[ 32  29 486 485   2]]\n",
            "[[ 20   1 720 708   0]]\n",
            "[[182  94 324 239   0 219 195 341 316   0 182 311 334 435   0   1 181 167\n",
            "  338   0 309 101 408 191   0 377 248 451 323   0 402  77 463 138   0 395\n",
            "  152 460 219   0]]\n",
            "epoch:19 step:   58/60, lr:0.000001, giou_loss:   0.81, conf_loss:  16.82, prob_loss:   1.40, total_loss:  19.03\n",
            "[[290 111 939 815   0]]\n",
            "[[339 227 851 784   0]]\n",
            "[[164 229 771 832   0]]\n",
            "[[ 36  35 210 210   0  18  22 168 190   0]]\n",
            "epoch:19 step:   59/60, lr:0.000001, giou_loss:   0.62, conf_loss:  16.92, prob_loss:   4.94, total_loss:  22.48\n",
            "[[123  42 321 233   2 214  75 383 248   2]]\n",
            "[[ 47  81 592 598   2]]\n",
            "[[ 23  85 694 342   1]]\n",
            "[[132  19 354 238   2 213  98 413 320   2]]\n",
            "epoch:19 step:    0/60, lr:0.000001, giou_loss:   0.94, conf_loss:  16.63, prob_loss:   0.81, total_loss:  18.38\n",
            "[[ 51  41 449 456   0]]\n",
            "[[ 228  136  845  756    2  990  370 1578  974    2]]\n",
            "[[174  62 607 387   1 322  77 696 439   1 116  48 555 293   1]]\n",
            "[[  4  29 340 392   2]]\n",
            "epoch:19 step:    1/60, lr:0.000001, giou_loss:   0.67, conf_loss:  16.29, prob_loss:   0.33, total_loss:  17.28\n",
            "[[   1  571  923 1006    1   80    6  481  395    0  543   17  942  430\n",
            "     2]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[155 105 453 436   0]]\n",
            "[[  99   96  532  592    2  652  196 1123  710    2  430  108  776  525\n",
            "     2]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[383   3 596 308   1 265  15 513 276   1 516  52 762 315   1 639  42 819\n",
            "  325   1]]\n",
            "[[ 21 216 173 371   2 108 160 244 310   2 263 166 415 307   2]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[150 165 923 983   2]]\n",
            "[[ 17 195 202 356   2 200 207 376 365   2 223  52 389 217   2 261 157 412\n",
            "  264   2]]\n",
            "[[ 313  228  935 1864    1  401  129 1078 1890    1  135  291  642 1747\n",
            "     1]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[228  83 401 270   2  43  74 183 244   1 128  56 282 221   0]]\n",
            "[[ 219  494  946 1173    0  930  598 1667 1174    0   78  323  744  945\n",
            "     0 1228  312 1896  916    0]]\n",
            "[[165 163 367 336   2   1 100 189 266   2 167  88 329 237   2]]\n",
            "[[113  36 276 182   0 280  83 452 255   1 188  77 412 217   1 172  78 395\n",
            "  176   1]]\n",
            "[[ 23 472 848 880   1  66 233 878 657   1 212 111 949 611   1]]\n",
            "[[162  50 310 354   1  81  16 220 348   1]]\n",
            "[[112 144 173 211   0 250 140 322 215   0]]\n",
            "[[122 127 484 471   1 212 115 691 407   1 238  77 743 294   1]]\n",
            "[[ 42  41 193 182   2 158   1 299 109   2 284  17 355 147   2   1  15  61\n",
            "  138   2  34   1 172  85   2]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 146  205  493  594    2  252  528  571  821    0  468   49 1228  287\n",
            "     1  540  208 1244  464    1  506  301 1279  574    1]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[  1 123 130 299   2 379   1 558 144   2]]\n",
            "[[166  92 638 559   2]]\n",
            "[[203 115 584 452   1]]\n",
            "[[ 25 135 663 543   1  86  92 736 385   1]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[145  36 421 286   2]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[ 44 164 369 277   1  87 220 401 355   1]]\n",
            "[[100  64 381 190   1  95 280 380 405   1]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[ 64  84 484 457   0 550  44 850 356   0 197   1 559 273   0]]\n",
            "[[123 110 414 427   0 338   1 638 242   0]]\n",
            "[[114 114 324 431   1  23  56 170 422   1]]\n",
            "[[133 165 922 525   1]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[133 246 337 434   2 270 364 400 476   2 522 344 649 470   2 537 302 661\n",
            "  402   2 328 229 544 422   0]]\n",
            "[[ 43  57 143 160   2 148  62 249 162   2]]\n",
            "[[ 71  60 175 164   0  12  22 105 111   0 134  23 243 115   0 107 126 216\n",
            "  229   0 207 138 298 229   0]]\n",
            "[[  90  287  608  809    2  560  217 1145  769    2  323   37  786  551\n",
            "     2]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[111  51 372 277   2]]\n",
            "[[174  20 364 199   2]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[227  62 723 500   0]]\n",
            "\n",
            "\n",
            "giou_val_loss:    nan, conf_val_loss:  18.57, prob_val_loss:   1.69, total_val_loss:    nan\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-3Ae77yB81l",
        "outputId": "21d5ae14-b0f0-4ba5-a5ae-f7cdf537db50"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35.06813456217448"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NoPYfFSXlWw"
      },
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "MKS9tanWXjI4",
        "outputId": "5af991a5-243b-44ae-8470-7fb5159ee815"
      },
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "YOLO_INPUT_SIZE = 416\n",
        "input_size=YOLO_INPUT_SIZE\n",
        "\n",
        "\n",
        "\n",
        "image_path =\"./fruit/test/banana_77.jpg\" #image_info[0]\n",
        "\n",
        "yolo = Create_Yolov3(input_size=input_size, CLASSES=TRAIN_CLASSES)\n",
        "yolo.load_weights(\"./chkpt_fruits/yolov3_custom\")\n",
        "\n",
        "\n",
        "\n",
        "img = detect_image(yolo, image_path, \"\", input_size=input_size, show=True, CLASSES=TRAIN_CLASSES, rectangle_colors=(255,0,0))\n",
        "cv2_imshow(img)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=300x360 at 0x7FC77AAC6710>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jHWJOLySH8Vh",
        "outputId": "886c2d3e-dbf8-4090-84cf-3ab448e99ba6"
      },
      "source": [
        "image_path"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./fruit/test/banana_77.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}